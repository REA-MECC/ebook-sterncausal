[[["index.html","chapter1.html","S1.html"],"1 O Paradoxo de Simpson ‣ Capítulo 1 Por que estudar Inferência Causal? ‣ Inferência Causal","Skip to content. O Paradoxo de Simpson 1 O Paradoxo de Simpson \\MakeFramed ## C 0 1 ## Z T ## 0 0 36 234 ## 1 6 81 ## 1 0 25 55 ## 1 71 192 \\endMakeFramed Tabela 1: Frequência conjunta das variáveis binárias T , C , e Z . Considere que observamos em 700 pacientes 3 variáveis: T e C são as indicadoras de que, respectivamente, o paciente recebeu um tratamento e o paciente curou de uma doença, e Z é uma variável binária cujo significado será discutido mais tarde. Os dados foram resumidos na tabela 1. Em uma primeira análise desta tabela, podemos verificar a efetividade do tratamento dentro de cada valor de Z . Por exemplo, quando Z=0 , a frequência de recuperação dentre aqueles que receberam e não receberam o tratamento são, respectivamente: \\frac{81}{6+81}\\approx 0.93 e \\frac{234}{36+234}\\approx 0.87 . Similarmente, quando Z=1 , as respectivas frequências são: \\frac{192}{71+192}\\approx 0.73 e \\frac{55}{25+55}\\approx 0.69 . À primeira vista, para todos os valores de Z , a taxa de recuperação é maior com o tratamento do que sem ele. Isso nos traz informação de que o tratamento é efetivo na recuperação do paciente? Em uma segunda análise, podemos considerar apenas as contagens para as variáveis T e C , sem estratificar por Z . Dentre os pacientes que receberam e não receberam o tratamento as taxas de recuperação são, respectivamente: \\frac{81+192}{6+71+81+192}\\approx 0.78 e \\frac{234+55}{36+25+234+55}\\approx 0.83 . Isto é, sem estratificar por Z , a frequência de recuperação é maior dentre aqueles que não receberam o tratamento do que dentre aqueles que o receberam. O que é possível concluir destas análises? Uma conclusão ingênua poderia ser a de que, se Z não for observada, então o tratamento não é recomendado. Por outro lado, se Z é observada, não importa qual seja o seu valor, o tratamento será recomendado. A falta de sentido desta conclusão ingênua é o que tornou este tipo de dado famoso como sendo um caso de Paradoxo de Simpson (Simpson1951). Contudo, se a conclusão ingênua é paradoxal e incorreta, então qual conclusão pode ser obtida destes dados? A primeira lição que verificaremos é que não é possível obter uma conclusão sobre o efeito causal do tratamento usando apenas a informação na tabela, isto é, associações. Para tal, analisaremos a tabela dando dois nomes distintos para a variável Z . Veremos que, usando exatamente os mesmos dados, uma conclusão válida diferente é obtida para cada nome de Z . Em outras palavras, o efeito causal depende de mais informação do que somente aquela disponível na tabela. Em um primeiro cenário, considere que Z é a indicadora de que o sexo do paciente é masculino. Observando a tabela, notamos que, proporcionalmente, mais homens receberam o tratamento do que mulheres. Como o tratamento não tem qualquer influência sobre o sexo do paciente, podemos imaginar um cenário em que, proporcionalmente, mais homens escolheram receber o tratamento do que mulheres. Usando esta observação, podemos fazer sentido do Paradoxo anteriormente obtido. Quando agregamos os dados, notamos que o primeiro grupo de pacientes que receberam o tratamento é predominantemente composto por homens e, similarmente, o segundo grupo de pacientes que não receberam o tratamento é predominantemente composto por mulheres. Isto é, na análise dos dados agregados estamos essencialmente comparando a taxa de recuperação de homens que receberam o tratamento com a de mulheres que não receberam o tratamento. Se assumirmos que, independentemente do tratamento, mulheres tem uma probabilidade de recuperação maior do que homens, então a taxa de recuperação menor no primeiro grupo pode ser explicada pelo fato de ele ser composto predominantemente por homens e não pelo fato de ser o grupo de pacientes que recebeu o tratamento. Também, da análise anterior, obtemos que para cada sexo, a taxa de recuperação é maior com o tratamento do que sem ele. Isto é, neste cenário, o tratamento parece efetivo para a recuperação dos pacientes. Isto significa que a análise estratificando Z é sempre a correta? Caso o significado da variável Z seja outro, veremos que esta conclusão é incorreta. Considere que Z é a indicadora de que a pressão sanguínea do paciente está elevada. Além disso, é sabido que o tratamento tem como efeito colateral aumentar o risco de pressão elevada nos pacientes. Neste caso, o fato de que há mais indivíduos com pressão elevada dentre aqueles que receberam o tratamento é um efeito direto do tratamento. Usando esta observação, podemos chegar a outras conclusões sobre o efeito do tratamento sobre a recuperação dos pacientes. Para tal, considere que o tratamento tem um efeito positivo moderado sobre a recuperação dos pacientes, mas que a pressão sanguínea elevada prejudica gravemente a recuperação. Quando fazemos comparações apenas dentre indivíduos com pressão alta ou apenas dentre indivíduos sem pressão alta, não é possível identificar o efeito coletaral do tratamento. Isto é, observamos apenas o efeito positivo moderado que o tratamento tem sobre a recuperação. Por outro lado, quando fazemos a análise agregada, observamos que a frequência de recuperação é maior dentre os indivíduos que não receberam o tratamento do que dentre os que o receberam. Isso ocorre pois o efeito colateral negativo tem um impacto maior sobre a recuperação do paciente do que o efeito geral benéfico. Assim, neste cenário, o tratamento não é eficiente para levar à recuperação do paciente. Como nossas conclusões dependem de qual história adotamos, podemos ver que a mera apresentação da tabela é insuficiente para determinar a eficiência do tratamento. Observando com cuidado os cenários, identificamos uma explicação geral para as diferentes conclusões. No primeiro cenário, quando Z é sexo, Z é uma causa do indivíduo receber ou não o tratamento. Já no segundo cenário, quando Z é pressão elevada, o tratamento é causa de Z . Isto é, a diferença nas relações entre as variáveis explica as diferenças entre as conclusões obtidas. Ao longo do curso, desenvolveremos ferramentas para formalizar a diferença entre estes cenários e, com base nisso, conseguir estimar o efeito causal que uma variável X tem sobre outra variável Y . Contudo, para tal, será necessário desenvolver um modelo em que seja possível descrever relações causais. Esta questão será tratada no capítulo 2. 1.1 Exercícios Exercício 1.1 (Glymour2016[p.6]). Há evidência de que há correlação positiva entre uma pessoa estar atrasada e estar apressada. Isso significa que uma pessoa pode evitar atrasos se não tiver pressa? Justifique sua resposta em palavras. Previous page Next page"],[["index.html","chapter4.html","S10.html"],"10 Contrafactuais ‣ Capítulo 4 Resultados potenciais ‣ Inferência Causal","Skip to content. Contrafactuais 10 Contrafactuais Existem situações em que gostaríamos de saber o que teria ocorrido, caso certas condições fossem diferentes daquelas que foram efetivamente observadas. Por exemplo, considere que a perna de um indivíduo é amputada em virtude de um erro de diagnóstico médico. Neste caso, o indivíduo tem o direito a ser indenizado por seus danos. Contudo, qual o valor da indenização? Para responder a esta pergunta, somos levados a questionar como seria a vida deste indivíduo caso não houvesse o erro de diagnóstico. Este tipo de pergunta é chamada de contrafactual. Uma característica fundamental de contrafactuais é que estamos interessados em uma “realidade” distinta daquela que foi observada. Contudo, se só observamos uma realidade, como é possível aprender algo sobre “realidades distintas”? Por exemplo, se supomos que não houve um erro de diagnóstico médico, o que mais seria diferente? Nesta realidade alternativa, consideramos que não houve erro médico porque há um médico muito mais concentrado, competente, ético e com exames mais precisos? Ou estamos supondo apenas que características pontuais que o levaram ao erro não estão presentes e, assim, essencialmente é o mesmo médico tratando o paciente? Ainda que não há uma resposta única para esta pergunta, há um cenário que é comumente analisado. Neste supomos que a realidade alternativa é a mais próxima possível da observada dada a restrição que um determinado fato ocorreu diferentemente. Neste sentido, resultados potenciais são um formalismo útil. Dentro deste formalismo, consideramos que {\\mathcal{V}} são as variáveis da “realidade observada”. Por outro lado, {\\mathcal{V}}_{{\\mathbf{X}}={\\mathbf{x}}} são as variáveis que seriam observadas quando {\\mathbf{X}} é fixado no valor {\\mathbf{x}} . Neste formalismo (Definição 4.7), consideramos que Y=g_{Y}(Pa^{*}(Y),U_{Y}) e Y_{{\\mathbf{X}}={\\mathbf{x}}}=g_{Y}(Pa^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}}}),U_{Y}) Isto é, na realidade contrafactual, Y_{{\\mathbf{X}}={\\mathbf{x}}} e Y são gerados pelo mesmo mecanismo, g_{Y} . Além disso, os ruídos locais representados por U_{Y} são os mesmos em Y e Y_{{\\mathbf{X}}={\\mathbf{x}}} . Pode-se argumentar que a equivalência de mecanismos e de ruídos locais satisfaz a condição de que realidades contrafactuais devem ser tão próximas quanto possível da realidade observada. Exemplo 4.28. Considere que X é a indicadora de que houve um erro médico e Y é a indicadora de que a perna do paciente não é amputada. Por simplicidade, vamos supor que estas são as únicas duas variáveis relevantes e que o grafo causal é {\\mathcal{G}} tal que X\\rightarrow Y . Também considere que f(X=1)=\\epsilon , f(Y=1|X=1)=p_{1} , e f(Y=1|X=0)=p_{0} , p_{0}>p_{1} . Assim, o CM é ({\\mathcal{G}},f) . Para definir, um modelo de resultados potenciais, é necessário determinar um SCM (Definição 4.3). Uma possibilidade é escolher U_{X},U_{Y}\\sim U(0,1) , g_{X}(U_{X})\\equiv{\\mathbb{I}}(U_{X}\\leq\\epsilon) , e g_{Y}(U_{Y},X)\\equiv{\\mathbb{I}}(U_{Y}\\leq p_{X}) . Podemos mostrar que este SCM representa o CM definido no parágrafo anterior: \\displaystyle f(X=1) \\displaystyle={\\mathbb{P}}({\\mathbb{I}}(U_{X}\\leq\\epsilon))=\\epsilon \\displaystyle\\text{\\lx@cref{creftype~refnum}{def:scm}},U_{X}\\sim U(0,1), \\displaystyle f(Y=1|X=x) \\displaystyle={\\mathbb{P}}({\\mathbb{I}}(U_{Y}\\leq p_{x}))=p_{x}. Com base no modelo de resultados potenciais, podemos perguntar qual teria sido a probabilidade de que a perna do paciente não fosse amputada sem um erro médico, sabendo que observou-se o erro e a amputação: {\\mathbb{P}}(Y_{X=0}|X=1,Y=1) . \\displaystyle{\\mathbb{P}}(Y_{X=0}=1|X=1,Y=0) \\displaystyle={\\mathbb{P}}({\\mathbb{I}}(U_{Y}\\leq p_{0})=1|{\\mathbb{I}}(U_{X}% \\leq\\epsilon)==1,{\\mathbb{I}}(U_{Y}>p_{1})=1) \\displaystyle={\\mathbb{P}}(U_{Y}\\leq p_{0}|U_{X}\\leq\\epsilon,U_{Y}>p_{1}) \\displaystyle={\\mathbb{P}}(U_{Y}\\leq p_{0}|U_{Y}>p_{1}) \\displaystyle=\\frac{{\\mathbb{P}}(p_{1}<U_{Y}\\leq p_{0})}{{\\mathbb{P}}(U_{Y}>p_% {1})}=\\frac{p_{0}-p_{1}}{1-p_{1}} Uma característica importante do Exemplo 4.28 é que a probabilidade contrafactual depende tanto da distribuição de U_{Y} quanto da funções g_{Y} . Estas quantidades não podem ser determinadas pelos dados. Em outras palavras, as probabilidades contrafactuais dependem fundamentalmente de suposições que não podem ser testadas. No Exemplo 4.28 definimos que g_{Y}(U_{Y},X)={\\mathbb{I}}(U_{Y}\\leq p_{X}) . Este acoplamento determina que todo paciente que não teve sua perna amputada com um erro médico, também não a teria sem o erro médico. Como nunca observamos ambas as situações para um mesmo paciente, esta afirmação não é testável. O Exemplo 4.29 mostra que a probabilidade contrafactual varia conforme o acoplamento utilizado. Exemplo 4.29. No Exemplo 4.28, considere que g_{Y}(U_{Y},1)={\\mathbb{I}}(U_{Y}\\leq p_{1}) e g_{Y}(U_{Y},0)={\\mathbb{I}}(U_{Y}\\geq 1-p_{0}) . \\displaystyle{\\mathbb{P}}(Y_{X=0}=1|X=1,Y=0) \\displaystyle={\\mathbb{P}}({\\mathbb{I}}(U_{Y}\\geq 1-p_{0})=1|{\\mathbb{I}}(U_{X% }\\leq\\epsilon)==1,{\\mathbb{I}}(U_{Y}>p_{1})=1) \\displaystyle={\\mathbb{P}}(U_{Y}\\geq 1-p_{0}|U_{X}\\leq\\epsilon,U_{Y}>p_{1}) \\displaystyle={\\mathbb{P}}(U_{Y}>1-p_{0}|U_{Y}>p_{1}) \\displaystyle=\\frac{{\\mathbb{P}}(U_{Y}>\\max(1-p_{0},p_{1})}{{\\mathbb{P}}(U_{Y}% >p_{1})}=\\frac{\\min(p_{0},1-p_{1})}{1-p_{1}} Neste caso, g_{Y} induz a mema distribuição sobre (X,Y) que o acoplamento no Exemplo 4.28. Ainda assim, a probabilidade contrafactual obtida é diferente. Isto ocorre pois, ao contrário do Exemplo 4.28, a nova g_{Y} indica que é possível que um paciente tenha a perna amputada sem o erro médico e não a tenha com o erro. De um ponto de vista operacional, o Teorema 4.30 abaixo provê um algoritmo para calcular probabilidades contrafactuais: Teorema 4.30 (Cálculo contrafactual). \\displaystyle{\\mathbb{P}}({\\mathbf{Y}}_{{\\mathbf{X}}={\\mathbf{x}}}\\leq{\\mathbf% {y}}|{\\mathbf{Z}}={\\mathbf{z}}) \\displaystyle=\\int{\\mathbb{P}}({\\mathbf{Y}}_{{\\mathbf{X}}={\\mathbf{x}}}\\leq{% \\mathbf{y}}|{\\mathbb{U}})f({\\mathbb{U}}|{\\mathbf{Z}}={\\mathbf{z}})d{\\mathbb{U}} O Teorema 4.30 indica que o cálculo de probabilidades contrafactuais pode ser dividido em duas etapas. Primeiramente, calcula-se a nova distribuição dos ruídos locais, {\\mathbb{U}} , após aprender que {\\mathbf{Z}}={\\mathbf{z}} , obtendo assim f({\\mathbb{U}}|{\\mathbf{Z}}={\\mathbf{z}}) . A seguir, calcula-se {\\mathbb{P}}({\\mathbf{Y}}_{{\\mathbf{X}}={\\mathbf{x}}}\\leq{\\mathbf{y}}) utilizando-se que a distribuição de {\\mathbb{U}} é f({\\mathbb{U}}|{\\mathbf{Z}}={\\mathbf{z}}) ao invés de f({\\mathbb{U}}) . Na próxima seção, veremos uma aplicação mais detalhada de contrafactuais no Direito. Esta discussão é baseada em Stern2019. 10.1 Contrafactuais e Responsabilidade Civil O art. 927 do Código Civil de 2002 determina que: Aquele que, por ato ilícito, causar dano a outrem, fica obrigado a repará-lo. Este artigo institui a Responsabilidade Aquiliana, isto é o dever daquele que causa um dano de forma ilícita a reparar as vítimas. Sem entrar em detalhes sobre a lei, um elemento fundamental deste conceito é a reparação. O que é a reparação? Esta pergunta é ainda mais complicada dado que o Direito somente pode exigir que esta reparação ocorra por meio de um pagamento em dinheiro. Neste sentido, uma posição comum é definir que reparação é o valor de pagamento que torna a vítima inteira, isto é, na condição em que ela estaria caso não tivesse sofrido o dano. Para nossos fins, a parte central desta definição de reparação é que ela exige uma consideração contrafactual. Isto é, ela questiona qual teria sido a condição da vítima caso esta não tivesse sofrido o dano. Podemos avaliar esta questão com as ferramentas desenvolvidas na Seção 10. Para tal, construíremos um modelo de resultados potenciais. Considere que {\\mathcal{V}} é o conjunto de todas as variáveis juridicamente relevantes. Dentre elas, I é a indicadora de que o causador do dano agiu de forma ilícita. Além disso, E\\subseteq{\\mathcal{V}} é um conjunto de variáveis que foi observada e trazida como evidência à Justiça. Finalmente, U({\\mathcal{V}},m) é uma função de utilidade que indica o quão desejável é para a vítima obter o resultado {\\mathcal{V}} e, além disso, uma indenização monetária de m . Dentro deste contexto, uma quantificação do dano é uma função, Q(E)\\in\\Re^{+} , que define uma compensação para cada possível evidência apresentada. Dentro deste contexto, uma primeira pergunta é o quanto da evidência pode ser usada para quantificar o dano. Neste aspecto, há uma tensão importante entre previsibilidade e reparação (Fisher1990). Considere que um diário com a assinatura de Janis Joplin é destruído em um sebo antes da fama da cantora. Poderia a posterior fama de Joplin ser levada em consideração na reparação? De forma mais geral, quanto da evidência conhecida no momento do julgamento pode ser usada na quantificação do dano? De uma perspectiva social, cada possível resposta apresenta vantagens e desvantagens. Por um lado, o uso de muita informação permite que seja possível colocar a vítima no presente o mais próximo da condição em que ela estaria caso o ato ilícito não tivesse ocorrido. Por outro lado, quanto mais informação é usada, mais imprevisível se torna o valor da reparação no momento em que o ilícito é cometido. Formalmente, podemos imagimar ao menos três respostas para a pergunta levantada. A primeira resposta é de que toda evidência pode ser usada. Isto é, não há restrições sobre Q . A segunda resposta é de que nenhuma evidência pode ser usada, isto é, Q deve ser uma função constante. Finalmente, uma resposta intermediária é obtida supondo que Q somente leva em consideração se a situação da vítima foi superior ou inferior àquela que ela poderia esperar sem o ilícito. Formalmente, tomando g(E)={\\mathbb{I}}({\\mathbb{E}}[U({\\mathcal{V}},0)|E]>{\\mathbb{E}}[U({\\mathcal{% V}}_{X=0},0)]) , temos que se g(E_{1})=g(E_{2}) , então Q(E_{1})=Q(E_{2}) . Em última análise, a escolha da resposta correta não é uma questão científica, mas de Direito. Contudo, utilizando contrafactuais podemos analisá-la e indicar os aspectos fundamentais envolvidos na escolha. Um outro aspecto fundamental na quantificação de danos é a especificação de Q . Para tal, há duas possíveis interpretações de reparação. Por um lado, pode-se entender que a reparação deve levar a vítima o mais próximo possível ao estado em que ela estaria sem o ato ilícito, levando em consideração toda a evidência disponível. Uma possível formalização deste raciocínio é: \\displaystyle Q=\\arg\\min_{Q^{*}}{\\mathbb{E}}[(U({\\mathcal{V}},Q^{*}(E))-U({% \\mathcal{V}}_{X=0},0))^{2}] Por outro lado, pode-se entender que, no momento em que o ato ilícito foi cometido, a vítima deveria estar indiferente entre não sofrer o ilícito ou sofrê-lo e receber a indenização. Esta outra interpretação pode ser formalizada da seguinte forma: \\displaystyle Q=\\arg\\min_{Q^{*}:{\\mathbb{E}}[U({\\mathcal{V}},Q^{*})]={\\mathbb{% E}}[U({\\mathcal{V}}_{X=0},0)]}{\\mathbb{E}}[(U({\\mathcal{V}},Q^{*}(E))-U({% \\mathcal{V}}_{X=0},0))^{2}] Finalmente, como estabelecer um modelo de resultados potenciais a partir de um modelo causal? Como vimos anteriormente, um mesmo CM  pode ser compatível com diversos SCM. Além disso, cada SCM pode levar a avaliações distintas de contrafactuais. Uma possível resposta é que o SCM deve ser estabelecido pelo perito. Contudo, o custo de um perito pode ser incompatível com o valor do pedido. Neste caso, algumas alternativas são supor que {\\mathcal{V}} e {\\mathcal{V}}_{X=0} são independentes ou tomar a sua dependência de forma a minimizar {\\mathbb{E}}[(U({\\mathcal{V}},0)-U({\\mathcal{V}}_{X=0},0))^{2}] . 10.2 Exercícios Exercício 4.31. Com base na discussão levantada na Seção 10.1, discuta formas de reparação para o caso levantado no Exemplo 4.28. Exercício 4.32. Suponha que sem o ato ilícito, um paciente tem o tempo de sobrevivência dado pela acumulada F_{0} . Por outro lado, com o ato ilícito, seu tempo de sobrevivência é dado por F_{1} . Ocorrido o ilícito, o paciente falece após 1 ano. Discuta possíveis reparações para o paciente com base na Seção 10.1. Previous page Next page"],[["index.html","chapter5.html","S11.html"],"11 Identificabilidade na Descoberta Causal ‣ Capítulo 5 Descoberta Causal ‣ Inferência Causal","Skip to content. Identificabilidade na Descoberta Causal 11 Identificabilidade na Descoberta Causal Para contextualizar o problema da identificabilidade na Descoberta Causal, podemos começar com um exemplo simplista. Considere que observamos as variáveis: {\\mathcal{V}}=\\{X,Y\\} . Neste caso, há pelo menos duas dificuldades. Primeiramente, considere que f é tal que X\\perp\\!\\!\\!\\!\\perp^{f}Y . Neste caso, apesar de intuitivamente considerarmos o grafo causal, X\\hskip 8.53581ptY , a densidade f também é compatível com X\\rightarrow Y . De fato, X\\rightarrow Y determina que f(x,y)=f(x)f(y|x) , o que é verdadeiro mesmo quando X e Y são independentes. Contudo, ainda que ambos os grafos sejam compatíveis com f , o grafo X\\hskip 8.53581ptY é mais informativo. Somente com base neste grafo podemos deduzir que X é independente de Y . Essa constatação elucida que não buscamos apenas um grafo causal, {\\mathcal{G}} , compatível com a distribuição geradora dos dados, f . Desejamos que {\\mathcal{G}} permita deduzir o maior número possível de relações de independência condicional presentes em f . Uma maneira de lidar com este problema é trocar o objetivo de encontrar {\\mathcal{G}} compatível com f por encontrar {\\mathcal{G}} fiel a f : Definição 5.1. Dizemos que f é fiel a {\\mathcal{G}} ou, equivalentemente, que {\\mathcal{G}} é fiel a f , se para quaisquer {\\mathbf{X}},{\\mathbf{Y}},{\\mathbf{Z}}\\subseteq{\\mathcal{V}} , {\\mathbf{X}}\\perp\\!\\!\\!\\!\\perp^{f}{\\mathbf{Y}}|{\\mathbf{Z}} se e somente se {\\mathbf{X}}\\perp^{\\mathcal{G}}{\\mathbf{Y}}|{\\mathbf{Z}} . Podemos interpretar a Definição 5.1 à luz do Teorema 2.49. Se {\\mathcal{G}} é compatível com f e {\\mathbf{X}}\\perp^{\\mathcal{G}}{\\mathbf{Y}}|{\\mathbf{Z}} , então o Teorema 2.49 indica que {\\mathbf{X}}\\perp\\!\\!\\!\\!\\perp^{f}{\\mathbf{Y}}|{\\mathbf{Z}} . Contudo, em geral não é possível inferir uma d-separação em um grafo compatível a partir de uma relação de independência condicional. Quando {\\mathcal{G}} é fiel a f , toda relação de independência condicional implica uma d-separação em {\\mathcal{G}} . Isto é, {\\mathcal{G}} traz a maior informação possível sobre as relações de independência condicional em f . O segundo problema de identificabilidade é que pode haver mais de um grafo causal fiel à distribuição geradora dos dados. Como ilustração, considere novamente que {\\mathcal{V}}=\\{X,Y\\} e que f é tal que X e Y não são independentes. Neste caso, tanto X\\rightarrow Y quanto X\\leftarrow Y são fiéis a f . Em outras palavras, se não fizermos mais suposições, ambos os grafos explicam igualmente bem f . Com base nesta observação, podemos definir um tipo de não-identificabilidade em descoberta causal. Se dois grafos causais são fiéis às mesmas distribuições, então não importa qual seja a densidade geradora dos dados, não é possível diferenciá-los por meio dos dados. Neste caso, dizemos que os grafos são fielmente indistinguíveis: Definição 5.2. Dois grafos, {\\mathcal{G}} e {\\mathcal{G}}^{*} , são fielmente indistinguíveis ( {\\mathcal{G}}\\sim{\\mathcal{G}}^{*} ) se, para todo f , \\displaystyle\\text{$f$ é fiel a ${\\mathcal{G}}$ se e somente se $f$ é fiel a ${\\mathcal{G}}^{*}$}. A seguir, a Definição 5.4 e o Teorema 5.5 mostram uma operacionalização da relação de indistinguibilidade fiel. Definição 5.3. Dizemos que (V_{1},V_{2},V_{3}) formam um colisor desprotegido em {\\mathcal{G}} se V_{1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}V_{2}\\stackrel{{% \\scriptstyle{\\mathcal{G}}}}{{\\leftarrow}}V_{3} e V_{1} e V_{3} não são adjacentes em {\\mathcal{G}} . Definição 5.4. Os DAGs {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão se: 1.​Para quaisquer vértices V_{1} e V_{2} , V_{1} e V_{2} são adjacentes em {\\mathcal{G}} se e somente se V_{1} e V_{2} são adjacentes em {\\mathcal{G}}^{*} . 2.​ (V_{1},V_{2},V_{3}) é um colisor desprotegido em {\\mathcal{G}} se e somente se é um colisor desprotegido em {\\mathcal{G}}^{*} . Teorema 5.5 (Verma2022). {\\mathcal{G}}\\sim{\\mathcal{G}}^{*} se e somente se {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão. De forma geral, o Teorema 5.5 mostra que, sem maiores suposições, só é possível aprender o padrão de um grafo a partir dos dados. Isto é, é possível determinar todas as relações de adjacência e, também, os colisores desprotegidos existentes no grafo. A seguir, estudaremos algoritmos que se propõem a descobrir esse padrão. Previous page Next page"],[["index.html","chapter5.html","S12.html"],"12 Algoritmos de Descoberta Causal ‣ Capítulo 5 Descoberta Causal ‣ Inferência Causal","Skip to content. Algoritmos de Descoberta Causal 12 Algoritmos de Descoberta Causal 12.1 Algoritmos baseados em testes de independência condicional 12.1.1 Algoritmo SGS Algoritmo 5.6. Algoritmo SGS 1.​Inicie um grafo não direcionado completo em {\\mathcal{V}} . 2.​Para cada V_{1},V_{2}\\in{\\mathcal{V}} , se H_{0}:V_{1}\\perp\\!\\!\\!\\!\\perp V_{2}|S é rejeitada para algum S\\subseteq{\\mathcal{V}}-\\{V_{1},V_{2}\\} , retire a aresta \\{V_{1},V_{2}\\} . 3.​Para todos V_{1},V_{2},V_{3}\\in{\\mathcal{V}} tais que \\{V_{1},V_{2}\\} e \\{V_{2},V_{3}\\} são adjacentes, mas \\{V_{1},V_{3}\\} não o são, caso exista S\\subseteq\\{V_{2}\\}\\cup{\\mathcal{V}}-\\{V_{1},V_{3}\\} tal que H_{0}:V_{1}\\perp\\!\\!\\!\\!\\perp V_{2}|S não é rejeitada, direcione as arestas V_{1}\\rightarrow V_{2}\\leftarrow V_{3} . 4.​Repetir até que não haja mudanças: (a)​Se A\\rightarrow B , B e C são adjacentes e não direcionados e A e C não são adjacentes, direcione B\\rightarrow C . (b)​Se B é descendente de A e \\{A,B\\} são adjacentes, direcione A\\rightarrow B . 12.1.2 Algoritmo PC Definição 5.7. Adj_{{\\mathcal{G}}}(X) é o conjunto de vértices em {\\mathcal{V}} que é adjacente a X em {\\mathcal{G}} . Algoritmo 5.8. Algoritmo PC 1.​Inicie um grafo não direcionado completo em {\\mathcal{V}} , {\\mathcal{G}} . 2.​Inicialize n=0 . 3.​Enquanto houver um vértice X\\in{\\mathcal{V}} com |Adj_{{\\mathcal{G}}}(X)|>n+1 : (a)​Para cada X\\in{\\mathcal{V}} com |Adj_{{\\mathcal{G}}}(X)|\\geq n+1 , cada Y\\in Adj_{{\\mathcal{G}}}(X) e S\\subseteq Adj_{{\\mathcal{G}}}(X)-\\{Y\\} em que |S|=n , se H_{0}:X\\perp\\!\\!\\!\\!\\perp Y|S é rejeitada, então retire a aresta \\{X,Y\\} de {\\mathcal{G}} e defina Sep(\\{X,Y\\})=S . (b)​Defina n=n+1 . 4.​Para cada X,Y,Z\\in{\\mathcal{V}} tais que \\{X,Y\\} e \\{Y,Z\\} são adjacentes, mas \\{X,Z\\} não o são e Y\\notin Sep(\\{X,Z\\}) , oriente X\\rightarrow Y\\leftarrow Z . 5.​Repetir até que não haja mudanças: (a)​Se A\\rightarrow B , B e C são adjacentes e não direcionados e A e C não são adjacentes, direcione B\\rightarrow C . (b)​Se B é descendente de A e \\{A,B\\} são adjacentes, direcione A\\rightarrow B . Previous page Next page"],[["index.html","chapter2.html","S2.html"],"2 Elementos de Modelos Probabilísticos em Grafos ‣ Capítulo 2 Modelo Causal (CM) ‣ Inferência Causal","Skip to content. Elementos de Modelos Probabilísticos em Grafos 2 Elementos de Modelos Probabilísticos em Grafos 2.1 Grafo Direcionado Definição 2.1. Um grafo direcionado, {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) , é composto por um conjunto de vértices, {\\mathcal{V}}=\\{V_{1},\\ldots,V_{n}\\} , e e um conjunto de arestas, {\\mathcal{E}}=\\{E_{1},\\ldots,E_{m}\\} , onde cada aresta é um par ordenado de vértices, isto é, E_{i}\\in{\\mathcal{V}}^{2} . Para auxiliar nossa intuição sobre a Definição 2.1, é comum representarmos o grafo por meio de uma figura. Nesta, representamos cada vértice por meio de um ponto. Além disso, para cada aresta, (V_{i},V_{j}) , traçamos uma seta que aponta de V_{i} para V_{j} . Por exemplo, considere que os vértices são {\\mathcal{V}}=\\{V_{1},V_{2},V_{3}\\} e as arestas são {\\mathcal{E}}=\\{(V_{1},V_{2}),(V_{1},V_{3}),(V_{2},V_{3})\\} . Neste caso, teremos os 3 pontos como vértices e, além disso, traçaremos setas de V_{1} para V_{2} e para V_{3} e, também, de V_{2} para V_{3} . Podemos desenhar este grafo utilizando os pacotes dagitty e ggdag (Barrett2022, Textor2016): \\MakeFramed library(dagitty) library(ggdag) library(ggplot2) # Especificar o grafo grafo <- dagitty(\"dag { V1 -> { V2 V3 } V2 -> V3 }\") # Exibir a figura do grafo ggdag(grafo, layout = \"circle\") + theme(axis.text.x = element˙blank(), axis.ticks.x = element˙blank(), axis.text.y = element˙blank(), axis.ticks.y = element˙blank()) + xlab() + ylab() \\endMakeFramed Refer to caption Figura 1: Exemplo de grafo. Grafos direcionados serão úteis para representar causalidade pois seus vértices serão variáveis e suas arestas irão apontar de cada causa imediata para seu efeito. Por exemplo, no Capítulo 1 consideramos um caso em que Sexo e Tratamento são causas imediatas de recuperação e, além disso, Sexo é causa imediata de Tratamento. O grafo na figur 1 poderia representar estas relações se definirmos que V_{1} é Sexo, V_{2} é Tratamento e V_{3} é Recuperação. Usando a representação de um grafo, podemos imaginar caminhos sobre ele. Um caminho direcionado inicia-se em um determinado vértice e, seguindo a direção das setas, vai de um vértice para outro. Por exemplo, (V_{1},V_{2},V_{3}) é um caminho direcionado na figur 1, pois existe uma seta de V_{1} para V_{2} e de V_{2} para V_{3} . É comum denotarmos este caminho direcionado por V_{1}\\rightarrow V_{2}\\rightarrow V_{3} . Similarmente, (V_{1},V_{3},V_{2}) não é um caminho direcionado, pois não existe seta de V_{3} para V_{2} . A definição de caminho direcionado é formalizada a seguir: Definição 2.2. Um caminho direcionado é uma sequência de vértices em um grafo direcionado, C=\\{C_{1},\\ldots,C_{n}\\} tal que, para cada 1\\leq i<n , (C_{i},C_{i+1})\\in{\\mathcal{E}} . Definição 2.3. Dizemos que V_{2} é descendente de V_{1} se existe um caminho direcionado de V_{1} em V_{2} . Um caminho é uma generalização de caminho direcionado. Em um caminho, começamos em um vértice e, seguindo por setas, mas não necessariamente na direção em que elas apontam, vamos de um vértice para outro. Por exemplo, na figur 1 vimos que (V_{1},V_{3},V_{2}) não é um caminho direcionado pois não existe seta de V_{3} para V_{2} . Contudo, (V_{1},V_{3},V_{2}) é um caminho pois existe uma seta ligando V_{3} e V_{2} , a seta que aponta de V_{2} para V_{3} . É comum representarmos este caminho por V_{1}\\rightarrow V_{3}\\leftarrow V_{2} . Caminho é formalizado a seguir: Definição 2.4. Dizemos que vértices V_{1} e V_{2} são adjacentes se (V_{1},V_{2})\\in{\\mathcal{E}} ou (V_{2},V_{1})\\in{\\mathcal{E}} . Definição 2.5. Um caminho é uma sequência de vértices, C=\\{C_{1},\\ldots,C_{n}\\} tal que, para cada 1\\leq i<n , C_{i} e C_{i+1}) são adjacentes. 2.2 Grafo Direcionado Acíclico (DAG) Um DAG é um grafo direcionado tal que, para todo vértice, V , não é possível seguir setas partindo de V e voltar para V . Este conceito é formalizado a seguir: Definição 2.6. Um grafo direcionado acíclico (DAG) é um grafo direcionado, {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) , tal que, para todo vértice, V\\in{\\mathcal{V}} , não existe um caminho direcionado, C=\\{C_{1},\\ldots,C_{n}\\} tal que C_{1}=V=C_{n} . Usualmente representaremos as relações causais por meio de um DAG. Especificamente, existirá uma aresta de V_{1} para V_{2} para indicar que V_{1} é causa imediata de V_{2} . Caso um grafo direcionado não seja um DAG, então existe um caminho de V em V , isto é, V seria uma causa de si mesma, o que desejamos evitar. Um DAG induz uma ordem parcial entre os seus vértices. Isto é, se existe uma aresta de V_{1} para V_{2} , então podemos interpretar que V_{1} antecede V_{2} causalmente. Com base nesta ordem parcial, é possível construir diversas definições que nos serão úteis. Dizemos que V_{1} é pai de V_{2} em um DAG, {\\mathcal{G}} , se existe uma aresta de V_{1} a V_{2} , isto é, (V_{1},V_{2})\\in{\\mathcal{E}} . Denotamos por Pa(V) o conjunto de todos os pais de V . Similarmente Pa({\\mathbb{V}}) é o conjunto de vértices que são pais de algum vértice em {\\mathbb{V}} : Definição 2.7. O conjunto de pais de {\\mathbb{V}}\\subseteq{\\mathcal{V}} em um DAG, {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) , é: Pa({\\mathbb{V}}):=\\{V^{*}\\in{\\mathcal{V}}:\\exists V\\in{\\mathbb{V}}\\text{ tal % que }(V^{*},V)\\in{\\mathcal{E}}\\}. Similarmente, dizemos que V_{1} é um ancestral de V_{2} em um DAG, se V_{1} antecede V_{2} causalmente. Isto é, se V_{1} é pai de V_{2} ou, pai de pai de V_{2} , ou pai de pai de pai de V_{2} , e assim por diante \\ldots Denotamos por Anc({\\mathbb{V}}) o conjunto de todos os ancestrais de elementos de {\\mathbb{V}} : Definição 2.8. Em um DAG, {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) , o conjunto de ancestrais de {\\mathbb{V}}\\subseteq{\\mathcal{V}} , Anc({\\mathbb{V}}) , é tal que Anc({\\mathbb{V}})\\subseteq{\\mathcal{V}} e V^{*}\\in Anc({\\mathbb{V}}) se e somente se existe V\\in{\\mathbb{V}} e um caminho direcionado, C, tal que C_{1}=V^{*} e C_{i}=V . Note que podemos interpretar Anc({\\mathbb{V}}) como o conjunto de todas as causas diretas e indiretas de {\\mathbb{V}} . Finalmente, diremos que um conjunto de vértices, {\\mathcal{A}}\\subseteq{\\mathcal{V}} é ancestral em um DAG, se não existe algum vértice fora de {\\mathcal{A}} que seja pai de algum vértice em {\\mathcal{A}} . Segundo nossa interpretação causal, {\\mathcal{A}} será ancestral quando nenhum vértice fora de {\\mathcal{A}} é causa direta de algum vértice em {\\mathcal{A}} : Definição 2.9. Dizemos que {\\mathcal{A}}\\subseteq{\\mathcal{V}} é ancestral em um DAG se, para todo vértice V\\in{\\mathcal{A}} , temos que Pa(V)\\subseteq{\\mathcal{A}} . Lema 2.10. Em um DAG, {\\mathcal{G}} , para todo {\\mathbb{V}}\\subseteq{\\mathcal{V}} , Anc({\\mathbb{V}}) é ancestral. 2.3 Modelo Probabilístico em um DAG Um modelo probabilístico em um DAG é tal que cada um dos vértices é uma variável aleatória. O DAG será usado para descrever relações de independência condicional existentes entre estas variáveis. Mais especificamente, cada vértice será independente dos demais vértices dados os seus pais. Uma maneira alternativa de pensar sobre esta afirmação é imaginar que cada vértice é gerado somente pelos seus pais. Esta intuição é formalizada em Definição 2.11: Definição 2.11. Para {\\mathcal{V}} um conjunto de variáveis aleatórias, dizemos que uma função de densidade sobre {\\mathcal{V}} , f , é compatível com um DAG, {\\mathcal{G}} , se: f(v_{1},\\ldots,v_{n})=\\prod_{i=1}^{n}f(v_{i}|Pa(v_{i})). Quando não há ambiguidade, também dizemos que {\\mathcal{G}} é compatível com f neste caso. Exemplo 2.12. Considere que X\\sim\\text{Bernoulli}(0.5) , Y|X=1\\sim\\text{Bernoulli}(0.99) e Y|X=0\\sim\\text{Bernoulli}(0.01) . Neste caso, \\displaystyle f(Y=1) \\displaystyle=f(X=0,Y=1)+f(X=1,Y=1) \\displaystyle=f(X=0)f(Y=1|X=0)+f(X=1)f(Y=1|X=1) \\displaystyle=0.5\\cdot 0.01+0.5\\cdot 0.99=0.5 Como f(X=1,Y=1)=0.5\\cdot 0.99\\neq 0.5\\cdot 0.5=f(X=1)f(Y=1) , decorre da Definição 2.11 que f não é compatível com o DAG sem arestas em que {\\mathcal{V}}=\\{X,Y\\} . Em outras palavras, X e Y não são independentes. Como sempre é verdade que f(x,y)=f(x)f(y|x) e que f(x,y)=f(y)f(x|y) , f é compatível com os DAGs X\\rightarrow Y e com X\\leftarrow Y . Exemplo 2.13. Considere que f(x,y)=f(x)f(y) . Isto é, (X,Y) são independentes segundo f . Neste caso, f é compatível com qualquer DAG sobre {\\mathcal{V}}=\\{X,Y\\} . Quando {\\mathcal{V}} tem muitos elementos, pode ser difícil verificar se a Definição 2.11 está satisfeita Para esses casos, pode ser útil aplicar o Lema 2.14: Lema 2.14. Uma função de densidade, f , é compatível com um DAG, {\\mathcal{G}} , se e somente se, existem funções, g_{1},\\ldots,g_{n} tais que: \\displaystyle f(v_{1},\\ldots,v_{n}) \\displaystyle=\\prod_{i=1}^{n}g_{i}(v_{i},Pa(v_{i}))\\text{, e} \\displaystyle\\int g_{i}(v_{i},Pa(v_{i}))dv_{i}=1 Exemplo 2.15. Considere que \\displaystyle f(x_{1},x_{2},x_{3}) \\displaystyle=0.5\\cdot 0.9^{{\\mathbb{I}}(x_{1}=x_{2})}\\cdot 0.1^{{\\mathbb{I}}(% x_{1}\\neq x_{2})}\\cdot 0.8^{{\\mathbb{I}}(x_{2}=x_{3})}\\cdot 0.2^{{\\mathbb{I}}(% x_{2}\\neq x_{3})}. Tome {\\mathcal{G}}=X_{1}\\rightarrow X_{2}\\rightarrow X_{3} . Para {\\mathcal{G}} , Pa(X_{1})=\\emptyset , Pa(X_{2})=\\{X_{1}\\} e Pa(X_{3})=\\{X_{2}\\} . Assim, tomando g_{1}(x_{1},Pa(x_{1}))=0.5 , g_{2}(x_{2},Pa(x_{2}))=0.9^{{\\mathbb{I}}(x_{1}=x_{2})}\\cdot 0.1^{{\\mathbb{I}}(% x_{1}\\neq x_{2})} e g_{3}(x_{3},Pa(x_{3}))=0.8^{{\\mathbb{I}}(x_{2}=x_{3})}\\cdot 0.2^{{\\mathbb{I}}(% x_{2}\\neq x_{3})} , temos que \\displaystyle f(x_{1},x_{2},x_{3}) \\displaystyle=g_{1}(x_{1},Pa(x_{1}))\\cdot g_{2}(x_{2},Pa(x_{2}))\\cdot g_{3}(x_% {3},Pa(x_{3})) Isto é, decorre do Lema 2.14 que f é compatível com {\\mathcal{G}} . Exercício 2.16. Usando a mesma f do Exemplo 2.15, prove que f é compatível com o DAG X_{1}\\leftarrow X_{2}\\leftarrow X_{3} . Temos que f é compatível com quais outros DAG’s? Se {\\mathcal{A}} é ancestral em um DAG, então f({\\mathcal{A}}) pode ser decomposto de forma similar a f({\\mathcal{V}}) . Este fato será útil e é formalizado no Lema 2.17. Lema 2.17. Seja {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) um DAG. Se {\\mathcal{A}} é ancestral e f é compatível com {\\mathcal{G}} , então \\displaystyle f({\\mathcal{A}})=\\prod_{V\\in{\\mathcal{A}}}f(V|Pa(V)) A seguir, estudaremos três tipos fundamentais de modelos probabilísticos em DAG’s com 3 vértices. A intuição obtida a partir destes exemplos continuará valendo quando estudarmos grafos mais gerais. 2.4 Exemplos de Modelo Probabilístico em um DAG Nos exemplos a seguir, considere que {\\mathcal{V}}=(V_{1},V_{2},V_{3}) . 2.4.1 Confundidor (Confounder) No modelo de confundidor, as únicas duas arestas são (V_{2},V_{1}) e (V_{2},V_{3}) . Uma ilustração de um confundidor pode ser encontrada na figur 2. O modelo de confundidor pode ser usado quando acreditamos que V_{2} é uma causa comum a V_{1} e a V_{3} . Além disso, V_{1} não é causa imediata de V_{3} nem vice-versa. Refer to caption Figura 2: Ilustração de confundidor. Em um modelo de confundidor a relação de dependência entre V_{1} e V_{3} é explicada pelos resultados a seguir: Lema 2.18. Para qualquer probabilidade compatível com o DAG na figur 2, V_{1}\\perp\\!\\!\\!\\!\\perp V_{3}|V_{2} . Lema 2.19. Existe ao menos uma probabilidade compatível com o DAG na figur 2 tal que V_{1}\\not\\perp\\!\\!\\!\\!\\perp V_{3} . Combinando os Lemas 2.18 e 2.19 é possível compreender melhor como usaremos confundidores num contexto causal. Nestes casos, V_{2} será uma causa comum a V_{1} e a V_{3} . Esta causa comum torna V_{1} e V_{3} associados, ainda que nenhum seja causa direta ou indireta do outro. Podemos contextualizar estas ideias em um caso de diagnóstico de dengue. Considere que V_{2} é a indicadora de que um indivíduo tem dengue, e V_{1} e V_{3} são indicadoras de sintomas típicos de dengue, como dor atrás dos olhos e febre. Neste caso, V_{1} e V_{3} tipicamente são associados: caso um paciente tenha febre, aumenta a probabilidade de que tenha dengue e, portanto, aumenta a probabilidade de que tenha dor atrás dos olhos. Contudo, apesar dessa associação V_{3} não tem influência causal sobre V_{1} . Se aumentarmos a temperatura corporal do indivíduo, não aumentará a probabilidade de que ele tenha dor atrás dos olhos. A dengue que causa febre, não o contrário. 2.4.2 Cadeia (Chain) No modelo de cadeia, as únicas duas arestas são (V_{1},V_{2}) e (V_{2},V_{3}) . Uma ilustração de uma cadeia pode ser encontrada na figur 3. Neste modelo, acreditamos que V_{1} é causa de V_{2} que, por sua vez, é causa de V_{3} . Assim, V_{1} é ancestral de V_{3} , isto é, o primeiro é causa indireta do segundo. Refer to caption Figura 3: Ilustração de cadeia. Em um modelo de cadeia a relação de dependência entre V_{1} e V_{3} é explicada pelos resultados a seguir: Lema 2.20. Para qualquer probabilidade compatível com o DAG na figur 3, V_{1}\\perp\\!\\!\\!\\!\\perp V_{3}|V_{2} . Lema 2.21. Existe ao menos uma probabilidade compatível com o DAG na figur 3 tal que V_{1}\\not\\perp\\!\\!\\!\\!\\perp V_{3} . Combinando os Lemas 2.20 e 2.21 é possível compreender melhor como usaremos cadeias num contexto causal. Nestes casos, V_{2} será uma consequência de V_{1} e uma causa de V_{3} . Assim, a cadeia torna V_{1} e V_{3} e associados, ainda que nenhum seja causa direta do outro. Contudo, ao contrário do confundidor, neste caso V_{1} é uma causa indireta de V_{3} , isto é, tem influência causal sobre V_{3} . Para contextualizar estas ideias, considere que V_{1} é a indicadora de consumo elevado de sal, V_{2} é a indicadora de pressão alta, e V_{3} é a indicadora de ocorrência de um derrame. Como consumo elevado de sal causa pressão alta e pressão alta tem influência causal sobre a ocorrência de um derrame, pressão alta é uma cadeia que é um mediador entre consumo elevado de sal e ocorrência de derrame. Assim, consumo elevado de sal tem influência causal sobre a ocorrência de derrame. 2.4.3 Colisor (Collider) O último exemplo de DAG com 3 vértices que estudaremos é o de modelo de colisor, em que as únicas duas arestas são (V_{1},V_{2}) e (V_{3},V_{2}) . Uma ilustração de um colisor pode ser encontrada na figur 4. O modelo de colisor pode ser usado quando acreditamos que V_{1} e V_{3} são causas comuns a V_{2} . Além disso, V_{1} não é causa imediata de V_{3} nem vice-versa. Refer to caption Figura 4: Ilustração de colisor. Em um modelo de colisor a relação de dependência entre V_{1} e V_{3} é explicada pelos resultados a seguir: Lema 2.22. Para qualquer probabilidade compatível com o DAG na figur 4, V_{1}\\perp\\!\\!\\!\\!\\perp V_{3} . Lema 2.23. Existe ao menos uma probabilidade compatível com o DAG na figur 4 tal que V_{1}\\not\\perp\\!\\!\\!\\!\\perp V_{3}|V_{2} . Combinando os Lemas 2.22 e 2.23 vemos como utilizaremos confundidores num contexto causal. Nestes casos, V_{1} e V_{3} serão causas comuns e independentes de V_{2} . Uma vez que obtemos informação sobre o efeito comum, V_{2} , V_{1} e V_{3} passam a ser associados. Esse modelo pode ser contextualizado observando a prevalência de doenças em uma determinada população (Sackett1979). Considere que V_{1} e V_{3} são indicadoras de que um indivíduo tem doenças que ocorrem independentemente na população. Além disso, V_{2} é a indicadora de que o indíviduo foi hospitalizado, isto é, V_{2} é influeciado causalmente tanto por V_{1} quanto por V_{3} . Para facilitar as contas envolvidas, desenvolveremos o exemplo com distribuições fictícias. Considere que V_{1} e V_{3} são independentes e tem distribuição Bernoulli(0.05). Além disso, quanto maior o número de doenças, maior a probabilidade de o indíviduo ser hospitalizado. Por exemplo, {\\mathbb{P}}(V_{2}=1|V_{1}=0,V_{3}=0)=0.01 , {\\mathbb{P}}(V_{2}=1|V_{1}=0,V_{3}=1)=0.1 , {\\mathbb{P}}(V_{2}=1|V_{1}=1,V_{3}=0)=0.1 , e {\\mathbb{P}}(V_{2}=1|V_{1}=1,V_{3}=1)=0.5 . Com base nestas especificações, podemos verificar se V_{1} e V_{3} estão associados quando V_{2}=1 . Para tal, primeiramente calcularemos algumas probabilidades conjuntas que serão úteis: \\displaystyle\\begin{cases}{\\mathbb{P}}(V_{1}=0,V_{2}=1,V_{3}=0)&=0.95\\cdot 0.0% 1\\cdot 0.95=0.009025\\\\ {\\mathbb{P}}(V_{1}=0,V_{2}=1,V_{3}=1)&=0.95\\cdot 0.1\\cdot 0.05=0.0475\\\\ {\\mathbb{P}}(V_{1}=1,V_{2}=1,V_{3}=0)&=0.05\\cdot 0.1\\cdot 0.95=0.0475\\\\ {\\mathbb{P}}(V_{1}=1,V_{2}=1,V_{3}=1)&=0.05\\cdot 0.5\\cdot 0.05=0.00125\\end{cases} (1) Com base nestes cálculos é possível obter a prevalência da doença dentre os indivíduos hospitalizados: \\displaystyle{\\mathbb{P}}(V_{1}=1|V_{2}=1) \\displaystyle=\\frac{{\\mathbb{P}}(V_{1}=1,V_{2}=1)}{{\\mathbb{P}}(V_{2}=1)} \\displaystyle=\\frac{0.0475+0.00125}{0.009025+0.0475+0.0475+0.00125} \\displaystyle\\approx 0.46 Finalmente, \\displaystyle{\\mathbb{P}}(V_{1}=1|V_{2}=1,V_{3}=1) \\displaystyle=\\frac{{\\mathbb{P}}(V_{1}=1,V_{2}=1,V_{3}=1)}{{\\mathbb{P}}(V_{2}=% 1,V_{3}=1)} \\displaystyle=\\frac{{\\mathbb{P}}(V_{1}=1,V_{2}=1,V_{3}=1)}{{\\mathbb{P}}(V_{1}=% 0,V_{2}=1,V_{3}=1)+{\\mathbb{P}}(V_{1}=1,V_{2}=1,V_{3}=1)} \\displaystyle=\\frac{0.00125}{0.0475+0.00125} \\displaystyle\\approx 0.26 Como {\\mathbb{P}}(V_{1}=1|V_{2}=1)=0.46\\neq 0.26\\approx{\\mathbb{P}}(V_{1}=1|V_{2}=1% ,V_{3}=1) , verificamos que V_{1} não é independente de V_{3} dado V_{2} . De fato, ao observar que um indivíduo está hospitalizado e tem uma das doenças, a probabilidade de que ele tenha a outra doença é inferior àquela obtida se soubéssemos apenas que o indivíduo está hospitalizado. Esta observação não implica que uma doença tenha influência causal sobre a outra. Note que a frequência de hospitalização aumenta drasticamente quando um indivíduo tem ao menos uma das doenças. Além disso, cada uma das doenças é relativamente rara na população geral. Assim, dentre os indíviduos hospitalizados, a frequência daqueles que tem somente uma das doenças é maior do que seria caso as doenças não estivessem associadas. Quando fixamos o valor de uma consequência comum (hospitalização), as causas (doenças) passam a ser associadas. Esta associação não significa que infectar um indivíduo com uma das doenças reduz a probabilidade que ele tenha a outra. 2.5 Modelo Causal (Causal Model) Com base nos conceitos abordados anteriormente, finalmente podemos definir o Modelo Causal (CM ): Definição 2.24. Um CM é um par ({\\mathcal{G}},f) tal que {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) é um DAG (Definição 2.6) e f é uma função de densidade sobre {\\mathcal{V}} compatível com {\\mathcal{G}} (Definição 2.11). Neste caso, é comum chamarmos {\\mathcal{G}} de grafo causal do CM ({\\mathcal{G}},f) . Note pela Definição 2.24 que um CM é formalmente um modelo probabilístico em um DAG. O principal atributo de um CM que o diferencia de um modelo probabilístico genérico em um DAG é como o interpretamos. Existe uma aresta de V_{1} em V_{2} em um CM se e somente se V_{1} é uma causa direta de V_{2} . Dentre os modelos causais, é de particular interesse o modelo linear Gaussiano. Definição 2.25. Dizemos que ({\\mathcal{G}},f) é um CM linear Gaussiano de parâmetros \\mu e \\beta se, existe matriz diagonal positiva, \\Sigma , \\mu\\in\\Re^{|{\\mathcal{V}}|} , e \\beta\\in\\Re^{|{\\mathcal{V}}|\\times|{\\mathcal{V}}|} tal que, para todo vértice V , \\beta_{V,W}=0 quando W\\notin Pa(V) e \\displaystyle V|Pa(V) \\displaystyle\\sim N\\left(\\mu_{V}+\\sum_{W\\in Pa(V)}\\beta_{V,W}\\cdot W,\\Sigma_{i% ,i}\\right) O modelo causal linear Gaussiano tem algumas propriedades especiais, que tornam mais simples suas compreensão. Algumas destas são apresentadas abaixo: Lema 2.26. Se ({\\mathcal{G}},f) é um CM linear Gaussiano, então {\\mathcal{V}} segue distribuição normal multivariada. Lema 2.27. Seja ({\\mathcal{G}},f) um CM linear Gaussiano com coeficientes \\beta . Para cada V,Y\\in{\\mathcal{V}} , defina \\mathbb{C}_{V,Y} como o conjunto de todos os caminhos direcionados de V a Y . \\displaystyle{\\mathbb{E}}[Y] \\displaystyle=\\sum_{V\\in\\mathbb{{\\mathcal{V}}}}\\sum_{C\\in\\mathbb{C}_{V,Y}}\\mu_% {V}\\cdot\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{i}} No próximo capítulo estudaremos consequências desta interpretação causal. Contudo, antes disso, a próxima seção desenvolverá um resultado fundamental de modelos probabilísticos em DAGs que será fundamental nos capítulos posteriores. 2.6 Exercícios Exercício 2.28. Em um DAG, {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) , Considere que Anc^{*}({\\mathbb{V}})\\subseteq{\\mathcal{V}} é definido como o menor conjunto tal que {\\mathbb{V}}\\subseteq Anc^{*}({\\mathbb{V}}) e, se V\\in Anc^{*}({\\mathbb{V}}) , então Pa(V)\\subseteq Anc^{*}({\\mathbb{V}}) . Prove que Anc({\\mathbb{V}})\\equiv Anc^{*}({\\mathbb{V}}) . Exercício 2.29. Prove o Lema 2.10. Exercício 2.30. Prove que se {\\mathbf{Z}} é ancestral, então f({\\mathbf{Z}})=\\prod_{Z\\in{\\mathbf{Z}}}f(Z|Pa(Z)) . Exercício 2.31. Sejam {\\mathcal{G}}_{1}=({\\mathcal{V}},{\\mathcal{E}}_{1}) e {\\mathcal{G}}_{2}=({\\mathcal{V}},{\\mathcal{E}}_{2}) grafos tais que {\\mathcal{E}}_{1}\\subseteq{\\mathcal{E}}_{2} . Prove que se f é compatível com {\\mathcal{G}}_{1} , então f é compatível com {\\mathcal{G}}_{2} . Exercício 2.32. Prove o Lema 2.14. Exercício 2.33. Prove o Lema 2.17. Exercício 2.34. Prove que, para qualquer {\\mathbb{V}}\\subseteq{\\mathcal{V}} , Anc({\\mathbb{V}})=Anc(Anc({\\mathbb{V}})) . Exercício 2.35. Prove que {\\mathbb{V}} é ancestral se e somente se Anc({\\mathbb{V}})={\\mathbb{V}} . Exercício 2.36. Considere que (X_{1},X_{2}) são independentes e tais que {\\mathbb{P}}(X_{i}=1)={\\mathbb{P}}(X_{i}=-1)=0.5 . Além disso, Y\\equiv X_{1}\\cdot X_{2} . (a)​Desenhe um DAG compatível com as relações de independência dadas pelo enunciado. (b)​Prove que Y e X_{1} são independentes. Isso contradiz sua resposta para o item anterior? Exercício 2.37. Para cada um dos modelos de confundidor, cadeia e colisor, dê exemplos de situações práticas em que este modelo é razoável. Exercício 2.38. Considere que, dado T , X_{1},\\ldots,X_{n} são i.i.d. e X_{i}|T\\sim\\text{Bernoulli}(T) . Além disso, T\\sim\\text{Beta}(a,b) . (a)​Seja f(t,x_{1},\\ldots,x_{n}) dada pelo enunciado. Exiba um DAG, {\\mathcal{G}} , tal que f é compatível com {\\mathcal{G}} . (b)​ (X_{1},\\ldots,X_{n}) são independentes? (c)​Determine f(x_{1},\\ldots,x_{n}) . Exercício 2.39. Exiba um exemplo em que V_{1} , V_{2} , V_{3} sejam binárias, que V_{2} seja um colisor e que, além disso, Corr[V_{1},V_{3}|V_{2}=1]>0 . Exercício 2.40. Seja {\\mathcal{V}}=(V_{1},V_{2},V_{3}) Exiba um exemplo de f sobre {\\mathcal{V}} e grafos {\\mathcal{G}}_{1} e {\\mathcal{G}}_{2} sobre {\\mathcal{V}} tais que {\\mathcal{G}}_{1}\\neq{\\mathcal{G}}_{2} e f é compatível tanto com {\\mathcal{G}}_{1} quanto com {\\mathcal{G}}_{2} . Exercício 2.41. Seja f uma densidade arbitrária sobre {\\mathcal{V}}=(V_{1},\\ldots,V_{n}) . Exiba um DAG sobre {\\mathcal{V}} , {\\mathcal{G}} , tal que f é compatível com {\\mathcal{G}} . Exercício 2.42. Exiba um exemplo em que V_{2} é um colisor entre V_{1} e V_{2} , V_{4} tem como único pai V_{2} e V_{1} e V_{3} são dependentes dado V_{4} . Exercício 2.43. Prove o Lema 2.26. Previous page Next page"],[["index.html","chapter2.html","S3.html"],"3 Independência Condicional e D-separação ‣ Capítulo 2 Modelo Causal (CM) ‣ Inferência Causal","Skip to content. Independência Condicional e D-separação 3 Independência Condicional e D-separação Independência condicional é uma forma fundamental de indicar relações entre variáveis aleatórias. Se {\\mathbf{X}}_{1},\\ldots,{\\mathbf{X}}_{d} e {\\mathbf{Y}} são vetores de variáveis aleatórias, definimos que ({\\mathbf{X}}_{1},\\ldots,{\\mathbf{X}}_{d})|{\\mathbf{Y}} , isto é, {\\mathbf{X}}_{1},\\ldots,{\\mathbf{X}}_{d} são independentes dado {\\mathbf{Y}} , se conhecido o valor de {\\mathbf{Y}} , observar quaisquer valores de {\\mathbf{X}} não traz informação sobre os demais valores. Nesta seção veremos que as relações de independência condicional em um CM estão diretamente ligadas ao seu grafo. 3.1 Independência Condicional Definição 2.44. Dizemos que ({\\mathbf{X}}_{1},\\ldots,{\\mathbf{X}}_{d}) são independentes dado {\\mathbf{Y}} se, para qualquer {\\mathbf{x}}_{1},\\ldots,{\\mathbf{x}}_{n} e {\\mathbf{y}} , \\displaystyle f({\\mathbf{x}}_{1},\\ldots,{\\mathbf{x}}_{n}|{\\mathbf{y}}) \\displaystyle=\\prod_{i=1}^{d}f({\\mathbf{x}}_{i}|{\\mathbf{y}}) Em particular, ({\\mathbf{X}}_{1},\\ldots,{\\mathbf{X}}_{d}) são independentes se, para quaiquer ({\\mathbf{x}}_{1},\\ldots,{\\mathbf{x}}_{d}) , \\displaystyle f({\\mathbf{x}}_{1},\\ldots,{\\mathbf{x}}_{d}) \\displaystyle=\\prod_{i=1}^{d}f({\\mathbf{x}}_{i}) Verificar se a Definição 2.44 está satisfeita nem sempre é fácil. A princípio, ela exige obter tanto a distribuição condicional conjunta, f({\\mathbf{x}}_{1},\\ldots,{\\mathbf{x}}_{d}|{\\mathbf{y}}) , quanto cada uma das marginais, f({\\mathbf{x}}_{i}|{\\mathbf{y}}) . O Lema 2.45 a seguir apresenta outras condições que são equivalentes a independência condicional: Lema 2.45. As seguintes afirmações são equivalentes: 1.​ ({\\mathbf{X}}_{1},\\ldots,{\\mathbf{X}}_{d}) são independentes dado {\\mathbf{Y}} , 2.​Existem funções, h_{1},\\ldots,h_{d} tais que f({\\mathbf{x}}_{1},\\ldots,{\\mathbf{x}}_{d}|{\\mathbf{y}})=\\prod_{j=1}^{d}{h_{j}% ({\\mathbf{x}}_{j},{\\mathbf{y}})} . 3.​Para todo i , f({\\mathbf{x}}_{i}|{\\mathbf{x}}_{-i},{\\mathbf{y}})=f({\\mathbf{x}}_{i}|{\\mathbf% {y}}) . 4.​Para todo i , f({\\mathbf{x}}_{i}|{\\mathbf{x}}_{1}^{i-1},{\\mathbf{y}})=f({\\mathbf{x}}_{i}|{% \\mathbf{y}}) . As condições no Lema 2.45 são, em geral, mais fáceis de verificar do que a definição direta de independência condicional. A seguir veremos que, em um SMC, pode ser mais fácil ainda verificar muitas das relações de independência condicional. 3.2 D-separação Em um CM , é possível indicar as relações de independência incondicional em {\\mathcal{V}} por meio do grafo associado. Intuitivamente, haverá uma dependência entre V_{1} e V_{2} se for possível transmitir a informação de V_{1} para V_{2} por um caminho que ligue ambos os vértices. Para entender se a informação pode ser transmitida por um caminho, classificaremos a seguir os vértices que o constituem. Definição 2.46. Seja C=(C_{1},\\ldots,C_{n}) um caminho em um DAG, {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) . Para cada 2\\leq i\\leq n-1 : •​ C_{i} é um colisor em C se (C_{i-1},C_{i})\\in{\\mathcal{E}} e (C_{i+1},C_{i})\\in{\\mathcal{E}} , isto é, existem arestas apontando de C_{i-1} e de C_{i+1} para C_{i} . Neste caso, desenhamos C_{i-1}\\rightarrow C_{i}\\leftarrow C_{i+1} . Note que a classificação na Definição 2.46 generaliza os exemplos de DAG’s com 3 vértices na Seção 2.4. Essa classificação é ilustrada com o DAG na figur 5. Existem dois caminhos que vão de V_{1} a V_{4} : V_{1}\\rightarrow V_{2}\\leftarrow V_{4} e V_{1}\\rightarrow V_{2}\\rightarrow V_{3}\\leftarrow V_{4} . No primeiro caminho V_{2} é um colisor, pois o caminho passa por duas arestas que apontam para V_{2} . Já no segundo caminho V_{2} é uma cadeia e V_{3} é um colisor. Note que a classificação do vértice depende do caminho analisado. Enquanto que no primeiro caminho V_{2} é um colisor, no segundo V_{2} é uma cadeia Refer to caption Figura 5: Ilustração do conceito de bloqueio de um caminho. No caminho (V1, V2, V4), V2 é um colisor. Isto ocorre pois, para chegar de V1 a V4 passando apenas por V2, as duas arestas apontam para V2. Já no caminho (V1, V2, V3, V4) temos que V2 é uma cadeia. Para chegar de V1 a V3 passando por V2, passa-se por duas arestas, uma entrando e outra saindo de V2. Como V2 é um colisor em (V1, V2, V4), este caminho está bloqueado se e somente se o valor de V2 é desconhecido. Como V2 é uma cadeia em (V1, V2, V3, V4), esse caminho está bloqueado quando o valor de V2 é conhecido. Com base nas conclusões da Seção 2.4, é possível compreender a racionalidade da Definição 2.46. Na Seção 2.4 vimos que, se Z não é um colisor entre X e Y , então X e Y são independentes dado Z . Por analogia, podemos intuir que um vértice que não é um colisor num caminho não permite a passagem de informação quando seu valor é conhecido. Similarmente, na Seção 2.4, se Z é um colisor entre X e Y , então X e Y são independentes. Assim, também podemos intuir que um vértice que é um colisor em um caminho não permite a passagem de informação quando seu valor e o de seus descendentes é desconhecido. Finalmente, a informação não passa pelo caminho quando ela não passa por pelo menos um de seus vértices. Neste caso, dizemos que o caminho está bloqueado: Definição 2.47. Seja C=(C_{1},\\ldots,C_{n}) um caminho em um DAG, {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) . Dizemos que C está bloqueado dado {\\mathbf{Z}}\\subset{\\mathcal{V}} , se 1.​Existe algum 2\\leq i\\leq n-1 tal que C_{i} não é um colisor em C e C_{i}\\in{\\mathbf{Z}} , ou 2.​Existe algum 2\\leq i\\leq n-1 tal que C_{i} é um colisor em C e C_{i}\\notin Anc({\\mathbf{Z}}) . Finalmente, dizemos que {\\mathbb{V}}_{1} está d-separado de {\\mathbb{V}}_{2} dado {\\mathbb{V}}_{3} se todos os caminhos de {\\mathbb{V}}_{1} a {\\mathbb{V}}_{2} estão bloqueados dado {\\mathbb{V}}_{3} : Definição 2.48. Seja {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) um DAG. Para {\\mathbb{V}}_{1},{\\mathbb{V}}_{2},{\\mathbb{V}}_{3}\\subseteq{\\mathcal{V}} , dizemos que {\\mathbb{V}}_{1} está d-separado de {\\mathbb{V}}_{2} dado {\\mathbb{V}}_{3} se, para todo caminho C=(C_{1},\\ldots,C_{n}) tal que C_{1}\\in V_{1} e C_{n}\\in{\\mathbb{V}}_{2} , C está bloqueado dado {\\mathbb{V}}_{3} . Neste caso, escrevemos {\\mathbb{V}}_{1}\\perp{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} . Intuitivamente, se {\\mathbb{V}}_{1}\\perp{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} , então não é possível passar informação de {\\mathbb{V}}_{1} a {\\mathbb{V}}_{2} quando {\\mathbb{V}}_{3} é conhecido. Assim, temos razão para acreditar que {\\mathbb{V}}_{1} é condicionalmente independente de {\\mathbb{V}}_{2} dado {\\mathbb{V}}_{3} , isto é {\\mathbb{V}}_{1}\\perp\\!\\!\\!\\!\\perp{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} . Esta conclusão é apresentada no Teorema 2.49 a seguir: Teorema 2.49. Seja {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) um DAG e {\\mathcal{V}} um conjunto de variáveis aleatórias. {\\mathbb{V}}_{1} está d-separado de {\\mathbb{V}}_{2} dado {\\mathbb{V}}_{3} se e somente se, para todo f compatível com {\\mathcal{G}} , {\\mathbb{V}}_{1}\\perp\\!\\!\\!\\!\\perp^{f}{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} . Exemplo 2.50. Considere o DAG na figur 5. Para avaliar se V_{1} e V_{3} são d-separados, precisamos analisar todos os caminhos de um para o outro. Estes caminhos são: V_{1}\\rightarrow V_{2}\\rightarrow V_{3} , e V_{1}\\rightarrow V_{2}\\leftarrow V_{4}\\rightarrow V_{3} . No primeiro caminho V_{2} não é um colisor e, assim, o caminho não está bloqueado marginalmente. Portanto, V_{1} e V_{3} não são d-separados marginalmente. Por outro lado, no segundo caminho V_{2} é um colisor e V_{4} não o é. Assim, condicionando em V_{2} , este caminho não está bloqueado. Portanto, V_{1} e V_{3} não são d-separados dado V_{2} . Finalmente, dado V_{2} e V_{4} , ambos os caminhos estão bloqueados, pois V_{2} não é um colisor no primeiro e V_{4} não é um colisor no segundo. Assim, V_{1} e V_{3} são d-separados dado (V_{2},V_{4}) . Para treinar este raciocínio, continue analisando a d-separação entre V_{1} e V_{4} . O algoritmo para testar d-separação está implementado em diversos pacotes. Além disso, é possível utilizar o Teorema 2.49 para enunciar todas as relações de independência condicional que são necessárias em um grafo. Estas implementações estão ilustradas abaixo: \\MakeFramed # Especificar o grafo grafo <- \"dag{ V1 -> V2 <- V4; V2 -> V3 <- V4 }\" dseparated(grafo, \"V1\", \"V3\", c(\"V2\")) ## [1] FALSE dseparated(grafo, \"V1\", \"V3\", c(\"V4\")) ## [1] FALSE dseparated(grafo, \"V1\", \"V3\", c(\"V2\", \"V4\")) ## [1] TRUE impliedConditionalIndependencies(grafo) ## V1 _||_ V3 | V2, V4 ## V1 _||_ V4 \\endMakeFramed Exemplo 2.51. Considere que {\\mathbb{V}}_{1} e {\\mathbb{V}}_{2} não são d-separados dado {\\mathbb{V}}_{3} . O Teorema 2.49 garante apenas que existe algum f compatível com o DAG tal que {\\mathbb{V}}_{1} e {\\mathbb{V}}_{2} são condicionalmente dependentes dado {\\mathbb{V}}_{3} segundo f . É possível mostrar que o conjunto de f ’s compatíveis com o grafo em que {\\mathbb{V}}_{1} e {\\mathbb{V}}_{2} são condicionalmente independentes dado {\\mathbb{V}}_{3} é relativamente pequeno àquele em que {\\mathbb{V}}_{1} e {\\mathbb{V}}_{2} são condicionalmente dependentes. Estudaremos um caso em que é possível observar esta relação em mais detalhe. Considere que V_{1},V_{2} , e Z são binárias e formam o grafo V_{1}\\leftarrow Z\\rightarrow V_{2} , isto é, Z é um confundidor. Além disso, {\\mathbb{P}}(Z=1)=0.5 , {\\mathbb{P}}(V_{i}=1|Z=j)=:p_{j} . Como V_{3} é um confundidor, V_{1} e V_{2} não são d-separados marginalmente. Para quais valores de p temos que V_{1} e V_{2} são marginalmente independentes? Para que V_{1} e V_{2} sejam independentes, é necessário que Cov[V_{1},V_{2}]=0 . Note que \\displaystyle{\\mathbb{E}}[V_{i}] \\displaystyle={\\mathbb{E}}[{\\mathbb{E}}[V_{i}|Z]]=0.5p_{1}+0.5p_{0} \\displaystyle{\\mathbb{E}}[V_{1}V_{2}] \\displaystyle={\\mathbb{E}}[|E[V_{1}V_{2}|Z]]=0.5p_{1}^{2}+0.5p_{0}^{2} Assim, para que Cov[V_{1},V_{2}]=0 , temos: \\displaystyle 0.5p^{2}_{1}+0.5p^{2}_{0} \\displaystyle=(0.5p_{1}+0.5p_{0})(0.5p_{1}+0.5p_{0}) \\displaystyle 0.5p^{2}_{1}+0.5p^{2}_{0} \\displaystyle=0.25p_{1}^{2}++0.5p_{1}p_{0}0.25p_{0}^{2} \\displaystyle 0.25p^{2}_{1}-0.5p_{1}p_{0}+0.25p^{2}_{0} \\displaystyle=0 \\displaystyle 0.25(p_{1}-p_{0})^{2} \\displaystyle=0 \\displaystyle p_{1} \\displaystyle=p_{0} Em outras palavras, dentre todos (p_{0},p_{1}) no quadrado [0,1]^{2} , somente os valores no segmento p_{1}=p_{0} tem alguma chance de levarem à independência entre V_{1} e V_{2} . Se imaginarmos que (p_{0},p_{1}) são equidistribuídos em [0,1]^{2} , então a probabilidade de sortearmos valores em que V_{1} e V_{2} são independentes é 0. Em conclusão, como V_{1} e V_{2} não são d-separados, somente para um conjunto pequeno de possíveis f ’s temos que V_{1} e V_{2} são independentes. 3.3 Exercícios Exercício 2.52. Considere que f é uma densidade sobre {\\mathcal{V}}=(V_{1},V_{2},V_{3},V_{4}) que é compatível com o grafo em figur 6. Além disso, cada V_{i}\\in\\{0,1\\} , V_{1},V_{2}\\sim\\text{Bernoulli}(0.5) , V_{3}\\equiv V_{1}\\cdot V_{2} e {\\mathbb{P}}(V_{4}=i|V_{3}=i)=0.9 , para todo i . (a)​ V_{1} e V_{2} são d-separados dado V_{3} ? (b)​ V_{1} e V_{2} são condicionalmente independentes dado V_{3} ? (c)​ V_{1} e V_{2} são d-separados dado V_{4} ? (d)​ V_{1} e V_{2} são condicionalmente independentes dado V_{4} ? Refer to caption Figura 6: Exemplo em que V4 é um descendente de um colisor, V3. Exercício 2.53. Prove que se um caminho, C=(C_{1},\\ldots,C_{n}) , está bloqueado dado {\\mathbb{V}} , então sempre que C é um sub-caminho de C^{*} , isto é, C^{*}=(A_{1},\\ldots,A_{m},C_{1},\\ldots,C_{n},B_{1},\\ldots,B_{l}) , temos que C^{*} está bloqueado dado {\\mathbb{V}} . Exercício 2.54. Prove que se {\\mathbb{V}}_{1}\\perp{\\mathbb{V}}_{3}|{\\mathbb{V}}_{4} e {\\mathbb{V}}_{2}\\perp{\\mathbb{V}}_{3}|{\\mathbb{V}}_{4} , então {\\mathbb{V}}_{1}\\cup{\\mathbb{V}}_{2}\\perp{\\mathbb{V}}_{3}|{\\mathbb{V}}_{4} . Exercício 2.55. Prove que se {\\mathbb{V}}_{1}\\perp{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} , então para todo V\\in(Anc({\\mathbb{V}}_{3})-{\\mathbb{V}}_{3}) , V\\perp{\\mathbb{V}}_{1}|{\\mathbb{V}}_{3} ou V\\perp{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} . Exercício 2.56. Sejam {\\mathcal{G}}_{1}=({\\mathcal{V}},{\\mathcal{E}}_{1}) e {\\mathcal{G}}_{2}=({\\mathcal{V}},{\\mathcal{E}}_{2}) grafos tais que {\\mathcal{E}}_{1}\\subseteq{\\mathcal{E}}_{2} . Prove que se {\\mathbb{V}}_{1}\\perp^{d}{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} em {\\mathcal{G}}_{2} , então {\\mathbb{V}}_{1}\\perp^{d}{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} em {\\mathcal{G}}_{1} . Previous page Next page"],[["index.html","chapter3.html","S4.html"],"4 O modelo de probabilidade para intervenções ‣ Capítulo 3 Intervenções ‣ Inferência Causal","Skip to content. O modelo de probabilidade para intervenções 4 O modelo de probabilidade para intervenções Com base no modelo estrutural causal discutido no capítulo 2, agora estabeleceremos um significado para o efeito causal de uma variável em outra. Refer to caption Figura 7: Grafo que representa as relações causais entre Z (Sexo), X (Tratamento), e Y (Cura). Para iniciar esta discussão, considere as variáveis Z (Sexo), X (Tratamento), e Y (Cura), discutidas no capítulo 1. Podemos considerar que Z é uma causa tanto de X quanto de Y e que X é uma causa de Y . Assim, podemos representar as relações causais entre estas variáveis por meio do grafo na figur 7. Usando este grafo, podemos discutir mais a fundo porque a probabilidade condicional de cura dado tratamento é distinta do efeito causal do tratamento na cura. Quando calculamos a probabilidade condicional de cura dado o tratamento, estamos perguntando: “Qual é a probabilidade de que um indivíduo selecionado aleatoriamente da população se cure dado que aprendemos que recebeu o tratamento?” Para responder a esta pergunta, propagamos a informação do tratamento usado em todos os caminhos do tratamento para a cura. Assim, além do efeito direto que o tratamento tem na cura, o tratamento também está associado ao sexo do paciente, o que indiretamente traz mais informação sobre a cura deste. Isto é, neste caso o tratamento traz informação tanto sobre seus efeitos (cura), quanto sobre suas causas (sexo). Uma outra maneira de verificar estas afirmações é calculando diretamente f(y|x) : \\displaystyle f(y|x) \\displaystyle=\\sum_{s}f(z,y|x) \\displaystyle=\\sum_{s}\\frac{f(z,y,x)}{f(x)} \\displaystyle=\\sum_{s}\\frac{f(z,x)f(y|z,x)}{f(x)} \\displaystyle=\\sum_{s}f(z|x)f(y|z,x) (2) Notamos na Seção 4 que f(y|x) é a média das probabilidades de cura em cada sexo, f(y|z,x) , ponderadas pela distribuição do sexo após aprender o tratamento do indivíduo, f(z|x) . A probabilidade condicional de cura dado tratamento não corresponde àquilo que entendemos por efeito causal de tratamento em cura. Este efeito é a resposta para a pergunta: “Qual a probabilidade de que um indivíduo selecionado aleatoriamente da população se cure dado que prescrevemos a ele o tratamento?”. Ao contrário da primeira pergunta, em que apenas observamos a população, nesta segunda fazemos uma intervenção sobre o comportamento do indivíduo. Assim, estamos fazendo uma pergunta sobre uma distribuição de probabilidade diferente, em que estamos agindo sobre a unidade amostral. Por exemplo, suponha que prescreveríamos o tratamento a qualquer indivíduo que fosse amostrado. Neste caso, saber qual tratamento foi aplicado não traria qualquer informação sobre o sexo do indivíduo. Em outras palavras, se chamarmos f(y|do(x)) como a probabilidade de cura dado que fazemos uma intervenção no tratamento, faria sentido obtermos: \\displaystyle f(y|do(x)) \\displaystyle=\\sum_{s}f(z)f(y|z,x) (3) Na likning 3 temos que o efeito causal do tratamento na cura é a média ponderada das probabilidades de cura em cada sexo ponderada pelas probabilidades de sexo de um indivíduo retirado aleatoriamente da população. Isto é, ao contrário da Seção 4, a distribuição do sexo do indivíduo não é alterada quando fazemos uma intervenção sobre o tratamento. Refer to caption Figura 8: Grafo que representa as relações causais entre S (Sexo), T (Tratamento), e C (Cura) quando há uma intervenção sobre T. Com base neste exemplo, podemos generalizar o que entendemos por intervenção. Quando fazemos uma intervenção em uma variável, V_{1} , tomamos uma ação para que V_{1} assuma um determinado valor. Assim, as demais variáveis que comumente seriam causas de V_{1} deixam de sê-lo. Por exemplo, para o caso na figur 7, o modelo de intervenção removeria a aresta de Sexo para Tratamento, resultado na figur 8. Com base nas observações acima, finalmente podemos definir o modelo de probabilidade sob intervenção: Definição 3.1. Seja {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) um DAG, ({\\mathcal{G}},f) um CM (Definição 2.24), e {\\mathbb{V}}_{1}\\subseteq{\\mathcal{V}} . O modelo de probabilidade obtido após uma intervenção em {\\mathbb{V}}_{1} é dado por: \\displaystyle f({\\mathcal{V}}|do({\\mathbb{V}}_{1})) \\displaystyle:=\\prod_{V_{2}\\in{\\mathbb{V}}_{2}}f(V_{2}|Pa(V_{2})) , ou equivalentemente \\displaystyle f({\\mathcal{V}}|do({\\mathbb{V}}_{1}={\\mathbf{v}}_{1})) \\displaystyle:=\\left(\\prod_{(v_{1},V_{1})\\in({\\mathbf{v}}_{1},{\\mathbb{V}}_{1}% )}{\\mathbb{I}}(V_{1}=v_{1})\\right)\\cdot\\left(\\prod_{V_{2}\\notin{\\mathbb{V}}_{1% }}f(V_{2}|Pa(V_{2}))\\right) Para compreender a Definição 3.1, podemos comparar o modelo de intervenção com o modelo observacional: \\displaystyle f({\\mathbb{V}}_{2}|{\\mathbb{V}}_{1}) \\displaystyle\\propto f({\\mathbb{V}}_{1},{\\mathbb{V}}_{2})=\\left(\\prod_{V_{1}% \\in{\\mathbb{V}}_{1}}f(V_{1}|Pa(V_{1})\\right)\\cdot\\left(\\prod_{V_{2}\\in{\\mathbb% {V}}_{2}}f(V_{2}|Pa(V_{2})\\right) No modelo observacional, a densidade de {\\mathbb{V}}_{2} dado {\\mathbb{V}}_{1} é proporcional ao produto, para todos os vértices, da densidade do vértice dadas suas causas. Ao contrário, no modelo de intervenção supomos que os vértices em {\\mathbb{V}}_{1} são pré-fixados e, assim, não são gerados por suas causas usuais. Assim, na Definição 3.1, a densidade de {\\mathbb{V}}_{2} dada uma intervenção em {\\mathbb{V}}_{1} é dada o produto somente nos vértices de {\\mathbb{V}}_{2} das densidades do vértice dadas suas causas. Esta análise é formalizada no Lema 3.2: Lema 3.2. Seja {\\mathcal{G}}(\\bar{{\\mathbf{X}}}) o grafo obtido retirando-se de {\\mathcal{G}} todas as arestas que apontam para algum vértice em {\\mathbf{X}} . A densidade f^{*}\\equiv f({\\mathcal{V}}|do({\\mathbf{X}}={\\mathbf{x}})) é compatível com {\\mathcal{G}}(\\bar{{\\mathbf{X}}}) . Além disso, {\\mathbf{X}} é degenerada em {\\mathbf{x}} segundo f^{*} . Com base na discussão acima, podemos definir o efeito causal que um conjunto de variáveis, {\\mathbf{X}} , tem em outro conjunto, {\\mathbf{Y}} : Definição 3.3. {\\mathbb{E}}[{\\mathbf{Y}}|do({\\mathbf{X}})]:=\\int{\\mathbf{y}}\\cdot f({\\mathbf{% y}}|do({\\mathbf{X}}))d{\\mathbf{y}} . Definição 3.4. O efeito causal médio, ACEX,Y,11 1 A sigla ACE tem como origem a expressão em inglês, Average Causal Effect. Optamos por manter a sigla sem tradução para facilitar a comparação com artigos da área. Em outros contextos, este termo também é chamado de Average Treatment Effect e recebe o acrônimo ATE. de X\\in\\Re em Y\\in\\Re é dado por: \\displaystyle{ACE}_{X,Y} \\displaystyle=\\begin{cases}{\\mathbb{E}}[Y|do(X=1)]-{\\mathbb{E}}[Y|do(X=0)]&% \\text{, se $X$ é binário},\\\\ \\frac{d{\\mathbb{E}}[Y|do(X=x)]}{dx}&\\text{, se $X$ é contínuo}.\\end{cases} Quando não há ambiguidade, escrevemos simplesmente {ACE} ao invés de {ACE}_{X,Y} . Com a Definição 3.4 podemos finalmente desvendar o Paradoxo de Simpson discutido no capítulo 1. Veremos que o método que desenvolvemos resolve a questão com simplicidade, assim trazendo clareza ao Paradoxo. Exemplo 3.5. Considere que (X,Y,Z)\\in\\Re^{3} são tais que X e Y são as indicadores de que, respectivamente, o paciente recebeu o tratamento e se curou. Além disso, suponha que a distribuição conjunta de (X,Y,Z) é dada pelas frequências na tabela 1. Isto é: \\displaystyle{\\mathbb{P}}(Z=1)=\\frac{25+55+71+192}{700}\\approx 0.49 \\displaystyle{\\mathbb{P}}(Z=0)=1-{\\mathbb{P}}(Z=1)\\approx 0.51 \\displaystyle{\\mathbb{P}}(Z=1|X=0)=\\frac{25+55}{25+55+36+234}\\approx 0.23 \\displaystyle{\\mathbb{P}}(Z=1|X=1)=\\frac{71+192}{71+192+6+81}\\approx 0.75 \\displaystyle{\\mathbb{P}}(Y=1|X=0,Z=0)=\\frac{234}{234+36}\\approx 0.87 \\displaystyle{\\mathbb{P}}(Y=1|X=1,Z=0)=\\frac{81}{81+6}\\approx 0.93 \\displaystyle{\\mathbb{P}}(Y=1|X=0,Z=1)=\\frac{55}{25+55}\\approx 0.69 \\displaystyle{\\mathbb{P}}(Y=1|X=1,Z=1)=\\frac{192}{71+192}\\approx 0.73 Agora, veremos que a probabilidade de Y dada uma intervenção em X depende do DAG usado no modelo causal estrutural. Suponha que Z é a indicadora de que o sexo do paciente é masculino. Neste caso, utilizaremos como grafo causal aquele em figur 7. este grafo, obtemos: \\displaystyle{\\mathbb{P}}_{1}(Y=i,Z=j|do(X=k)) \\displaystyle={\\mathbb{P}}(Z=j){\\mathbb{P}}(Y=i|X=k,Z=j) (4) Assim, \\displaystyle{\\mathbb{P}}_{1}(Y=1|do(X=1)) \\displaystyle={\\mathbb{P}}_{1}(Y=1,Z=0|do(X=1))+{\\mathbb{P}}_{1}(Y=1,Z=1|do(X=% 1)) \\displaystyle={\\mathbb{P}}(Z=0){\\mathbb{P}}(Y=1|X=1,Z=0)+{\\mathbb{P}}(Z=1){% \\mathbb{P}}(Y=1|X=1,Z=1) \\displaystyle\\approx 0.51\\cdot 0.93+0.49\\cdot 0.73\\approx 0.83 \\displaystyle{\\mathbb{P}}_{1}(Y=1|do(X=0)) \\displaystyle={\\mathbb{P}}_{1}(Y=1,Z=0|do(X=0))+{\\mathbb{P}}_{1}(Y=1,Z=1|do(X=% 0)) \\displaystyle={\\mathbb{P}}(Z=0){\\mathbb{P}}(Y=1|X=0,Z=0)+{\\mathbb{P}}(Z=1){% \\mathbb{P}}(Y=1|X=0,Z=1) \\displaystyle\\approx 0.51\\cdot 0.87+0.49\\cdot 0.69\\approx 0.78 Portanto, o efeito causal do tratamento na cura quando Z é o sexo do paciente é obtido abaixo: \\displaystyle{ACE}_{1} \\displaystyle={\\mathbb{E}}_{1}[Y|do(X=1)]-{\\mathbb{E}}_{1}[Y|do(X=0)] \\displaystyle={\\mathbb{P}}_{1}(Y=1|do(X=1))-{\\mathbb{P}}_{1}(Y=1|do(X=0))% \\approx 0.05 Como esperado da discussão na Seção 1, o tratamento tem efeito causal médio positivo, isto é, ele aumenta a probabilidade de cura do paciente. Refer to caption Figura 9: Grafo que representa as relações causais entre Z (Pressão sanguínea elevada), X (Tratamento), e Y (Cura). A seguir, consideramos que Z é a indicadora de pressão sanguínea elevada do paciente. Assim, tomamos o grafo causal como aquele na figur 9. Utilizando este grafo, obtemos: \\displaystyle{\\mathbb{P}}_{2}(Y=i,Z=j|do(X=k)) \\displaystyle={\\mathbb{P}}(Z=j|X=k){\\mathbb{P}}_{1}(Y=i|X=k,Z=j) (5) Assim, \\displaystyle{\\mathbb{P}}_{2}(Y=1|do(X=1)) \\displaystyle={\\mathbb{P}}_{2}(Y=1,Z=0|do(X=1))+{\\mathbb{P}}_{2}(Y=1,Z=1|do(X=% 1)) \\displaystyle={\\mathbb{P}}(Z=0|X=1){\\mathbb{P}}(Y=1|X=1,Z=0)+{\\mathbb{P}}(Z=1|% X=1){\\mathbb{P}}(Y=1|X=1,Z=1) \\displaystyle\\approx 0.25\\cdot 0.93+0.75\\cdot 0.73\\approx 0.78 \\displaystyle{\\mathbb{P}}_{2}(Y=1|do(X=0)) \\displaystyle={\\mathbb{P}}_{2}(Y=1,Z=0|do(X=0))+{\\mathbb{P}}_{2}(Y=1,Z=1|do(X=% 0)) \\displaystyle={\\mathbb{P}}(Z=0|X=0){\\mathbb{P}}(Y=1|X=0,Z=0)+{\\mathbb{P}}(Z=1|% X=0){\\mathbb{P}}(Y=1|X=0,Z=1) \\displaystyle\\approx 0.77\\cdot 0.87+0.23\\cdot 0.69\\approx 0.83 Portanto, o efeito causal do tratamento na cura quando Z é a pressão sanguínea do paciente é obtido abaixo: \\displaystyle{ACE}_{1} \\displaystyle={\\mathbb{E}}_{2}[Y|do(X=1)]-{\\mathbb{E}}_{2}[Y|do(X=0)] \\displaystyle={\\mathbb{P}}_{2}(Y=1|do(X=1))-{\\mathbb{P}}_{2}(Y=1|do(X=0))% \\approx-0.05 Como esperado da discussão na Seção 1, o tratamento tem efeito causal médio negativo, isto é, ele tem como efeito colateral grave a elevação da pressão sanguínea do paciente, reduzindo a probabilidade de cura deste. Comparando as expressões obtidas em {ACE}_{1} e {ACE}_{2} , verificamos que o grafo causal desempenha papel fundamental na determinação do modelo de probabilidade sob intervenção. Ademais, o uso do grafo causal adequado em cada situação formaliza a discussão qualitativa desenvolvida na Seção 1. Não há paradoxo! Se ({\\mathcal{G}},f) é um CM linear Gaussiano, então é possível obter uma equação direta para o {ACE} . Este resultado é apresentado no Teorema 3.6 abaixo. Teorema 3.6. Se ({\\mathcal{G}},f) é um CM linear Gaussiano de parâmetros \\mu e \\beta e \\mathbb{C}_{X,Y} é o conjunto de todos os caminhos direcionados de X a Y , então \\displaystyle{ACE}_{X,Y} \\displaystyle=\\sum_{C\\in\\mathbb{C}_{X,Y}}\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{% i}}. O Teorema 3.6 indica um algoritmo para calcular o {ACE}_{X,Y} em um CM linear Gaussiano. Primeiramente, para cada caminho direcionado de X em Y calcula-se o produto dos coeficientes de regressão ligados a este caminho. Se imaginarmos os vértices no meio do caminho como mediadores, então estamos combinando o efeito de X em C_{2} , de C_{2} em C_{3} …e de C_{m-1} em Y para obter o efeito total de X em Y por este caminho. Ao final, somamos os efeitos totais obtidos por todos os caminhos. Cada caminho direcionado indica uma forma em que X pode ter efeito sobre Y . Ao levarmos todoas as formas em consideração, obtemos o efeito causal médio. Além do efeito causal médio, às vezes desejamos determinar o efeito causal de X em Y quando observamos que a unidade amostral faz parte de determinado estrato da população. Em outras palavras, desejamos saber o efeito causal de X em Y quando observamos que outras variáveis, {\\mathbf{Z}} , assumem um determinado valor. Definição 3.7. O efeito causal médio condicional, CACE, de X\\in\\Re em Y\\in\\Re dado {\\mathbf{Z}} é: \\displaystyle{CACE}({\\mathbf{Z}}) \\displaystyle=\\begin{cases}{\\mathbb{E}}[Y|do(X=1),{\\mathbf{Z}}]-{\\mathbb{E}}[Y% |do(X=0),{\\mathbf{Z}}]&\\text{, se $X$ é binário},\\\\ \\frac{d{\\mathbb{E}}[Y|do(X=x),{\\mathbf{Z}}]}{dx}&\\text{, se $X$ é contínuo}.% \\end{cases} Uma vez estabelecido o modelo de probabilidade utilizado quando estudamos intervenções, agora podemos fazer inferência sobre o efeito causal. Para realizar tal inferência, em geral teremos de abordar duas questões: 1.​Identificação causal: Temos acesso a dados que são gerados segundo a distribuição observacional. Como é possível determinar o efeito causal em termos da distribuição observacional? 2.​Estimação: Uma vez estabelecida uma ligação entre a distribuição observacional dos dados e o efeito causal, como é possível estimá-lo? Nas próximas seções estudaremos algumas estratégias gerais para a resolução destas questões. Consideraremos que desejamos medir o efeito causal de X em Y , onde X,Y\\in{\\mathcal{V}} . 4.1 Exercícios Exercício 3.8. Considere que X_{1} e X_{2} são variáveis binárias. Também considere as seguintes definições: ACE := {\\mathbb{P}}(X_{2}=1|do(X_{1}=1))-{\\mathbb{P}}(X_{2}=1|do(X_{1}=0)) , e RD := {\\mathbb{P}}(X_{2}=1|X_{1}=1)-{\\mathbb{P}}(X_{2}=1|X_{1}=0) . Explique em palavras a diferença entre ACE e RD e apresente um exemplo em que essa diferença ocorre. Exercício 3.9 (Glymour2016[p.32]). (X_{1},X_{2},X_{3},X_{4}) são variáveis binárias tais que X_{i-1} é a única causa imediata de X_{i} . Além disso, {\\mathbb{P}}(X_{1}=1)=0.5 , {\\mathbb{P}}(X_{i}=1|X_{i-1}=1)=p_{11} e {\\mathbb{P}}(X_{i}=1|X_{i-1}=0)=p_{01} . Calcule: (a)​ {\\mathbb{P}}(X_{1}=1,X_{2}=0,X_{3}=1,X_{4}=0) , (b)​ {\\mathbb{P}}(X_{4}=1|X_{1}=1) , {\\mathbb{P}}(X_{4}=1|do(X_{1}=1) , (c)​ {\\mathbb{P}}(X_{1}=1|X_{4}=1) , {\\mathbb{P}}(X_{1}=1|do(X_{4}=1) , e (d)​ {\\mathbb{P}}(X_{3}=1|X_{1}=0,X_{4}=1) Exercício 3.10 (Glymour2016[p.29]). Considere que (U_{1},U_{2},U_{3}) são independentes e tais que U_{i}\\sim N(0,1) . Também, X_{1}\\equiv U_{1} , X_{2}\\equiv 3^{-1}X_{1}+U_{2} , e X_{3}\\equiv 2^{-4}X_{2}+U_{3} . Considere que X_{1} é a causa imediata de X_{2} , que por sua vez é a causa imediata de X_{3} . Além disso, cada U_{i} influencia diretamente somente X_{i} . (a)​Desenhe o DAG que representa a estrutura causal indicada no enunciado. (b)​Calcule {\\mathbb{E}}[X_{2}|X_{1}=3] e {\\mathbb{E}}[X_{2}|do(X_{1}=3)] . (c)​Calcule {\\mathbb{E}}[X_{3}|X_{1}=6] e {\\mathbb{E}}[X_{3}|do(X_{1}=6)] . (d)​Calcule {\\mathbb{E}}[X_{1}|X_{2}=1] e {\\mathbb{E}}[X_{1}|do(X_{2}=1)] . (e)​Calcule {\\mathbb{E}}[X_{2}|X_{1}=1,X_{3}=3] , {\\mathbb{E}}[X_{2}|X_{1}=1,do(X_{3}=3)] , e {\\mathbb{E}}[X_{2}|do(X_{1}=1),X_{3}=3] . Previous page Next page"],[["index.html","chapter3.html","S5.html"],"5 Controlando confundidores (critério backdoor) ‣ Capítulo 3 Intervenções ‣ Inferência Causal","Skip to content. Controlando confundidores (critério 5 Controlando confundidores (critério backdoor) Um confundidor é uma causa comum, direta ou indireta, de X em Y . Na existência de confundidores, a regressão de Y em X no modelo observacional, {\\mathbb{E}}[Y|X] , é diferente desta regressão no modelo de intervenção, {\\mathbb{E}}[Y|do(X)] . Isto ocorre pois, quando calculamos {\\mathbb{E}}[Y|X] , utilizamos toda a informação em X para prever Y . Esta informação inclui não apenas o efeito causal de X em Y , como também a informação que X traz indiretamente sobre Y pelo fato de ambas estarem associados aos seus confundidores. Para ilustrar este raciocínio, podemos revisitar o Exemplo 3.5. uma vez que Sexo (Z) é causa comum do Tratamento (X) e da Cura (Y), Z é um confundidor. Quando calculamos f(y|x) (Seção 4), utilizamos não só o efeito direto de X em Y , expresso em f(y|x,z) , como também a informação que indireta que X traz sobre Y por meio do confundidor Z , expressa pela combinação de f(z|x) com f(y|x,z) . Esta seção desenvolve uma estratégia para medir o efeito causal chamada de critério backdoor, que consiste em bloquear todos os caminhos de informação que passam por confundidores: Definição 3.11. Seja {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) um grafo causal e X,Y\\in{\\mathcal{V}} . Dizemos que {\\mathbf{Z}}\\subseteq{\\mathcal{V}}-\\{X,Y\\} satisfaz o critério “backdoor” se: 1.​ X\\notin Anc({\\mathbf{Z}}) , 2.​Para todo caminho de X em Y , C=(X,C_{2},\\ldots,C_{n-1},Y) tal que (C_{2},X)\\in{\\mathcal{E}} , C está bloqueado dado {\\mathbf{Z}} . Exemplo 3.12. No Exemplo 3.5 o único caminho de X em Y em que o vértice ligado a X é pai de X é X\\leftarrow Z\\rightarrow Y . Como Z é um confudidor neste caminho, ele o bloqueia. Assim, Z satisfaz o critério backdoor. Exemplo 3.13. Refer to caption Figura 10: Para medir o efeito causal de X em Y, podemos aplicar o critério backdoor. Neste grafo o único caminho aplicável ao critério backdoor é (X, Z, W, Y). Neste caminho, Z é uma cadeia e W é um confundidor. Assim, todas as possibilidades dentre Z, W, (Z,W) bloqueiam o caminho e satisfazem o critério backdoor. Considere o grafo causal na figur 10. Para aplicar o critério backdoor, devemos identificar todos os caminhos de X em Y em que o vértice ligado a X é pai de X , isto é, temos X\\leftarrow . O único caminho deste tipo é: X\\leftarrow Z\\leftarrow W\\rightarrow Y . Neste caminho, Z é uma cadeia e W é um confudidor. Assim, é possível bloquear este caminho condicionando em Z , em W , e em (Z,W) . Isto é, todos estas combinações satisfazem o critério backdoor. Exemplo 3.14. Refer to caption Figura 11: Para medir o efeito causal de X em Y, podemos aplicar o critério backdoor. Neste grafo existem dois caminhos aplicáveis ao critério backdoor: (X, A, B, Y) e (X, C, Y). No primeiro, A é um confundidor. No segundo caminho, C é um confudidor. Assim, (A,C) bloqueia ambos os caminhos e satisfaz o critério backdoor. Considere o grafo causal na figur 11. Para aplicar o critério backdoor, encontramos todos os caminhos de X em Y em que o vértice ligado a X é pai de X . Há dois caminhos deste tipo: X\\leftarrow A\\rightarrow B\\rightarrow Y e X\\leftarrow C\\rightarrow Y . Como A e C são confudidores, respectivamente, no primeiro e segundo caminhos, (A,C) bloqueia ambos eles. Assim (A,C) satisfaz o critério backdoor. Você consegue encontrar outro conjunto de variáveis que satisfaz o critério backdoor? Também é possível identificar os conjuntos de variáveis que satisfazem o critério backdoor por meio do pacote dagitty, como ilustrado a seguir: \\MakeFramed library(dagitty) # Especificar o grafo grafo <- dagitty(\"dag{ X[e] Y[o] A -> { X B }; B -> { Y }; C -> { X Y }; X -> { D Y }; D -> Y }\") adjustmentSets(grafo, type = \"all\") ## { A, C } ## { B, C } ## { A, B, C } \\endMakeFramed O critério backdoor generaliza duas condições especiais que são muito utilizadas. Em uma primeira condição, o valor de X é gerado integralmente por um aleatorizador, independente de todas as demais variáveis. Esta ideia é captada pela Definição 3.15, abaixo: Definição 3.15. Dizemos que X é um experimento aleatorizado simples se X é ancestral. Em um experimento aleatorizado simples não há confundidores. Assim, \\emptyset satisfaz o critério backdoor: Lema 3.16. Se X é um experimento aleatorizado simples, então \\emptyset satisfaz o critério backdoor. Veremos que em um experimento aleatorizado simples a distribuição intervencional é igual à distribuição observacional. Assim, {\\mathbb{E}}[Y|do(X)]={\\mathbb{E}}[Y|X] e a inferência causal é reduzida à inferência comumente usadas para a distribuição observacional. Além disso, o conjunto de todos os pais de X também satisfaz o critério backdoor: Lema 3.17. {\\mathbf{Z}}=Pa(X) satisfaz o critério backdoor para medir o efeito causal de X em Y . A seguir, veremos como o critério backdoor permite a identificação causal, isto é, uma equivalência entre quantidades de interesse obtidas pelo modelo de intervenção e quantidades obtidas pelo modelo observacional. 5.1 Identificação causal usando o critério backdoor A seguir, o Teorema 3.19 mostra que, se {\\mathbf{Z}} satisfaz o critério backdoor, então é possível ligar algumas distribuições sob intervenção em X a distribuições observacionais: Definição 3.18. {\\mathbf{Z}} controla confundidores para medir o efeito causal de X em Y se \\displaystyle f({\\mathbf{z}}|do(x)) \\displaystyle=f({\\mathbf{z}}),\\text{ e } \\displaystyle f(y|do(x),{\\mathbf{z}}) \\displaystyle=f(y|x,{\\mathbf{z}}). Teorema 3.19. Se {\\mathbf{Z}} satisfaz o critério backdoor para medir o efeito causal de X em Y , então {\\mathbf{Z}} satisfaz a Definição 3.18. O Teorema 3.19 mostra que, se {\\mathbf{Z}} satisfaz o critério backdoor, então distribuição de {\\mathbf{Z}} quando aplicamos uma intervenção em X é igual à distribuição marginal de {\\mathbf{Z}} . Além disso, a distribuição condicional de Y dado {\\mathbf{Z}} quando aplicamos uma intervenção em X é igual à distribuição de Y dado X e Z . Assim, o Teorema 3.19 relaciona distribuições que não geraram os dados a distribuições que os geraram. Com base neste resultado, é possível determinar f(y|do(x)) a partir de f(y,x,{\\mathbf{z}}) : Corolário 3.20. Se {\\mathbf{Z}} satisfaz a Definição 3.18, então \\displaystyle f(y|do(x)) \\displaystyle=\\int f(y|x,{\\mathbf{z}})f({\\mathbf{z}})d{\\mathbf{z}}. Para compreender intuitivamente o Corolário 3.20, podemos retornar ao Exemplo 3.5. Considere o caso em que X,Y,Z são as indicadoras de que, respectivamente, o paciente foi submetido ao tratamento, se curou e, é de sexo masculino. Similarmente ao Teorema 3.19, vimos em Exemplo 3.5 que f(y|do(x)) é a média de f(y|x,z) ponderada por f(z) . Nesta ponderação, utilizamos f(z) ao invés de f(z|x) pois Z é um confundidor e, assim, no modelo intervencional não propagamos a informação em X por esta variável. A mesma lógica se aplica às variáveis que satisfazem o critério backdoor. Para calcular quantidades como o {ACE} (Definição 3.4), utilizamos {\\mathbb{E}}[Y|do(X)] . Por meio do Teorema 3.19, é possível obter equivalências entre {\\mathbb{E}}[Y|do(X)] e esperanças obtidas no modelo observacional. Estas equivalências são descritas nos Teoremas 3.21 e 3.22. Teorema 3.21. Se {\\mathbf{Z}} satisfaz a Definição 3.18, então \\displaystyle{\\mathbb{E}}[g(Y)|do(X=x),{\\mathbf{Z}}] \\displaystyle={\\mathbb{E}}[g(Y)|X=x,{\\mathbf{Z}}],\\text{ e } \\displaystyle{\\mathbb{E}}[g(Y)|do(X=x)] \\displaystyle={\\mathbb{E}}[{\\mathbb{E}}[g(Y)|X=x,{\\mathbf{Z}}]] Teorema 3.22. Se {\\mathbf{Z}} satisfaz a Definição 3.18 e X é discreto, então \\displaystyle{\\mathbb{E}}[g(Y)|do(X=x),{\\mathbf{Z}}] \\displaystyle=\\frac{{\\mathbb{E}}[g(Y){\\mathbb{I}}(X=x)|{\\mathbf{Z}}]}{f(x|{% \\mathbf{Z}})},\\text{ e } \\displaystyle{\\mathbb{E}}[g(Y)|do(X=x)] \\displaystyle={\\mathbb{E}}\\left[\\frac{g(Y){\\mathbb{I}}(X=x)}{f(x|{\\mathbf{Z}})% }\\right] A seguir, veremos como os Teoremas 3.21 e 3.22 podem ser usados para estimar o efeito causal. Para provar resultados sobre os estimadores obtidos, a seguinte definição será útil Definição 3.23. Seja \\hat{g} um estimador treinado com os dados ({\\mathcal{V}}_{1},\\ldots,{\\mathcal{V}}_{n}) . Dizemos que \\hat{g} é invariante a permutações se o estimador não depende da ordem dos dados. Isto é, para qualquer permutações dos índices, \\pi:\\{1,\\ldots,n\\}\\rightarrow\\{1,\\ldots,n\\} , \\hat{g}({\\mathcal{V}}_{1},\\ldots,{\\mathcal{V}}_{n})\\equiv\\hat{g}({\\mathcal{V}}% _{\\pi(1)},\\ldots,{\\mathcal{V}}_{\\pi(n)}) Exemplo 3.24. A média amostral é invariante a permutações pois, para qualquer permutação \\pi , \\displaystyle\\frac{\\sum_{i=1}^{n}X_{i}}{n}=\\frac{\\sum_{i=1}^{n}X_{\\pi(i)}}{n}. 5.2 Estimação usando o critério backdoor 5.2.1 Fórmula do ajuste Se {\\mathbf{Z}} satisfaz a Definição 3.18, então {\\mathbb{E}}[Y|do(X),{\\mathbf{Z}}]={\\mathbb{E}}[Y|X,{\\mathbf{Z}}] . Como \\mu(X,Z):={\\mathbb{E}}[Y|X,{\\mathbf{Z}}] é a função de regressão de Y em X e Z , podemos estimar \\mu utilizando quaisquer métodos de estimação para regressão. Por exemplo, se Y é contínua, possíveis métodos são: regressão linear, Nadaraya-Watson, floresta aleatória de regressão, redes neurais, …Por outro lado, se Y é discreta, então a função de regressão é estimada por métodos de classificação como: regressão logística, k-NN, floresta aleatória de classificação, redes neurais, …Para qualquer opção escolhida, denotamos o estimador de \\mu por {\\widehat{\\mu}} . Utilizando {\\widehat{\\mu}} , podemos estimar {CACE}({\\mathbf{Z}}) diretamente. Para tal, note que {CACE}({\\mathbf{Z}}) é função de {\\mathbb{E}}[Y|do(X),{\\mathbf{Z}}] . Como o Teorema 3.21 garante que {\\mathbb{E}}[Y|do(X=x),{\\mathbf{Z}}]=\\mu(x,{\\mathbf{Z}}) , podemos definir o estimador \\displaystyle{\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x),{\\mathbf{Z}}]:={\\widehat{% \\mu}}(x,{\\mathbf{Z}}). O Teorema 3.21 também orienta a estimação do {ACE} . Similarmente ao caso anterior, o {ACE} é função de {\\mathbb{E}}[Y|do(X)] . Pelo Teorema 3.21, {\\mathbb{E}}[Y|do(X=x)]={\\mathbb{E}}[\\mu(x,{\\mathbf{Z}})] . Assim, se {\\widehat{\\mu}}\\approx\\mu , {\\mathbb{E}}[Y|do(X=x)]\\approx{\\mathbb{E}}[{\\widehat{\\mu}}(x,{\\mathbf{Z}})] . Como {\\mathbb{E}}[{\\widehat{\\mu}}(x,{\\mathbf{Z}})] é simplesmente uma média populacional, podemos estimá-la com base na média amostral: \\displaystyle{\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]:=\\frac{\\sum_{i=1}^{n}{% \\widehat{\\mu}}(x,{\\mathbf{Z}}_{i})}{n}\\approx{\\mathbb{E}}[{\\widehat{\\mu}}(x,{% \\mathbf{Z}})] Definição 3.25. Considere que {\\mathbf{Z}} satisfaz a Definição 3.18 e {\\widehat{\\mu}}(x,{\\mathbf{z}}) é uma estimativa da regressão {\\mathbb{E}}[Y|X=x,{\\mathbf{Z}}={\\mathbf{z}}] . Os estimadores de {\\mathbb{E}}[Y|do(X=x),{\\mathbf{Z}}] e {\\mathbb{E}}[Y|do(X=x)] pela fórmula do ajuste são: \\displaystyle{\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x),{\\mathbf{Z}}] \\displaystyle:={\\widehat{\\mu}}(x,{\\mathbf{Z}}) \\displaystyle{\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)] \\displaystyle:=\\frac{\\sum_{i=1}^{n}{\\widehat{\\mu}}(x,{\\mathbf{Z}}_{i})}{n} A seguir mostraremos que, se {\\widehat{\\mu}} converge para \\mu , então {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)] converge para {\\mathbb{E}}[Y|do(X=x)] . Em outras palavras, é possível utilizar {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)] para estimar o efeito causal de X em Y por meio de expressões como o {ACE} . Teorema 3.26. Seja \\mu(X,{\\mathbf{Z}}):={\\mathbb{E}}[Y|X,{\\mathbf{Z}}] . Se {\\mathbf{Z}} satisfaz a Definição 3.18, {\\mathbb{E}}[|\\mu(x,{\\mathbf{Z}}_{1})|]<\\infty , {\\mathbb{E}}[|{\\widehat{\\mu}}(x,{\\mathbf{Z}}_{1})-\\mu(x,{\\mathbf{Z}}_{1})|]=o(1) , e {\\widehat{\\mu}} é invariante a permutações (Definição 3.23), então {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]\\stackrel{{\\scriptstyle{\\mathbb{P}}}}{{% \\longrightarrow}}{\\mathbb{E}}[Y|do(X=x)] . A seguir, utilizamos dados simulados para ilustrar a implementação da fórmula do ajuste. Exemplo 3.27. Refer to caption Figura 12: DAG usado como exemplo para estimar efeito de X em Y. Considere que o grafo causal é dado pela figur 12. Vamos supor que os dados são gerados da seguinte forma: \\sigma^{2}=0.01 , A\\sim N(0,\\sigma^{2}) , B\\sim N(0,\\sigma^{2}) , \\epsilon\\sim Bernoulli(0.95) X\\equiv{\\mathbb{I}}(A+B>0)\\epsilon+{\\mathbb{I}}(A+B<0)(1-\\epsilon) , C\\sim N(X,\\sigma^{2}) , e Y\\sim N(A+B+C+X,\\sigma^{2}) : \\MakeFramed # Especificar o grafo grafo <- dagitty::dagitty(\"dag { X[e] Y[o] {A B} -> { X Y }; X -> {C Y}; C -> Y }\") # Simular os dados n <- 10^5 sd = 0.1 A <- rnorm(n, 0, sd) B <- rnorm(n, 0, sd) eps <- rbinom(n, 1, 0.8) X <- as.numeric(eps*((A + B) > 0) + (1-eps)*((A + B) <= 0)) C <- rnorm(n, X, sd) Y <- rnorm(n, A + B + C + X, sd) data <- dplyr::tibble(A, B, C, X, Y) \\endMakeFramed Estimaremos o efeito causal pela fórmula do ajuste (Definição 3.25). Iniciaremos a análise utilizando {\\widehat{\\mu}} como sendo uma regressão linear simples: \\MakeFramed # Sejam Z variáveis que satisfazem o critério backdoor para # estimar o efeito causal de causa em efeito em grafo. # Retorna uma fórmula do tipo Y ~ X + Z˙1 + … + Z˙d fm_ajuste <- function(grafo, causa, efeito) { var_backdoor <- dagitty::adjustmentSets(grafo)[[1]] regressores = c(causa, var_backdoor) fm = paste(regressores, collapse = \"+\") fm = paste(c(efeito, fm), collapse = \"~\") as.formula(fm) } # Estima E[Efeito—do(causa = x)] pela # formula do ajuste usando mu˙chapeu como regressao est_do_x_lm <- function(data, mu_chapeu, causa, x) { data %>% dplyr::mutate({{causa}} := x) %>% predict(mu_chapeu, newdata = .) %>% mean() } # Estimação do ACE com regressão linear simples fm <- fm˙ajuste(grafo, \"X\", \"Y\") mu_chapeu_lm <- lm(fm, data = data) ace_ajuste_lm = est˙do˙x˙lm(data, mu_chapeu_lm, \"X\", 1) - est˙do˙x˙lm(data, mu_chapeu_lm, \"X\", 0) round(ace_ajuste_lm) ## [1] 2 \\endMakeFramed Em alguns casos, não é razoável supor que {\\mathbb{E}}[Y|X,{\\mathbf{Z}}] é linear. Nestas situações, é fácil adaptar o código anterior para algum método não-paramétrico arbitrário. Exibimos uma implementação usando XGBoost (Chen2023): \\MakeFramed library(xgboost) var_backdoor <- dagitty::adjustmentSets(grafo, \"X\", \"Y\")[[1]] mu_chapeu <- xgboost( data = data %>% dplyr::select(all˙of(c(var_backdoor, \"X\"))) %>% as.matrix(), label = data %>% dplyr::select(Y) %>% as.matrix(), nrounds = 100, objective = \"reg:squarederror\", early_stopping_rounds = 3, max_depth = 2, eta = .25, verbose = FALSE ) est_do_x_xgb <- function(data, mu_chapeu, causa, x) { data %>% dplyr::mutate({{causa}} := x) %>% dplyr::select(c(var_backdoor, causa)) %>% as.matrix() %>% predict(mu_chapeu, newdata = .) %>% mean() } ace_est_xgb = est˙do˙x˙xgb(data, mu_chapeu, \"X\", 1) - est˙do˙x˙xgb(data, mu_chapeu, \"X\", 0) round(ace_est_xgb, 2) ## [1] 2 \\endMakeFramed Como o modelo linear era adequado para {\\mathbb{E}}[Y|X,{\\mathbf{Z}}] , não vemos diferença entre a estimativa obtida pela regressão linear simples e pelo XGBoost. Mas será que as estimativas estão adequadas? Como simulamos os dados, é possível calcular diretamente {\\mathbb{E}}[Y|do(X=x)] : \\displaystyle{\\mathbb{E}}[Y|do(X=x)] \\displaystyle={\\mathbb{E}}[{\\mathbb{E}}[Y|X=x,A,B]] \\displaystyle={\\mathbb{E}}[{\\mathbb{E}}[{\\mathbb{E}}[Y|X=x,A,B,C]|X=x,A,B]] Lei da esperança total \\displaystyle={\\mathbb{E}}[{\\mathbb{E}}[A+B+C+X|X=x,A,B]] \\displaystyle Y\\sim N(A+B+C+X,\\sigma^{2}) \\displaystyle={\\mathbb{E}}[A+B+2x] \\displaystyle C\\sim N(X,\\sigma^{2}) \\displaystyle=2x \\displaystyle{\\mathbb{E}}[A]={\\mathbb{E}}[B]=0 (6) Uma vez calculado {\\mathbb{E}}[Y|do(X=x)] , podemos obter o {ACE} : \\displaystyle{ACE} \\displaystyle={\\mathbb{E}}[Y|do(X=1)]-{\\mathbb{E}}[Y|do(X=0)] \\displaystyle=2\\cdot 1-2\\cdot 0=2 Portanto, as estimativas do {ACE} obtidas pela regressão linear e pelo xgboost estão adequadas. 5.2.2 Ponderação pelo inverso do escore de propensidade (IPW) Uma outra forma de estimar {\\mathbb{E}}[Y|do(X=x)] e {\\mathbb{E}}[Y|do(X=x),{\\mathbf{Z}}] é motivada pelo Teorema 3.22. Este resultado determina que, se {\\mathbf{Z}} satisfaz a Definição 3.18, então \\displaystyle{\\mathbb{E}}[Y|do(X=x),{\\mathbf{Z}}] \\displaystyle=\\frac{{\\mathbb{E}}\\left[Y{\\mathbb{I}}(X=x)|{\\mathbf{Z}}\\right]}{% f(x|{\\mathbf{Z}})}. Na segunda expressão, {\\mathbb{E}}[Y{\\mathbb{I}}(X=x)|{\\mathbf{Z}}] é a regressão de Y{\\mathbb{I}}(X=x) em {\\mathbf{Z}} . Assim, esta quantidade pode ser estimada por um método de regressão arbitrário, que denotaremos por \\widehat{{\\mathbb{E}}}[Y{\\mathbb{I}}(X=x)|{\\mathbf{Z}}] . Também f(x|{\\mathbf{z}}) é usualmente chamado de escore de propensidade. Este escore captura a forma como os confundidores atuam sobre X nos dados observacionais. Como f em geral é desconhecido, f(x|{\\mathbf{z}}) também o é. Contudo, quando X é discreto, podemos estimar f(x|{\\mathbf{z}}) utilizando algum algoritmo arbitrário de classificação. Denotaremos esta estimativa por \\widehat{f}(x|{\\mathbf{z}}) . Se a estimativa for boa, temos \\displaystyle{\\mathbb{E}}[Y|do(X=x),{\\mathbf{Z}}] \\displaystyle=\\frac{\\widehat{{\\mathbb{E}}}[Y{\\mathbb{I}}(X=x)|{\\mathbf{Z}}]}{f% (x|{\\mathbf{Z}})}\\approx\\frac{\\widehat{{\\mathbb{E}}}[Y{\\mathbb{I}}(X=x)|{% \\mathbf{Z}}]}{\\widehat{f}(x|{\\mathbf{z}})}. O Teorema 3.22 também orienta a estimação de {\\mathbb{E}}[Y|do(X=x)] . Se {\\mathbf{Z}} satisfaz o critério backdoor, então \\displaystyle{\\mathbb{E}}[Y|do(X=x)] \\displaystyle={\\mathbb{E}}\\left[\\frac{Y{\\mathbb{I}}(X=x)}{f(x|{\\mathbf{Z}})}\\right] Como nesta expressão a esperança é uma média populacional, ela pode ser aproximada pela média amostral \\displaystyle{\\mathbb{E}}\\left[\\frac{Y{\\mathbb{I}}(X=x)}{f(x|{\\mathbf{Z}})}\\right] \\displaystyle\\approx n^{-1}\\sum_{i=1}^{n}\\frac{Y_{i}{\\mathbb{I}}(X_{i}=x)}{% \\widehat{f}(x|{\\mathbf{Z}}_{i})}. Combinando estas aproximações, obtemos: Definição 3.28. Considere que {\\mathbf{Z}} satisfaz a Definição 3.18 e \\widehat{f}(x|{\\mathbf{z}}) é uma estimativa de f(x|{\\mathbf{z}}) . Os estimadores de {\\mathbb{E}}[Y|do(X=x),{\\mathbf{Z}}] e {\\mathbb{E}}[Y|do(X=x)] por IPW são: \\displaystyle{\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x),{\\mathbf{Z}}] \\displaystyle:=\\frac{\\widehat{{\\mathbb{E}}}[Y{\\mathbb{I}}(X=x)|{\\mathbf{Z}}]}{% \\widehat{f}(x|{\\mathbf{z}})} \\displaystyle{\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)] \\displaystyle:=n^{-1}\\sum_{i=1}^{n}\\frac{Y_{i}{\\mathbb{I}}(X_{i}=x)}{\\widehat{% f}(x|{\\mathbf{Z}}_{i})}. Se \\widehat{f} converge para f , então sob condições relativamente pouco restritivas {\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)] converge para {\\mathbb{E}}[Y|do(X=x)] . Teorema 3.29. Se {\\mathbf{Z}} satisfaz a Definição 3.18, também \\widehat{f} é invariante a permutações (Definição 3.23), {\\mathbb{E}}[|\\widehat{f}(x|{\\mathbf{Z}}_{1})-f(x|{\\mathbf{Z}}_{1})|]=o(1) , e existe M>0 tal que \\sup_{\\mathbf{z}}{\\mathbb{E}}[|Y|{\\mathbb{I}}(X=x)|{\\mathbf{Z}}={\\mathbf{z}}]<M , e existe \\delta>0 tal que \\inf_{z}\\min\\{f(x|{\\mathbf{Z}}_{1}),\\widehat{f}(x|{\\mathbf{Z}}_{1})\\}>\\delta , então {\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)]\\stackrel{{\\scriptstyle{\\mathbb{P}}}}{{% \\longrightarrow}}{\\mathbb{E}}[Y|do(X=x)] . A seguir, utilizamos novamente dados simulados para ilustrar a implementação de IPW: Exemplo 3.30. Considere que o grafo causal e o modelo de geração dos dados são idênticos àqueles do Exemplo 3.27. Iniciaremos a análise utilizando regressão logística para estimar \\widehat{f} . \\MakeFramed # Sejam Z variáveis que satisfazem o critério backdoor para # estimar o efeito causal de causa em efeito em grafo. # Retorna uma fórmula do tipo X ~ Z˙1 + … + Z˙d fm_ipw <- function(grafo, causa, efeito) { var_backdoor <- dagitty::adjustmentSets(grafo)[[1]] fm = paste(var_backdoor, collapse = \"+\") fm = paste(c(causa, fm), collapse = \"~\") as.formula(fm) } # Estimação do ACE por IPW onde # Supomos X binário e # f˙1 é o vetor P(X˙i=1—Z˙i) ACE_ipw <- function(data, causa, efeito, f_1) { data %>% mutate(f_1 = f_1, est_1 = {{efeito}}*({{causa}}==1)/f_1, est_0 = {{efeito}}*({{causa}}==0)/(1-f_1) ) %>% summarise(do_1 = mean(est_1), do_0 = mean(est_0)) %>% mutate(ACE = do_1 - do_0) %>% dplyr::select(ACE) } fm <- fm˙ipw(grafo, \"X\", \"Y\") f_chapeu <- glm(fm, family = \"binomial\", data = data) f_1_lm <- predict(f_chapeu, type = \"response\") ace_ipw_lm <- data %>% ACE˙ipw(X, Y, f_1_lm) %>% as.numeric() ace_ipw_lm %>% round(2) ## [1] 2.09 \\endMakeFramed Também é fácil adaptar o código acima para estimar {ACE} por IPW utilizando algum método não-paramétrico para estimar \\widehat{f} . Abaixo há um exemplo utilizando o XGBoost: \\MakeFramed var_backdoor <- dagitty::adjustmentSets(grafo)[[1]] f_chapeu <- xgboost( data = data %>% dplyr::select(all˙of(var_backdoor)) %>% as.matrix(), label = data %>% dplyr::select(X) %>% as.matrix(), nrounds = 100, objective = \"binary:logistic\", early_stopping_rounds = 3, max_depth = 2, eta = .25, verbose = FALSE ) covs <- data %>% dplyr::select(all˙of(var_backdoor)) %>% as.matrix() f_1 <- predict(f_chapeu, newdata = covs) data %>% ACE˙ipw(X, Y, f_1) %>% as.numeric() %>% round(2) ## [1] 1.97 \\endMakeFramed 5.2.3 Estimador duplamente robusto Os Teoremas 3.26 e 3.29 mostram que, sob suposições diferentes, {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)] e {\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)] convergem para {\\mathbb{E}}[Y|do(X=x)] . A ideia do estimador duplamente robusto é combinar ambos os estimadores de forma a garantir esta convergência sob suposições mais fracas. Para tal, a ideia por trás do estimador duplamente é que este convirja junto a {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)] quando este é consistente e para {\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)] quando aquele o é. Definição 3.31. Se {\\mathbf{Z}} satisfaz a Definição 3.18 e sejam \\widehat{f} e {\\widehat{\\mu}} tais quais nas Definições 3.25 e 3.28. O estimador duplamente robusto para {\\mathbb{E}}[Y|do(X=x)] , {\\widehat{{\\mathbb{E}}}}_{3}[Y|do(X=x)] é tal que \\displaystyle{\\widehat{{\\mathbb{E}}}}_{3}[Y|do(X=x)] \\displaystyle={\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]+{\\widehat{{\\mathbb{E}}}}% _{2}[Y|do(X=x)]-\\sum_{i=1}^{n}\\frac{{\\mathbb{I}}(X_{i}=x){\\widehat{\\mu}}(x,{% \\mathbf{Z}}_{i})}{n\\hat{f}(x|{\\mathbf{Z}}_{i})} O estimador duplamente robusto é consistente para {\\mathbb{E}}[Y|do(X=x)] tanto sob as condições do Teorema 3.26 quanto sob as do Teorema 3.29. A ideia básica é que, sob as condições do Teorema 3.26, {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)] é consistente para {\\mathbb{E}}[Y|do(X=x)] e {\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)]-\\sum_{i=1}^{n}\\frac{{\\mathbb{I}}(X_{i}% =x){\\widehat{\\mu}}(x,{\\mathbf{Z}}_{i})}{n\\hat{f}(x|{\\mathbf{Z}}_{i})} converge para 0 . Isto é, quando {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)] é consistente, o estimador duplamente robusto seleciona este termo. Similarmente, sob as condições do Teorema 3.29, {\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)] é consistente para {\\mathbb{E}}[Y|do(X=x)] e {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]-\\sum_{i=1}^{n}\\frac{{\\mathbb{I}}(X_{i}% =x){\\widehat{\\mu}}(x,{\\mathbf{Z}}_{i})}{n\\hat{f}(x|{\\mathbf{Z}}_{i})} converge para 0 . Teorema 3.32. Suponha que existe \\epsilon>0 tal que \\inf_{\\mathbf{z}}\\widehat{f}(x|{\\mathbf{z}})>\\epsilon , existe M>0 tal que \\sup_{\\mathbf{z}}{\\widehat{\\mu}}(x,{\\mathbf{z}})<M , e {\\widehat{\\mu}} e \\widehat{f} são invariantes a permutações (Definição 3.23). Se as condições do Teorema 3.26 ou do Teorema 3.29 estão satisfeitas, então \\displaystyle{\\widehat{{\\mathbb{E}}}}_{3}[Y|do(X=x)]\\stackrel{{\\scriptstyle{% \\mathbb{P}}}}{{\\longrightarrow}}{\\mathbb{E}}[Y|do(X=x)]. Exemplo 3.33 (Estimador duplamente robusto). Considere que o grafo causal e o modelo de geração dos dados são iguais àqueles descritos no Exemplo 3.27. Para implementar o estimador duplamente robusto combinaremos o estimador da fórmula do ajuste obtido por regressão linear no Exemplo 3.27 e aquele de IPW por regressão logística no Exemplo 3.30. \\MakeFramed mu_1_lm <- data %>% dplyr::mutate(X = 1) %>% predict(mu_chapeu_lm, newdata = .) mu_0_lm <- data %>% dplyr::mutate(X = 0) %>% predict(mu_chapeu_lm, newdata = .) corr <- data %>% mutate(mu_1 = mu_1_lm, mu_0 = mu_0_lm, f_1 = f_1_lm, corr_1 = (X == 1)*mu_1/f_1, corr_0 = (X == 0)*mu_0/(1-f_1)) %>% summarise(corr_1 = mean(corr_1), corr_0 = mean(corr_0)) %>% mutate(corr = corr_1 - corr_0) %>% dplyr::select(corr) %>% as.numeric() ace_rob_lm <- ace_ajuste_lm + ace_ipw_lm - corr ace_rob_lm %>% round(2) ## [1] 2 \\endMakeFramed 5.3 Exercícios Exercício 3.34. Prove o Lema 3.16. Exercício 3.35. Prove o Lema 3.17. Exercício 3.36. Prove que se X\\notin Anc(Y) , então {ACE}=0 . Exercício 3.37. Prove que a variância amostral satisfaz o Definição 3.23. Exercício 3.38. Utilizando como referência o grafo e o código no Exemplo 3.27, simule dados tais que a estimativa do {ACE} é diferente quando um método de regressão linear e um de regressão não-paramétrica são usados. 5.4 Regression Discontinuity Design (RDD) Em determinadas situações, X é completamente determinado pelos confundidores, {\\mathbf{Z}} (Lee2010). Por exemplo, considere que desejamos determinar o efeito causal que um determinado programa social do governo traz sobre o nível de educação dos cidadãos. Neste caso, X é a indicadora de que o indíviduo é elegível ao programa e Y mede o seu nível de educação. Em alguns casos, é razoável supor que X é completamente determinado por Z , a renda do indivíduo. A situação acima traz desafios para a fórmula do ajuste e IPW discutidos anteriormente. Primeiramente, como X=h({\\mathbf{Z}}) , não é possível estimar {\\mathbb{E}}[Y|X=x,{\\mathbf{Z}}={\\mathbf{z}}] quando x\\neq h({\\mathbf{z}}) . Portanto, não é possível utilizar a fórmula do ajuste, uma vez que ela se baseia na expressão {\\mathbb{E}}[{\\mathbb{E}}[Y|X=x,{\\mathbf{Z}}]] . Similarmente, o estimador de IPW envolve uma divisão por f(x|{\\mathbf{Z}}) . Assim, quando x\\neq h({\\mathbf{Z}}) há uma divisão por 0 , o que torna o estimador indefinido. 5.4.1 Identificação causal no RDD Apesar destas dificuldades, é possível medir nestas situações parte do efeito causal de X em Y . Suponha que {\\mathbf{Z}}\\in\\Re e que existe z_{1} tal que X={\\mathbb{I}}({\\mathbf{Z}}\\geq{\\mathbf{z}}_{1}) . Por exemplo, um benefício pode estar disponível apenas para cidadãos que tenham renda abaixo de um teto ou uma lei pode ter efeitos a partir de uma determinada data. Neste caso, podemos estar interessados em {\\mathbb{E}}[Y|do(X=x),{\\mathbf{Z}}={\\mathbf{z}}_{1}] , o efeito causal que X tem na fronteira de sua implementação. Intuitivamente, próximo a esta fronteira, as unidades amostrais são todas similares em relação aos confundidores. Assim, se na fronteira houver uma diferença em Y entre os valores de X , esta diferença deve ser decorrente do efeito causal de X . Esta intuição é formalizada no resultado de identicação causal abaixo: Teorema 3.39 (Hahn2001). Considere que {\\mathbf{Z}}\\in\\Re satisfaz a Definição 3.18, X\\in\\{0,1\\} , e {\\mathbb{E}}[Y|do(X=0),{\\mathbf{Z}}] e {\\mathbb{E}}[Y|do(X=1),{\\mathbf{Z}}] são contínuas em {\\mathbf{Z}}={\\mathbf{z}}_{1} . Se X\\equiv{\\mathbb{I}}({\\mathbf{Z}}\\geq{\\mathbf{z}}_{1}) , então \\displaystyle{CACE}({\\mathbf{Z}}={\\mathbf{z}}_{1}) \\displaystyle=\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}}_{1}}{\\mathbb{E}}[Y|{% \\mathbf{Z}}={\\mathbf{z}}]-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}{\\mathbb{% E}}[Y|{\\mathbf{Z}}={\\mathbf{z}}]. Se f(x|{\\mathbf{Z}})\\in(0,1) é contínua exceto em {\\mathbf{z}}_{1} , então \\displaystyle{CACE}({\\mathbf{Z}}={\\mathbf{z}}_{1}) \\displaystyle=\\frac{\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}}_{1}}{\\mathbb{E}}[% Y|{\\mathbf{Z}}={\\mathbf{z}}]-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}{% \\mathbb{E}}[Y|{\\mathbf{Z}}={\\mathbf{z}}]}{\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf% {z}}_{1}}f(X=1|{\\mathbf{Z}}={\\mathbf{z}})-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z% }}_{1}}f(X=1|{\\mathbf{Z}}={\\mathbf{z}})}. Um detalhe sutil do Teorema 3.39 é que X\\equiv{\\mathbb{I}}({\\mathbf{Z}}>{\\mathbf{z}}_{1}) não é o suficiente para termos certeza que {\\mathbf{Z}} satisfaz o critério backdoor. Por exemplo, considere que o governo criasse um benefício fiscal para todas empresas sediadas em um determinado município. Neste caso, a ocorrência do benefício é função da sede da empresa. Contudo, a relação causal é mais complexa. Se o benefício for suficientemente alto, poderia motivar empresas a moverem sua sede para o município. Em outras palavras, o benefício seria causa da localização da sede e não o contrário. Neste caso, não seria possível aplicar o Teorema 3.39. Este tipo de raciocínio indica que a análise por RDD é mais efetiva quando é difícil interferir sobre o valor de {\\mathbf{Z}} . Por exemplo, como um indivíduo não pode interferir sobre a sua idade, é mais fácil justificar o uso de RDD em uma campanha de vacinação em que apenas indivíduos acima de uma determinada idade são vacinados. Um outro ponto importante de interpretação do Teorema 3.39 é que, embora {\\mathbb{E}}[Y|do(X=0),{\\mathbf{Z}}] e {\\mathbb{E}}[Y|do(X=1),{\\mathbf{Z}}] sejam supostas contínuas, f(X=1|{\\mathbf{Z}}) e {\\mathbb{E}}[Y|{\\mathbf{Z}}] não o são. Intuitivamente, podemos imaginar X representa a indicadora de que uma determinada política é adotada. Por exemplo, podemos imaginar que X indica que um indivíduo foi vacinado, {\\mathbf{Z}} a sua idade e Y a sua hospitalização. Neste caso, {\\mathbb{E}}[Y|do(X=0),{\\mathbf{Z}}] e {\\mathbb{E}}[Y|do(X=1),{\\mathbf{Z}}] representam a taxa de hospitalização quando todos os indivíduos são vacinados ou quando todos eles não o são. Nestas situações, seria razoável supor que a taxa de hospitalização é contínua em função da idade, pois não esperamos que exista uma grande descontinuidade nas condições de saúde entre indivíduos com 69 e com 70 anos de idade. Este tipo de conclusão muitas vezes é resumido pela expressão em latim natura non facit saltus (a natureza não faz saltos). Por outro lado, nos dados observados, a política não é adotada para uma faixa de valores de {\\mathbf{Z}} e passa a ser adotada a partir de um ponto, o que é responsável pela descontinuidade em {\\mathbb{E}}[Y|{\\mathbf{Z}}] e em f(X=1|{\\mathbf{Z}}) . Podemos imaginar que a vacinação é empregada somente em indivíduos com mais de 70 anos. Esta descontinuidade na política humana cria uma diferença importante entre indivíduos com 69 e com 70 anos, o que explica uma diferença grande nas taxas de hospitalização entre estas idades nos dados observados. 5.4.2 Estimação no RDD O Teorema 3.39 indica que {CACE}({\\mathbf{Z}}={\\mathbf{z}}_{1}) é função da regressão de Y sobre {\\mathbf{Z}} , {\\mathbb{E}}[Y|{\\mathbf{Z}}] , e sobre o classificador, f(X=1|{\\mathbf{Z}}) . Uma possível estratégia é estimarmos estas quantidades separadamente e, a seguir, estimarmos o {CACE} trocando as quantias populacionais pelas quantias estimadas. Uma dificuldade nesta estratégia é que sabemos que {\\mathbb{E}}[Y|{\\mathbf{Z}}] e f(X=1|{\\mathbf{Z}}) são discontínuas. Para lidar com esta dificuldade, uma possibilidade é realizar uma regressão para {\\mathbf{Z}}<{\\mathbf{z}}_{1} e outra para {\\mathbf{Z}}\\geq{\\mathbf{z}}_{1} . Definição 3.40. Seja D_{<}=\\{i:{\\mathbf{Z}}_{i}<{\\mathbf{z}}_{1}\\} o conjunto de unidades amostrais em que {\\mathbf{Z}}_{i}<{\\mathbf{z}}_{1} , \\widehat{{\\mathbb{E}}}_{<}[Y|{\\mathbf{Z}}] e \\widehat{f}_{<}(X=1|{\\mathbf{Z}}) regressões ajustadas utilizando apenas dados em D_{<} e \\widehat{{\\mathbb{E}}}_{\\geq}[Y|{\\mathbf{Z}}] e \\widehat{f}_{\\geq}(X=1|{\\mathbf{Z}}) ajustadas em D^{c}_{<} . O estimador RDD para {CACE}({\\mathbf{z}}_{1}) é \\displaystyle{\\widehat{{CACE}}}({\\mathbf{z}}_{1}):=\\frac{\\widehat{{\\mathbb{E}}% }_{\\geq}[Y|{\\mathbf{z}}_{1}]-\\widehat{{\\mathbb{E}}}_{<}[Y|{\\mathbf{z}}_{1}]}{% \\widehat{f}_{\\geq}(X=1|{\\mathbf{Z}})-\\widehat{f}_{<}(X=1|{\\mathbf{Z}})}. Em particular, se sabemos a priori que f(X=1|{\\mathbf{z}})=1 para {\\mathbf{z}}\\geq{\\mathbf{z}}_{1} e f(X=1|{\\mathbf{z}})=0 para {\\mathbf{z}}<{\\mathbf{z}}_{1} , então \\displaystyle{\\widehat{{CACE}}}({\\mathbf{z}}_{1}):=\\widehat{{\\mathbb{E}}}_{% \\geq}[Y|{\\mathbf{z}}_{1}]-\\widehat{{\\mathbb{E}}}_{<}[Y|{\\mathbf{z}}_{1}] O exemplo a seguir ilustra a implementação de RDD quando X\\equiv{\\mathbb{I}}({\\mathbf{Z}}\\geq{\\mathbf{z}}_{1}) utilizando tanto regressão linear quanto regressão de Kernel de Nadaraya-Watson. Exemplo 3.41. Considere que Z_{i} satisfaz o critério backdoor para estimar o efeito causal de X em Y . Além disso, Z_{i}\\sim N(0,1) , X_{i}\\equiv{\\mathbb{I}}(Z_{i}\\geq 0) e Y_{i}|X_{i},Z_{i}\\sim N(50(X_{i}+1)(Z_{i}+1),1) . Podemos simular os dados da seguinte forma: \\MakeFramed n <- 1000 Z <- rnorm(n) X <- Z >= 0 Y <- rnorm(n, 50*(X+1)*(Z+1)) data <- tibble(X, Y, Z) plot(Z, Y) \\endMakeFramed Refer to caption Figura 13: Exemplo em que Z satisfaz o critério backdoor para medir o efeito causal de X em Y e X = I(Z > 0). Como resultado da descontinuidade da propensidade de X em Z = 0, há uma descontinuidade na regressão de Y em Z no ponto Z=0. Como estamos simulando os dados, podemos calcular {CACE}(0) : \\displaystyle{CACE}(0) \\displaystyle=\\lim_{{\\mathbf{z}}\\downarrow 0}{\\mathbb{E}}[Y|{\\mathbf{Z}}={% \\mathbf{z}}]-\\lim_{{\\mathbf{z}}\\uparrow 0}{\\mathbb{E}}[Y|{\\mathbf{Z}}={\\mathbf% {z}}] \\displaystyle=\\lim_{{\\mathbf{z}}\\downarrow 0}{\\mathbb{E}}[Y|X=1,{\\mathbf{Z}}={% \\mathbf{z}}]-\\lim_{{\\mathbf{z}}\\uparrow 0}{\\mathbb{E}}[Y|X=0,{\\mathbf{Z}}={% \\mathbf{z}}] \\displaystyle=\\lim_{{\\mathbf{z}}\\downarrow 0}50(1+1)({\\mathbf{z}}+1)-\\lim_{{% \\mathbf{z}}\\uparrow 0}50(0+1)({\\mathbf{z}}+1)=50 O código abaixo estima {CACE}(0) usando regressão linear: \\MakeFramed regs = data %>% mutate(Z1 = (Z >= 0)) %>% group˙by(Z1) %>% summarise( intercepto = lm(Y ~ Z)$coefficients[1], coef_angular = lm(Y ~ Z)$coefficients[2] ) regs ## # A tibble: 2 x 3 ## Z1 intercepto coef_angular ## <lgl> <dbl> <dbl> ## 1 FALSE 50.1 50.1 ## 2 TRUE 99.9 100. est_cace = 1*regs[2, 2] + 0*regs[2, 3] - 1*regs[1, 2] + 0*regs[1, 3] round(as.numeric(est_cace), 2) ## [1] 49.81 \\endMakeFramed Similarmente, podemos estimar {CACE}(0) usando regressão por kernel de Nadaraya-Watson: \\MakeFramed library(np) options(np.messages = FALSE) nw_reg <- function(data, valor) { bw <- npregbw(xdat = data$Z, ydat = data$Y)$bw npksum(txdat= data$Z, exdat = valor, tydat = data$Y, bws = bw)$ksum/ npksum(txdat = data$Z, exdat = valor, bws = bw)$ksum } reg_baixo <- data %>% filter(Z < 0) %>% nw˙reg(0) reg_cima <- data %>% filter(Z >= 0) %>% nw˙reg(0) est_cace <- reg_cima - reg_baixo round(est_cace, 2) ## [1] 51.31 \\endMakeFramed 5.5 Exercícios Exercício 3.42. Crie um exemplo em que, ao contrário do Exemplo 3.41, {\\mathbb{E}}[Y|X=1,{\\mathbf{Z}}] não é linear em {\\mathbf{Z}} . Compare as estimativas de {CACE} usando a regressão linear e algum método de regressão não-paramétrica. Previous page Next page"],[["index.html","chapter3.html","S6.html"],"6 Controlando mediadores (critério frontdoor) ‣ Capítulo 3 Intervenções ‣ Inferência Causal","Skip to content. Controlando mediadores (critério frontdoor) 6 Controlando mediadores (critério frontdoor) Refer to caption Figura 14: . Há casos em que não existem variáveis observadas que satisfazem o critério backdoor. Por exemplo, considere o grafo causal na figur 14 (Glymour2016). Neste grafo, estamos interessados em compreender o efeito causal do fumo ( X ) sobre a incidência de câncer ( Y ). Além disso, fatores genéticos não observáveis ( G ) são um potencial confundidor, uma vez que podem ter influência tanto sobre o fumo quanto sobre a incidência de câncer. Assim, como G não é observado, não é possível implementar os métodos de estimação vistos na última seção. Apesar desta dificuldade, ainda é possível medir o efeito causal de X em Y na figur 14. Para tal, primeiramente observe que é possível estimar o efeito causal de X em P e de P em Y . Para medir o efeito causal de X em P , note que \\emptyset satisfaz o critério backdoor. Isso ocorre pois Y é um colisor em X\\leftarrow G\\rightarrow Y\\leftarrow P . Além disso, como X=Pa(P) , decorre do Lema 3.17 que X satisfaz o critério backdoor para medir o efeito causal de P em Y . Das duas últimas conclusões decorre do Teorema 3.19 que f(P|do(X))=f(P|X) e que f(Y|do(P))=\\int f(Y|P,X)f(X)dX . A seguir, o critério frontdoor consiste em observar que P está no único caminho direcionado de X a Y , X\\rightarrow P\\rightarrow Y . Assim, é possível provar a identificação causal \\displaystyle f(Y|do(X)) \\displaystyle=\\int f(P|do(X))f(Y|do(P))dP \\displaystyle=\\int f(P|do(X))\\int f(Y|P,X)f(X)dX. O critério frontdoor é formalizado a seguir: Definição 3.43. {\\mathbf{W}} satisfaz o critério frontdoor para medir o efeito causal de X em Y se: 1.​para todo caminho direcionado de X em Y , C , existe C_{i}\\in{\\mathbf{W}} e, para todo W\\in{\\mathbf{W}} , existe caminho direcionado de X em Y , C , e i tal que C_{i}=W . 2.​ \\emptyset satisfaz o item 2 do critério backdoor (Definição 3.11) para medir o efeito causal de X em {\\mathbf{W}} . 3.​ X satisfaz o item 2 do critério backdoor (Definição 3.11) para medir o efeito causal de {\\mathbf{W}} em Y . A Definição 3.43 elenca todos os itens que utilizamos na análise da figur 14. O primeiro item do critério identifica que {\\mathbf{W}} deve interceptar todos os caminhos direcionados de X a Y . Isto é, {\\mathbf{W}} capturar a informação de todos os mediadores de X a Y . O segundo e terceiro itens estabelecem as condições para que seja possível aplicar o critério backdoor para identificar f({\\mathbf{W}}|do(X)) e f(Y|do({\\mathbf{W}})) . 6.0.1 Identificação causal O critério frontdoor possibilita a identificação do efeito causal de X em Y : Teorema 3.44. Se {\\mathbf{W}} satisfaz o critério frontdoor para medir o efeito causal de X em Y , então \\displaystyle f(Y|do(X=x)) \\displaystyle=\\int f({\\mathbf{W}}|x)\\int f(Y|{\\mathbf{W}},X)f(X)dXd{\\mathbf{W}} Teorema 3.45. Se {\\mathbf{W}} satisfaz o critério frontdoor para estimar o efeito causal de X em Y , então \\displaystyle{\\mathbb{E}}[Y|do(X=x)] \\displaystyle={\\mathbb{E}}\\left[\\frac{Y\\cdot f(W|x)}{f(W|X)}\\right] 6.0.2 Estimação pelo critério frontdoor A estimação é um tema menos desenvolvido ao aplicar o critério frontdoor. Alguns estimadores não-paramétricos são apresentados em Tchetgen2012. A seguir, desenvolvemos um estimador não-paramétrico mais simples inspirado na estratégia de IPW. Definição 3.46. Considere que {\\mathbf{W}} satisfaz o critério frontdoor para medir o efeito causal de X em Y e que \\widehat{f}({\\mathbf{W}}|X) é um estimador de f({\\mathbf{W}}|X) . Um estimador do tipo IPW para {\\mathbb{E}}[Y|do(X=x)] é dado por \\displaystyle\\widehat{{\\mathbb{E}}}_{f}[Y|do(X=x)] \\displaystyle:=n^{-1}\\sum_{i=1}^{n}\\frac{Y_{i}\\widehat{f}({\\mathbf{W}}_{i}|x)}% {\\widehat{f}({\\mathbf{W}}_{i}|X_{i})}. Para provar o Teorema 3.44 utilizamos o do calculus, que é discutido na Seção 7. Previous page Next page"],[["index.html","chapter3.html","S7.html"],"7 Do-calculus ‣ Capítulo 3 Intervenções ‣ Inferência Causal","Skip to content. Do-calculus 7 Do-calculus O do calculus consiste em um conjunto de regras para alterar densidade envolvend o operador “do”. Por exemplo, o do calculus explica como remover o operador do, trocá-lo pelo condicionamento simples, ou remover algum condicionamento simples. Para apresentar o do calculus, é necessário primeiramente definir algumas modificações sobre o grafo causal. Definição 3.47. Seja ({\\mathcal{G}},f) um CM tal que {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) : \\displaystyle{\\mathcal{G}}(\\bar{{\\mathbb{V}}}) \\displaystyle:=({\\mathcal{V}},\\{E\\in{\\mathcal{E}}:E_{2}\\notin{\\mathbb{V}}\\}) \\displaystyle{\\mathcal{G}}(\\bar{{\\mathbb{V}}}_{1},\\underline{{\\mathbb{V}}}_{2}) \\displaystyle:=({\\mathcal{V}},\\{E\\in{\\mathcal{E}}:E_{2}\\notin{\\mathbb{V}}_{1}% \\text{ e }E_{1}\\notin{\\mathbb{V}}_{2}\\}) \\displaystyle{\\mathcal{G}}(\\bar{{\\mathbb{V}}}_{1},{\\mathbb{V}}_{2}^{+}) \\displaystyle=({\\mathcal{V}}\\cup\\{I_{V}:V\\in{\\mathbb{V}}_{2}\\},\\{E\\in{\\mathcal% {E}}:E_{2}\\notin{\\mathbb{V}}_{1}\\}\\cup\\{(I_{V},V):V\\in{\\mathbb{V}}_{2}\\}) Isto é, {\\mathcal{G}}(\\bar{{\\mathbb{V}}}) é o grafo obtido retirando de {\\mathcal{G}} as arestas que apontam para {\\mathbb{V}} , {\\mathcal{G}}(\\bar{{\\mathbb{V}}}_{1},\\underline{{\\mathbb{V}}}_{2}) é o grafo obtido retirando de {\\mathcal{G}} todas as arestas que apontam para {\\mathbb{V}}_{1} ou que saem de {\\mathbb{V}}_{2} , e {\\mathcal{G}}(\\bar{{\\mathbb{V}}}_{1},{\\mathbb{V}}_{2}^{+}) é o grafo obtido adicionando a {\\mathcal{G}} um novo vértice I_{V} e uma aresta I_{V}\\rightarrow V , para todo V\\in{\\mathbb{V}}_{2} , e retirando todas as arestas que apontam para {\\mathbb{V}}_{1} . Com base na Definição 3.47, é possível apresentar o do calculus: Teorema 3.48. Seja ({\\mathcal{G}},f) um CM e {\\mathbf{X}} , {\\mathbf{Y}} , {\\mathbf{W}} e {\\mathbf{Z}} conjuntos de vértices disjuntos: 1.​Se {\\mathbf{Y}}\\perp^{d}{\\mathbf{Z}}|{\\mathbf{X}}\\cup{\\mathbf{W}} em {\\mathcal{G}}(\\bar{{\\mathbf{X}}}) , então f({\\mathbf{Y}}|do({\\mathbf{X}}),{\\mathbf{Z}},{\\mathbf{W}})=f({\\mathbf{Y}}|do({% \\mathbf{X}}),{\\mathbf{W}}) . 2.​Se {\\mathbf{Y}}\\perp^{d}{\\mathbf{W}}|{\\mathbf{Z}}\\cup{\\mathbf{X}} em {\\mathcal{G}}(\\bar{{\\mathbf{X}}},\\underline{{\\mathbf{W}}}) , então f({\\mathbf{Y}}|do({\\mathbf{X}}),do({\\mathbf{W}}),{\\mathbf{Z}})=f({\\mathbf{Y}}|% do({\\mathbf{X}}),{\\mathbf{W}},{\\mathbf{Z}}) . 3.​Se {\\mathbf{Y}}\\perp^{d}I_{{\\mathbf{X}}}|{\\mathbf{Z}}\\cup{\\mathbf{W}} em {\\mathcal{G}}(\\bar{{\\mathbf{W}}},{\\mathbf{X}}^{+}) , então f(Y|do({\\mathbf{W}}),do({\\mathbf{X}}),{\\mathbf{Z}})=f(Y|do({\\mathbf{W}}),{% \\mathbf{Z}}) . O seguinte lema mostra como o do calculus generaliza certos aspectos do critério backdoor: Lema 3.49. X satisfaz o item 2 do critério backdoor para medir o efeito causal de {\\mathbf{W}} em Y se e somente se Y\\perp^{d}{\\mathbf{W}}|X em {\\mathcal{G}}(\\underline{{\\mathbf{W}}}) . Utilizando o do calculus, é possível obter todas as relações de identificação que são válidas supondo apenas que f é compatível com o grafo causal (Shpitser2006, Shpitser2008). Contudo, às vezes é razoável fazer mais suposições. Discutiremos este tipo de situação no próximo capítulo. 7.1 Exercícios Exercício 3.50 (Glymour2016[p.48]). Considere o modelo estrutural causal em figur 15. (a)​Para cada um dos pares de variáveis a seguir, determine um conjunto de outras variáveis que as d-separa: (Z_{1},W) , (Z_{1},Z_{2}) , (Z_{1},Y) , (Z_{3},W) , e (X,Y) . (b)​Para cada par de variáveis no item anterior, determine se elas são d-separadas dado todas as demais variáveis. (c)​Determine conjuntos de variáveis que satisfazem, respectivamente, o critério backdoor e o critério frontdoor para estimar o efeito causal de X em Y . (d)​Considere que para cada variável, V , temos que V\\equiv\\beta_{V}\\cdot Pa(V)+\\epsilon_{V} , onde os \\epsilon são i.i.d. e normais padrão e \\beta_{V} são vetores conhecidos. Isto é, a distribuição de cada variável é determinada através de uma regressão linear simples em seus pais. Determine f(Y|do(X=x)) utilizando a fórmula do ajuste nos 2 casos abordados no item anterior. Z_{1} Z_{3} Z_{2} X W_{1} W_{2} Y Figura 15: Modelo estrutural causal do Exercício 3.50 Exercício 3.51. Considere que {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) é um grafo causal e {\\mathbf{X}},{\\mathbf{W}},{\\mathbf{Y}}\\subseteq{\\mathcal{V}} . Além disso, para todo caminho, C=(C_{1},\\ldots,C_{n}) , com C_{1}=X\\in{\\mathbf{X}} , C_{n}=Y\\in{\\mathbf{Y}} , e com X\\rightarrow C_{2} , C está bloqueado dado {\\mathbf{W}} . Prove que f({\\mathbf{y}}|do({\\mathbf{X}}))=\\int f({\\mathbf{y}}|{\\mathbf{w}})f({\\mathbf{w% }}|do({\\mathbf{X}}))d{\\mathbf{w}} e {\\mathbb{E}}[Y|do({\\mathbf{X}})]={\\mathbb{E}}[{\\mathbb{E}}[Y|{\\mathbf{W}}]|do(% {\\mathbf{X}})] . Exercício 3.52. Prove que se {\\mathbf{W}} satisfaz o critério frontdoor para medir o efeito causal de X em Y , então f({\\mathbf{W}}|do(X))=f({\\mathbf{W}}|X) e f(Y|do({\\mathbf{W}}))=\\int f(Y|{\\mathbf{W}},X=x^{*})f(X=x^{*})dx^{*} . Exercício 3.53. Prove o Lema 3.49. Previous page Next page"],[["index.html","chapter4.html","S8.html"],"8 Levando a intuição do SCM ao POM ‣ Capítulo 4 Resultados potenciais ‣ Inferência Causal","Skip to content. Levando a intuição do SCM ao POM 8 Levando a intuição do SCM ao POM Ainda que seja uma formalização conveniente, o POM é consideravelmente mais complexo que o SCM original. Para ganhar intuição sobre o POM, alguns lemas de tradução são fundamentais. Lema 4.9. Se {\\mathbb{V}},{\\mathbf{Z}}\\subseteq{\\mathcal{V}} e {\\mathbb{V}}\\cap{\\mathbf{Z}}=\\emptyset , então {\\mathbb{P}}({\\mathcal{V}}_{{\\mathbb{V}}={\\mathbf{v}}}={\\mathcal{V}}_{{\\mathbf% {Z}}={\\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}}|{\\mathbf{Z}}_{{\\mathbb{V}}={% \\mathbf{v}}}={\\mathbf{z}})=1 . Em particular, \\displaystyle{\\mathbb{P}}({\\mathcal{V}}={\\mathcal{V}}_{{\\mathbf{Z}}={\\mathbf{z% }}}|{\\mathbf{Z}}={\\mathbf{z}}) \\displaystyle=1. O Lema 4.9 conecta o dado observacional em {\\mathcal{V}} ao resultado potencial {\\mathcal{V}}_{{\\mathbf{Z}}={\\mathbf{z}}} . Mais especificamente, quando observamos que {\\mathbf{Z}}={\\mathbf{z}} , então os resultados potenciais dada a intervenção {\\mathbf{Z}}={\\mathbf{z}} são idênticos aos resultados observados. Em outras palavras, ao observamos que {\\mathbf{Z}}={\\mathbf{z}} , aprendemos que estamos justamente na hipótese de resultados potenciais em que {\\mathbf{Z}}={\\mathbf{z}} . Lema 4.10. Se {\\mathbb{V}},{\\mathbf{Z}}\\subseteq{\\mathcal{V}} e {\\mathbb{V}}\\cap{\\mathbf{Z}}=\\emptyset , então para todo W\\in{\\mathcal{V}} , \\displaystyle W_{{\\mathbb{V}}={\\mathbf{v}}}{\\mathbb{I}}({\\mathbf{Z}}_{{\\mathbb% {V}}={\\mathbf{v}}}={\\mathbf{z}}) \\displaystyle\\equiv W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}}{% \\mathbb{I}}({\\mathbf{Z}}_{{\\mathbb{V}}={\\mathbf{v}}}={\\mathbf{z}}) O Lema 4.10 é extremamente útil, ainda que de natureza mais técnica. Ele permite que relacionemos resultados potenciais em que diferentes tipos de intervenção são adotados. Um outro resultado essencial é o de que {\\mathcal{V}}_{{\\mathbb{V}}={\\mathbf{v}}} tem a distribuição de quando realizamos a intervenção do({\\mathbb{V}}={\\mathbf{v}}) . Esta resultado é estabelecido no Lema 4.11. Lema 4.11. No modelo de resultados potenciais (Definição 4.7): \\displaystyle f^{*}({\\mathcal{V}}_{{\\mathbb{V}}={\\mathbf{v}}}) \\displaystyle\\equiv f({\\mathcal{V}}|do({\\mathbb{V}}={\\mathbf{v}})). O Lema 4.11 fornece uma outra forma de pensar sobre o efeito causal. Decorre do Lema 4.11 que {\\mathbb{E}}[Y_{X=x}]={\\mathbb{E}}[Y|do(X=x)] . Assim, se por exemplo X é binário, {ACE}={\\mathbb{E}}[Y_{1}]-{\\mathbb{E}}[Y_{0}] . Em outras palavras, como o Definição 4.7 cria variáveis aleatórias que tem a distribuição intervencional, ele permite que imaginemos o efeito causal em termos destas variáveis. Como na capítulo 3 não havia acesso aos resultados potenciais, era necessário imaginar o efeito causal somente em termos da distribuição intervencional. Assim, a Definição 4.7 oferece mais formas de pensar sobre o efeito causal.55 5 Esta outra forma de pensar sobre o efeito causal é tão relevante que outras construções de Inferência Causal, como o Rubin Causal Model (Holland1986) partem diretamente dela. Uma forma alternativa de pensar sobre identificação causal está na definição de ignorabilidade. Dizemos que X é ignorável para medir o efeito causal em Y se ele é independente dos resultados potenciais Y_{x} . Em outras palavras, saber o valor de X não traz informação sobre o resultado de Y em uma outra realidade em que realizamos uma intervenção sobre X . Definição 4.12 (Ignorabilidade). Dizemos que X é ignorável para medir o efeito causal em Y se Y_{x}\\perp^{d}X . O critério da ignorabilidade é equivalente a afirmar que X e Y não tem um ancestral comum. Em outras palavras, X é ignorável se e somente se \\emptyset satisfaz o critério backdoor para medir o efeito causal de X em Y . Lema 4.13. As seguintes afirmações são equivalentes: 1.​ \\emptyset satisfaz o critério backdoor para medir o efeito causal de X em Y , 2.​ Anc(X)\\cap Anc(Y)=\\emptyset , isto é, X e Y não tem um ancestral em comum, e 3.​ X é ignorável para medir o efeito causal em Y . Assim, decorre do fato de que X é ignorável para o efeito causal em Y que a distribuição intervencional de Y dado X é equivalente à sua distribuição observacional. Em outras palavras, dizer que X é ignorável tem consequências similares a dizer que X é atribuído por aleatorização. Corolário 4.14. Se X é ignorável para medir o efeito causal em Y , então \\displaystyle f(y|do(x)) \\displaystyle=f(y|x). A ignorabilidade condicional oferece uma generalização da Definição 4.12. Dizemos que, dado {\\mathbf{Z}} , X é ignorável para medir o efeito causal em Y se X é independente de todo Y_{x} dado {\\mathbf{Z}} . Definição 4.15 (Ignorabilidade condicional). Dizemos que X é condicionalmente ignorável para medir o efeito causal em Y dado {\\mathbf{Z}} se Y_{x}\\perp^{d}X|{\\mathbf{Z}} . Se {\\mathbf{Z}} não tem descendentes de X , a ignorabilidade condicional é uma restrição mais forte que o critério backdoor, conforme formalizado no Lema 4.16. Lema 4.16. Suponha que X\\notin Anc({\\mathbf{Z}}) . Se X é condicionalmente ignorável para medir o efeito causal em Y dado {\\mathbf{Z}} , então {\\mathbf{Z}} satisfaz o critério backdoor para medir o efeito causal de X em Y . Apesar do critério backdoor e das ignorabilidade condicional não serem equivalentes, eles induzem o mesmo tipo de identificação causal. Lema 4.17. Se X é condicionalmente ignorável para medir o efeito causal em Y dado {\\mathbf{Z}} , então {\\mathbf{Z}} controla confundidores para medir o efeito causal de X em Y (Definição 3.18). Decorre do Lema 4.17 que todas as estratégia de estimação do efeito causal estudadas na Seção 5 também podem ser usadas sob a suposição de ignorabilidade condicional. Em outras palavras, ignorabilidade condicional fornece um critério alternativo para justificar o tipo de identificação causal obtida pelo critério backdoor. 8.1 Exercícios Exercício 4.18. Prove o Lema 4.1. Exercício 4.19. Mostre que no Exemplo 4.4 a distribuição de (X,Y) no SCM em equações estruturais é igual àquela no SCM original. Exercício 4.20. Exiba um exemplo em que X é condicionalmente ignorável para Y dado {\\mathbf{Z}} mas {\\mathbf{Z}} não satisfaz o critério backdoor para medir o efeito causal de X em Y . Exercício 4.21. Exiba um exemplo em que {\\mathbf{Z}} satisfaz o critério backdoor para medir o efeito causal de X em Y mas X não é condicionalmente ignorável para Y dado {\\mathbf{Z}} . Previous page Next page"],[["index.html","chapter4.html","S9.html"],"9 Variáveis Instrumentais ‣ Capítulo 4 Resultados potenciais ‣ Inferência Causal","Skip to content. Variáveis Instrumentais 9 Variáveis Instrumentais Há situações em que não nos sentimentos confortáveis com a suposição de que observamos todos os confundidores ou todos os mediadores de X a Y . Nestes casos, não é possível justificar os métodos baseados nos critérios backdoor e frontdoor vistos na capítulo 3. Variáveis instrumentais são um modo de evitar esse tipo de suposição. Intuitivamente, uma variável instrumental, I , tem todo o seu efeito causal sobre Y mediado por X . Em outra palavras, a única forma em que I tem efeito sobre Y é na medida em que I tem efeito sobre X e, por sua vez, X tem efeito sobre Y . Por exemplo, Angrist1990 estuda como participar da guerra do Vietnam, X , tem efeito sobre a renda de um indivíduo, Y . Para tal, o estudo considera os sorteios que foram realizados para determinar quem era recrutado para a guerra. O único efeito que o sorteio tem sobre a renda de um indivíduo é indireto, apenas na medida em que afeta a probabilidade de este indivíduo ir para a guerra. Com base neste tipo de variável, sob certas circunstâncias é possível estimar o efeito causal de X em Y . A ideia básica é a de que, fazendo intervenções em I , vemos mudanças tanto em X quanto em Y . Como as mudanças em Y devem-se apenas às mudanças que ocorreram em X , pode ser possível estimar o efeito causal de X em Y . Dada esta intuição, podemos definir formalmente uma variável instrumental. Para tal, iremos seguir de perto a abordagem em Angrist1996. Definição 4.22. Dizemos que I é um instrumento para medir o efeito causal de X em Y se 1.​ I é ignorável para medir o efeito em Y . 2.​ Y_{I=i,X=x}\\equiv Y_{X=x} , para todo f compatível com {\\mathcal{G}} . 3.​ Cov[I,X]\\neq 0 . Apesar de as condições na Definição 4.22 terem sido utilizadas originalmente por Angrist1996, é possível reinterpretá-las utilizando o grafo causal. Já vimos no Lema 4.13 que a primeira condição é equivalente a dizer que \\emptyset satisfaz o critério backdoor para medir o efeito causal de I em Y . Isto é, I e Y não tem ascendentes em comum no grafo causal. Além disso, a segunda condição é equivalente a afirmar que no grafo causal todo caminho direcionado de I a Y passa por X . Isto é, X é o mediador do efeito causal de I em Y . Este resultado é apresentado no Lema 4.23. Lema 4.23. Y_{I=i,X=x}\\equiv Y_{X=x} , para todo f compatível com {\\mathcal{G}} se e somente se todo caminho direcionado de I a Y , C , é tal que existe j com C_{j}=X . Sob algumas circunstâncias, a existência de um instrumento é suficiente para que seja possível identificar o efeito causal. Uma suposição usual é de que estamos analisando um CM linear Gaussiano (Definição 2.25). Teorema 4.24. Se ({\\mathcal{G}},f) é um CM linear Gaussiano e I é um instrumento para medir o efeito causal de X em Y , então \\displaystyle{ACE} \\displaystyle=\\frac{Cov[I,Y]}{Cov[I,X]} Caso o modelo causal não seja linear Gaussiano, então mais suposições são necessárias para identificar o efeito causal com base em um instrumento, Uma suposição usual é a de monotonicidade do instrumento. Segundo esta, ao aumentar o valor do instrumento por uma intervenção, o valor de X necessariamente irá aumentar Definição 4.25. I é um instrumento monotônico para medir o efeito causal de X em Y se, para todo i_{1}>i_{0} , \\displaystyle{\\mathbb{P}}(X_{I=i_{1}}>X_{I=i_{0}})=1. O Instrumento monotônico foi originalmente contextualizado em uma aplicação a alistados na Guerra do Vietnam Angrist1990. Pode-se imaginar que a população é dividida em 4 grupos. Pessoas que sempre iriam à guerra (always-taker), que nunca iriam à guerra (never-taker), que iriam à guerra somente se alistados (compliers), e que iriam à guerra somente se não alistados (defiers). Neste caso, o alistamento ser um instrumento monotônico corresponde a afirmar que não existem pessoas no último grupo. Quando o instrumento é monotônico e X e I são binários, é possível identificar o efeito causal de X em Y em uma sub-população. Especificamente, é possível identificar o efeito de X em Y na sub-população em que o resultado potencial de X é diferente para cada intervenção em I . No exemplo da Guerra do Vietnam, esta é a sub-população dos compliers, isto é, indivíduos que iriam à guerra somente se alistados. A definição de Local Average Treatment Effect (LATE) é formalizada abaixo: Definição 4.26. Se X,I\\in\\{0,1\\} , então \\displaystyle{LATE} \\displaystyle={\\mathbb{E}}[Y_{X=1}-Y_{X=0}|X_{I=1}-X_{I=0}=1]. O Teorema 4.27 mostra como identificar o LATE por meio de um instrumento monotônico. Teorema 4.27. Se I\\in\\ \\{0,1\\} é um instrumento monotônico para o efeito causal de X\\in\\{0,1\\} em Y , então \\displaystyle{LATE} \\displaystyle=\\frac{{\\mathbb{E}}[Y|I=1]-{\\mathbb{E}}[Y|I=0]}{{\\mathbb{E}}[X|I=% 1]-{\\mathbb{E}}[X|I=0]}. Previous page Next page"],[["index.html","chapter5.html","bib.html"],"Referências ‣ Capítulo 5 Descoberta Causal ‣ Inferência Causal","Skip to content. Referências Referências Previous page Next page"],[["index.html","chapter1.html"],"Capítulo 1 Por que estudar Inferência Causal? ‣ Inferência Causal","Skip to content. Por que estudar Inferência Causal? Capítulo 1 Por que estudar Inferência Causal? Você já deve ter ouvido diversas vezes que correlação não implica causalidade. Contudo, o que é causalidade e como ela pode ser usada para resolver problemas práticos? Antes de estudarmos definições formais, veremos como conceitos intuitivos de causalidade podem ser necessários para resolver questões usuais em Inferência Estatística. Para tal, a seguir estudaremos um exemplo de Glymour2016. Previous page Next page"],[["index.html","chapter2.html"],"Capítulo 2 Modelo Causal (CM) ‣ Inferência Causal","Skip to content. Modelo Causal (CM) Capítulo 2 Modelo Causal (CM) No capítulo 1 vimos que as relações causais entre variáveis são essenciais para conseguirmos determinar o efeito que uma variável pode ter em outra. Contudo, como podemos especificar relações causais formalmente? Como resposta a esta pergunta iremos definir o Modelo Causal (CM ), que permite especificar formalmente relações causais. Para tal, será necessário primeiro introduzir modelos probabilísticos em grafos. Um curso completo sobre estes modelos pode ser encontrado, por exemplo, em Maua2022. A seguir, estudaremos resultados essenciais destes modelos. Previous page Next page"],[["index.html","chapter3.html"],"Capítulo 3 Intervenções ‣ Inferência Causal","Skip to content. Intervenções Capítulo 3 Intervenções Previous page Next page"],[["index.html","chapter4.html"],"Capítulo 4 Resultados potenciais ‣ Inferência Causal","Skip to content. Resultados potenciais Capítulo 4 Resultados potenciais No capítulo passado, vimos que f(y|do(x)) nos permite entender o comportamento de Y em um cenário distinto dos dados observados. Por exemplo, se X é a indicadora de um tratamento e Y é a indicadora de cura, então f(y|do(X=1)) nos permite entender a proporção de cura em um cenário hipotético em que administramos o tratamento a todos os indivíduos. Esta distribuição nos permite investigar questões causais que não eram acessíveis usando apenas a distribuição observacional, f(y,x) . Contudo, algumas perguntas causais não são respondidas utilizando apenas os mecanismos desenvolvidos no capítulo 3. Por exemplo, qual a probabilidade de que um indivíduo se cure quando recebe o tratamento e não se cure quando não o recebe. Quando tentamos traduzir esta questão, notamos que partes dela envolvem Y=1 e do(X=1) e outras partes envolvem Y=0 e do(X=0) . Se tentarmos uma tradução ingênua, podemos obter uma expressão como {\\mathbb{P}}(Y=1,Y=0|do(X=1),do(X=0)) . Contudo, a probabilidade acima não responde à pergunta colocada. Em primeiro lugar, não está definido fazermos as intervenções do(X=1) e do(X=0) na mesma unidade amostral. Além disso, mesmo que a probabilidade estivesse definida, é impossível que o mesmo Y assuma tanto o valor 1 quanto 0 . Isto é, {\\mathbb{P}}(Y=1,Y=0|\\ldots)=0 . A última constatação nos revela que o modelo no capítulo 3 não tem variáveis suficientes para traduzir a pergunta levantada. Se imaginamos que é possível que um indivíduo se cure ao receber o tratamento e não se cure quando não o recebe, isto ocorre pois as ocorrências de cura em cada cenário hipotético não são logicamente equivalentes. Em outras palavras, é como se houvessem resultados potenciais22 2 Esta é uma tradução livre da expressão “potential outcomes” usada em inglês., Y_{1} e Y_{0} , para indicar a ocorrência de cura em cada cenário considerado. Com o uso destas variáveis, poderíamos escrever {\\mathbb{P}}(Y_{1}=1,Y_{0}=0) . O objetivo desta seção é incluir este tipo de variável de forma a preservar as ferramentas desenvolvidas no capítulo 3.33 3 Para tal, adotaremos uma construção baseada em Galles1998. Neste quesito, a maior dificuldade será estabelecer a distribuição conjunta entre os resultados potenciais. Para tal, será útil relembrar um lema fundamental em simulação: Lema 4.1. Considere que F(v|Pa(V)) é uma função de densidade acumulada condicional arbitrária e U\\sim U(0,1) . Se definirmos, V\\equiv F^{-1}(U|Pa(V)) , então V|Pa(V)\\sim F . O Lema 4.1 traz várias interpretações que nos serão úteis. A primeira interpretação, de caráter técnico, é que podemos simular de qualquer distribuição multivariada utilizando apenas variáveis i.i.d. e funções determinísticas. Em particular, podemos reescrever um SCM de tal forma que cada vértice, V , seja função determinística de seus pais e uma variável de ruído, U_{V} . Esta abordagem, que está ligada a modelos de equações estruturais, é apresentada nas Definições 4.2 e 4.3. Definição 4.2. Seja {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) um grafo causal. O grafo causal estrutural, {\\mathcal{G}}^{+}=({\\mathcal{V}}^{+},{\\mathcal{E}}^{+}) , é tal que {\\mathcal{V}}^{+}={\\mathcal{V}}\\cup(U_{V})_{V\\in{\\mathcal{V}}} e {\\mathcal{E}}^{+}={\\mathcal{E}}\\cup\\{(U_{V},V):V\\in{\\mathcal{V}}\\} . Isto é, para cada V\\in{\\mathcal{V}} , {\\mathcal{G}}^{+} adiciona uma nova variável U_{V} e uma aresta de U_{V} a V . Definição 4.3. Seja ({\\mathcal{G}},f) um CM. O Modelo Estrutural Causal (SCM) para ({\\mathcal{G}},f) , ({\\mathcal{G}}^{+},f^{+}) , é tal que {\\mathcal{G}}^{+} é o grafo causal estrutural de {\\mathcal{G}} , (U_{V})_{V\\in{\\mathcal{V}}} são independentes segundo f^{+} e, para cada V\\in{\\mathcal{V}} , existe uma função determinística, g_{V}:U_{V}\\times Pa(V)\\rightarrow\\Re , tal que f^{+}(V|U_{V},Pa(V))={\\mathbb{I}}(V=g_{V}(U_{V},Pa(V))) e f^{+}({\\mathcal{V}})=f({\\mathcal{V}}) . O Exemplo 4.4 ilustra uma forma de obter um SCM em equações estruturais a partir de um SCM com dois vértices. Exemplo 4.4. Considere que X\\rightarrow Y , X\\sim\\text{Exp}(1) e Y|X\\sim\\text{Exp}(X) . Neste caso, o grafo estrutural causal é dado por U_{X}\\rightarrow X\\rightarrow Y\\leftarrow U_{Y} . Além disso, existem várias representações do SCM em equações estruturais. Uma possibilidade é definir que U_{X} e U_{Y} são i.i.d. e U(0,1) , X\\equiv-\\log(U_{X}) e Y\\equiv-\\log(U_{Y})/X . O Lema 4.1 também permite uma interpretação de caráter mais filosófico. Podemos imaginar que toda variável em um SCM, V , é uma função determinística de seus pais e de condições locais não-observadas, U_{V} . A expressão “condições locais” indica que cada U_{V} é usada somente para gerar V e que as variáveis em U são independentes, isto é, não trazem informação umas sobre as outras. A interpretação acima é usada na definição de resultados potenciais. A ideia principal é que as mesmas funções determínistas e variáveis de ruído locais são usadas para gerar todos os resultados potenciais. A única diferença é que, para cada resultado potencial, o valor das variáveis em que houve intervenção é fixado. Esta definição é compatível com a ideia de que não é possível modificar os ruídos locais por meio da intervenção. Em outras palavras, o resultado potencial é o mais próximo possível do resultado observado sob a restrição que fixamos os valores das variáveis em que houve intervenção. Definição 4.5. Seja ({\\mathcal{G}},f) um CM de grafo causal {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) e ({\\mathcal{G}}^{+},f^{+}) o seu SCM. O grafo de resultados potenciais dado por intervenções em {\\mathbf{X}}\\subseteq{\\mathcal{V}} , {\\mathcal{G}}^{*}=({\\mathcal{V}}^{*},{\\mathcal{E}}^{*}) é tal que \\displaystyle{\\mathcal{V}}^{*} \\displaystyle=\\{W_{{\\mathbb{V}}={\\mathbf{v}}}:W\\in{\\mathcal{V}},{\\mathbb{V}}% \\subseteq{\\mathbf{X}},{\\mathbf{v}}\\in supp({\\mathbb{V}})\\}\\cup\\{U_{W}:W\\in{% \\mathcal{V}}\\}, \\displaystyle{\\mathcal{E}}^{*} \\displaystyle=\\{(W_{{\\mathbb{V}}={\\mathbf{v}}},Z_{{\\mathbb{V}}={\\mathbf{v}}}):% {\\mathbb{V}}\\subseteq{\\mathbf{X}},{\\mathbf{v}}\\in supp({\\mathbb{V}}),(W,Z)\\in{% \\mathcal{E}}^{+},Z\\notin{\\mathbb{V}}\\}. Para todo W\\in{\\mathcal{V}} , abreviamos W_{\\emptyset} por W . Em palavras, o grafo de resultados potenciais cria uma cópia de {\\mathcal{G}} para cada possível intervenção, {\\mathbb{V}}={\\mathbf{v}} . Além disso, adiciona-se uma aresta de U_{W} para cada cópia de W . Esta construção indica que as mesmas variáveis em U geram todos os resultados potenciais. Também, para cada vértice em que houve uma intervenção, W_{{\\mathbb{V}}={\\mathbf{v}}}\\in{\\mathbb{V}}_{{\\mathbb{V}}={\\mathbf{v}}} , removem-se todas as arestas que apontam para W_{{\\mathbb{V}}={\\mathbf{v}}} . Esta remoção ocorre porque, quando realizamos uma intervenção em {\\mathbb{V}} o valor desta variável é fixado e, assim, não é gerado por suas causas em {\\mathcal{G}} . Exemplo 4.6. Considere que (X,Y)\\in\\{0,1\\}^{2} e o grafo causal é X\\rightarrow Y . Vimos no Exemplo 4.4 que o grafo causal estrutural é dado por U_{X}\\rightarrow X\\rightarrow Y\\leftarrow U_{Y} . Vamos construir o grafo de resultados potenciais dadas intervenções em X . Neste caso, além dos vértices U_{X},U_{Y},X,Y , temos também X_{X=0},Y_{X=0},X_{X=1},Y_{X=1} . Como não há ambiguidade neste caso, podemos abreviar os últimos quatro vértices por X_{0},Y_{0},X_{1},Y_{1} . O grafo de resultados potenciais é ilustrado na figur 16. O grafo causal estrutural é a reta horizontal de U_{X} a U_{Y} . Os resultados potenciais são cópias deste grafo que usam as mesmas variáveis U e em que removemos as arestas que apontam para a intervenções, X_{0} e X_{1} . Refer to caption Figura 16: Grafo de resultados potenciais dadas intervenções em X\\in\\{0,1\\} . Uma vez definido o grafo de resultados potenciais, podemos extender a distribuição do modelo de equações estruturais para este grafo. Esta extensão envolve três etapas. Primeiramente, a distribuição de U continua a mesma. Em segundo lugar, para todo vértice do grafo de resultados potenciais, W_{{\\mathbb{V}}={\\mathbf{v}}} , em que não houve uma intervenção, este vértice é gerado pelo mesmo mecanismo que W . Isto é, W_{{\\mathbb{V}}={\\mathbf{v}}}={\\mathbb{I}}(g_{W}(U_{W},Pa^{*}(W_{{\\mathbb{V}}=% {\\mathbf{v}}}))) . Finalmente, se houve uma intervenção em W_{{\\mathbb{V}}={\\mathbf{v}}} , então ela é uma variável degenerada no valor desta intervenção. Esta construção é formalizada na Definição 4.7. Definição 4.7. Seja ({\\mathcal{G}}^{+},f^{+}) um SCM para ({\\mathcal{G}},f) com funções determinísticas, g . O modelo de resultados potenciais (POM)44 4 utilizamos a sigla POM em referência ao termo em inglês “potential outcomes model” dado por intervenções em {\\mathbf{X}} , é um modelo probabilístico em um DAG, ({\\mathcal{G}}^{*},f^{*}) , tal que {\\mathcal{G}}^{*} é o grafo de resultados potenciais dado por intervenções em {\\mathbf{X}} (Definição 4.5) e \\displaystyle f^{*}(U_{W}) \\displaystyle=f(U_{W}) \\displaystyle\\text{, para todo }W\\in{\\mathcal{V}}, \\displaystyle f^{*}(W_{{\\mathbb{V}}={\\mathbf{v}}}|Pa^{*}(W_{{\\mathbb{V}}={% \\mathbf{v}}})) \\displaystyle=\\begin{cases}{\\mathbb{I}}(W_{{\\mathbb{V}}={\\mathbf{v}}}={\\mathbf% {v}}_{i})&\\text{, se }W\\equiv{\\mathbb{V}}_{i}\\\\ {\\mathbb{I}}(W_{{\\mathbb{V}}={\\mathbf{v}}}=g_{W}(U_{W},Pa^{*}(W_{{\\mathbb{V}}=% {\\mathbf{v}}})))&\\text{, caso contrário.}\\end{cases} O Exemplo 4.8 ilustra um modelo de resultados potenciais. Exemplo 4.8. Considere o SCM em equações estruturais em Exemplo 4.4. Na construção do modelo de resultados potenciais, definimos X,Y,U_{X},U_{Y} igualmente a em Exemplo 4.4. Além disso, para cada x>0 , X_{x}\\equiv x e Y_{x}\\equiv-log(U_{Y})/X_{x} . Previous page Next page"],[["index.html","chapter5.html"],"Capítulo 5 Descoberta Causal ‣ Inferência Causal","Skip to content. Descoberta Causal Capítulo 5 Descoberta Causal Há situações em que observamos dados de um CM desconhecido, ({\\mathcal{G}},f) . Nestes casos, podemos ter interesse em estimar o grafo causal, {\\mathcal{G}} . Neste capítulo, estudaremos estratégias para esta estimação. Antes de apresentar métodos de estimação, avaliaremos condições para que o problema de estimação esteja bem posto. Especificamente, a próxima seção mostra que, de forma irrestrita, o grafo causal é estatisticamente não-identificável. Assim, comumente precisamos nos contentar com um objetivo menos ambicioso de estimação. Previous page Next page"],[["index.html","chapter6.html","chapter6.A1.html"],"Apêndice 6.A Seção 2.4 (2.4 Exemplos de Modelo Probabilístico em um DAG) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. ( Apêndice 6.A Seção 2.4 (Exemplos de Modelo Probabilístico em um DAG) Prova do Lema 2.18. \\displaystyle f(v_{1},v_{3}|v_{2}) \\displaystyle=\\frac{f(v_{1},v_{2},v_{3})}{f(v_{2})} \\displaystyle=\\frac{f(v_{2})f(v_{1}|v_{2})f(v_{3}|v_{2})}{f(v_{2})} \\displaystyle=f(v_{1}|v_{2})f(v_{3}|v_{2}) ∎ Prova do Lema 2.19. Considere que V_{2}\\sim\\text{Bernoulli}(0.02) . Além disso, V_{1},V_{3}\\in\\{0,1\\} são independentes dado V_{2} . Também, {\\mathbb{P}}(V_{1}=1|V_{2}=1)={\\mathbb{P}}(V_{3}=1|V_{2}=1)=0.9 e {\\mathbb{P}}(V_{1}=1|V_{2}=0)={\\mathbb{P}}(V_{3}=1|V_{2}=0)=0.05 . Note que, por construção, {\\mathbb{P}} é compatível com figur 2. Isto é, P(v_{1},v_{2},v_{3})={\\mathbb{P}}(v_{2}){\\mathbb{P}}(v_{1}|v_{2}){\\mathbb{P}}(% v_{3}|v_{2}) . Além disso, \\displaystyle{\\mathbb{P}}(V_{1}=1) \\displaystyle={\\mathbb{P}}(V_{1}=1,V_{2}=1)+{\\mathbb{P}}(V_{1}=1,V_{2}=0) \\displaystyle={\\mathbb{P}}(V_{2}=1){\\mathbb{P}}(V_{1}=1|V_{2}=1)+{\\mathbb{P}}(% V_{2}=0){\\mathbb{P}}(V_{1}=1|V_{2}=0) \\displaystyle=0.02\\cdot 0.9+0.98\\cdot 0.05=0.067 Por simetria, {\\mathbb{P}}(V_{3}=1)=0.067 . Além disso, \\displaystyle{\\mathbb{P}}(V_{1}=1,V_{3}=1) \\displaystyle={\\mathbb{P}}(V_{1}=1,V_{3}=1,V_{2}=1)+{\\mathbb{P}}(V_{1}=1,V_{3}% =1,V_{2}=0) \\displaystyle={\\mathbb{P}}(V_{2}=1){\\mathbb{P}}(V_{1}=1|V_{2}=1){\\mathbb{P}}(V% _{3}=1|V_{2}=1)+{\\mathbb{P}}(V_{2}=0){\\mathbb{P}}(V_{1}=1|V_{2}=0){\\mathbb{P}}% (V_{3}=1|V_{2}=0) \\displaystyle=0.02\\cdot 0.9\\cdot 0.9+0.98\\cdot 0.05\\cdot 0.05=0.01865 Como {\\mathbb{P}}(V_{1}=1){\\mathbb{P}}(V_{3}=1)=0.067\\cdot 0.067\\approx 0.0045\\neq 0% .01865={\\mathbb{P}}(V_{1}=1,V_{3}=1) , temos que V_{1} e V_{3} não são independentes. ∎ Prova do Lema 2.20. \\displaystyle f(v_{3}|v_{1},v_{2}) \\displaystyle=\\frac{f(v_{1},v_{2},v_{3})}{f(v_{1},v_{2})} \\displaystyle=\\frac{f(v_{1})f(v_{2}|v_{1})f(v_{3}|v_{2})}{f(v_{1})f(v_{2}|v_{1% })} \\displaystyle=f(v_{3}|v_{2}) ∎ Prova do Lema 2.21. Considere que V_{1}\\sim\\text{Bernoulli}(0.5) , {\\mathbb{P}}(V_{2}=1|V_{1}=1)=0.9 , {\\mathbb{P}}(V_{2}=1|V_{1}=0)=0.05 , {\\mathbb{P}}(V_{3}=1|V_{2}=1,V_{1})=0.9 , e {\\mathbb{P}}(V_{3}=1|V_{2}=0,V_{1})=0.05 . Note que (V_{1},V_{2},V_{3}) formam uma Cadeia de Markov. Note que, por construção, {\\mathbb{P}} é compatível com figur 3. Isto é, P(v_{1},v_{2},v_{3})={\\mathbb{P}}(v_{1}){\\mathbb{P}}(v_{2}|v_{1}){\\mathbb{P}}(% v_{3}|v_{2}) . Além disso, \\displaystyle{\\mathbb{P}}(V_{3}=1) \\displaystyle={\\mathbb{P}}(V_{1}=0,V_{2}=0,V_{3}=1)+{\\mathbb{P}}(V_{1}=0,V_{2}% =1,V_{3}=1) \\displaystyle+{\\mathbb{P}}(V_{1}=1,V_{2}=0,V_{3}=1)+{\\mathbb{P}}(V_{1}=1,V_{2}% =1,V_{3}=1) \\displaystyle=0.5\\cdot 0.9\\cdot 0.05+0.5\\cdot 0.05\\cdot 0.9 \\displaystyle+0.5\\cdot 0.05\\cdot 0.05+0.5\\cdot 0.9\\cdot 0.9=0.45125 Além disso, \\displaystyle{\\mathbb{P}}(V_{1}=1,V_{3}=1) \\displaystyle={\\mathbb{P}}(V_{1}=1,V_{2}=0,V_{3}=1)+{\\mathbb{P}}(V_{1}=1,V_{2}% =1,V_{3}=1) \\displaystyle=0.5\\cdot 0.05\\cdot 0.9+0.5\\cdot 0.9\\cdot 0.9=0.40625 Como {\\mathbb{P}}(V_{1}=1){\\mathbb{P}}(V_{3}=1)=0.5\\cdot 0.45125\\approx 0.226\\neq 0% .40625={\\mathbb{P}}(V_{1}=1,V_{3}=1) , temos que V_{1} e V_{3} não são independentes. ∎ Prova do Lema 2.22. \\displaystyle f(v_{1},v_{3}) \\displaystyle=\\int f(v_{1},v_{2},v_{3})dv_{2} \\displaystyle=\\int f(v_{1})f(v_{3})f(v_{2}|v_{1},v_{3})dv_{2} \\displaystyle=f(v_{1})f(v_{3})\\int f(v_{2}|v_{1},v_{3})dv_{2} \\displaystyle=f(v_{1})f(v_{3}) ∎ Prova do Lema 2.23. Considere que V_{1} e V_{3} são independentes e tem distribuição \\text{Bernoulli}(0.5) . Além disso, V_{2}\\equiv V_{1}+V_{3} . Como {\\mathbb{P}}(V_{3}=1)=0.5 e {\\mathbb{P}}(V_{3}=1|V_{1}=1,V_{2}=2)=1 , conclua que V_{1}\\not\\perp\\!\\!\\!\\!\\perp V_{3}|V_{2} . ∎ Previous page Next page"],[["index.html","chapter6.html","chapter6.A10.html"],"Apêndice 6.J Seção 11 (11 Identificabilidade na Descoberta Causal) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. ( Apêndice 6.J Seção 11 (Identificabilidade na Descoberta Causal) Lema 6.24 (Verma2022). Para quaisquer vértices V_{1},V_{2}\\in{\\mathcal{V}} em um grafo causal, {\\mathcal{G}} , as seguintes afirmações são equivalentes: 1.​ V_{1} e V_{2} são adjacentes, 2.​Não existe {\\mathbb{V}}\\subseteq{\\mathcal{V}}-\\{V_{1},V_{2}\\} tal que V_{1}\\perp V_{2}|{\\mathbb{V}} , 3.​ V_{1} e V_{2} não são d-separados dado A:=Anc(\\{V_{1},V_{2}\\})-\\{V_{1},V2\\} , 4.​ V_{1} e V_{2} não são d-separados dado Pa:=Pa(\\{V_{1},V_{2}\\})-\\{V_{1},V2\\} . Demonstração. (1\\rightarrow 2) Sem perda de generalidade, suponha que V_{1}\\rightarrow V_{2} . Inicialmente, construíremos uma f compatível com {\\mathcal{G}} . Para todo V\\notin\\{V_{1},V_{2}\\} , tomamos f(V|Pa(V))={\\mathbb{I}}(V=0) . Isto é, V é degenerado em 0 . Além disso, tomamos V_{1}|Pa(V_{1})\\sim\\text{Bernoulli}(0.5) e V_{2}|Pa(V_{2})\\equiv V_{1} . Para todo {\\mathbb{V}}\\subseteq{\\mathcal{V}}-\\{V_{1},V_{2}\\} , {\\mathbb{P}}({\\mathbb{V}}=0)=1 . Portanto, Cov(V_{1},V_{2}|{\\mathbb{V}})=Cov(V_{1},V_{2})=0.25\\neq 0 . Portanto, V_{1} e V_{2} não são independentes dado {\\mathbb{V}} segundo f . Como f é compatível com {\\mathcal{G}} , decorre do Teorema 2.49 que V_{1} e V_{2} não são d-separados dado {\\mathbb{V}} . (2\\rightarrow 3) Decorre do fato de que Anc(\\{V_{1},V_{2}\\})-\\{V_{1},V_{2}\\} é um caso particular de {\\mathbb{V}} em (2). (3\\rightarrow 4) Provaremos que se existe um caminho de V_{1} em V_{2} , C , que não está bloqueado dado A , ele também não está bloqueado dado Pa . Faremos esta prova em duas etapas: primeiramente considerando vértices em C que não sejam colisores e, a seguir, que sejam colisores. Tome um vértice em C , C_{i} , que não é um colisor. Como C não está bloqueado dado A , C_{i}\\notin A . Como Pa\\subseteq A , C_{i}\\notin Pa . A seguir, tome um vértice em C , C_{i} , que é um colisor. Como C não está bloqueado dado A , existe V_{1}\\in A tal que V_{1} é descendente de C . Como V_{1}\\in A , existe V_{2}\\in Pa tal que V_{2}=V_{1} ou V_{2} é descendente de V_{1} . Portanto, V_{2}\\in Pa é descendente de C_{i} . Decorre das conclusões anteriores que C não está bloqueado dado Pa . (4\\rightarrow 1) Considere que V_{1} e V_{2} não são adjacentes e suponha por abusrdo que há um caminho de V_{1} a V_{2} , C , não bloqueado dado Pa . Como V_{1} e V_{2} não são adjacentes, |C|>2 . Caso, V_{1}\\leftarrow C_{2} , então C_{2}\\in Pa e C_{2} não é um colisor, isto é, C está bloqueado dado Pa . Conclua que V_{1}\\rightarrow C_{2} . Por simetria, conclua que C_{n-1}\\rightarrow V_{2} . Como V_{1}\\rightarrow C_{2} e C_{n-1}\\rightarrow V_{2} , há pelo menos um colisor em C . Defina C_{i} como o colisor de menor índice (o mais próximo de V_{1} ) e C_{j} o de maior índice (o mais próximo de V_{2} ). Por definição construção, C_{i} é descendente de V_{1} e C_{j} é descendente de V_{2} . Note que, se C_{i} é ascendente de V_{2} e C_{j} é ascendente de V_{1} , então \\displaystyle V_{1}\\rightarrow\\ldots\\rightarrow C_{i}\\rightarrow\\ldots% \\rightarrow V_{2}\\rightarrow\\ldots\\rightarrow C_{j}\\rightarrow\\ldots% \\rightarrow V_{1}, é um ciclo em {\\mathcal{G}} . Como {\\mathcal{G}} é um DAG, ou C_{i} não é ascendente de V_{2} ou C_{j} não é ascendente de V_{1} . Sem perda de generalidade, considere que C_{i} não é ascendente de V_{2} . Como C_{i} é descendente de V_{1} , C_{i} também não é ascendente de V_{1} . Como C_{i} não é ascendente de V_{1} ou de V_{2} , Não há vértice em Pa que é descendente de C_{i} . Portanto, C_{i} é um colisor e não há descendente de C_{i} em Pa . Conclua que C está bloqueado dado Pa , um absurdo. Portanto, V_{1}\\perp V_{2}|Pa . ∎ Lema 6.25 (Verma2022). Considere que {\\mathcal{G}} é tal que V_{1} é adjacente a V_{2} , V_{2} é adjacente a V_{3} , mas V_{1} não é adjacente a V_{3} . Temos que V_{1}\\rightarrow V_{2} e V_{2}\\leftarrow V_{3} se e somente se V_{1} não é d-separado de V_{3} dado qualquer {\\mathbb{V}} tal que V_{2}\\in{\\mathbb{V}} . Demonstração. ( \\rightarrow ) Considere que V_{1}\\rightarrow V_{2} e V_{2}\\leftarrow V_{3} . Tome o caminho C=V_{1}\\rightarrow V_{2}\\leftarrow V_{3} . Como V_{2} é um colisor em C , C não está bloqueado dado qualquer {\\mathbb{V}} tal que V_{2}\\in{\\mathbb{V}} . Conclua que V_{1} não é d-separado de V_{3} dado qualquer {\\mathbb{V}} tal que V_{2}\\in{\\mathbb{V}} . ( \\leftarrow ) Considere que V_{1}\\leftarrow V_{2} ou V_{2}\\rightarrow V_{3} . Portanto, V_{2}\\in Pa:=Pa(\\{V_{1},V_{3}\\})-\\{V_{1},V_{3}\\} . Como V_{1} não é adjacente a V_{3} , conclua do Lema 6.24 que V_{1}\\perp V_{3}|Pa . ∎ Lema 6.26. Se {\\mathcal{G}} e {\\mathcal{G}}^{*} não tem o mesmo padrão, então {\\mathcal{G}} e {\\mathcal{G}}^{*} não são fielmente equivalentes. Demonstração. Decorre diretamente dos Lemas 6.24 e 6.25. ∎ Lema 6.27. Se {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão, V_{1}\\rightarrow V_{2}\\leftarrow V_{3} é um colisor tanto em {\\mathcal{G}} quanto em {\\mathcal{G}}^{*} e Z é tal que V_{2}=Z ou Z é um descendente de V_{2} em {\\mathcal{G}} , então ou V_{1} e V_{3} são adjacentes em ambos os DAGs, ou existe um vértice V_{*} tal que V_{1}\\rightarrow V_{*}\\leftarrow V_{3} é um colisor em {\\mathcal{G}}^{*} e Z=V_{*} ou Z é descendente de V_{*} em {\\mathcal{G}} . Demonstração. Para provar este resultado basta mostrar que, se V_{1} e V_{3} não são adjacentes nas condições do lema, então existe V_{*} com as condições especificadas. Como Z é descendente de V_{2} em {\\mathcal{G}} , existe um caminho direcionado em {\\mathcal{G}} , C=(C_{1},\\ldots,C_{n}) , tal que C_{1}=V_{2} e C_{n}=Z . Como {\\mathcal{G}}^{*} tem o mesmo padrão de {\\mathcal{G}} , C é um caminho de V_{2} em Z . Faremos a demonstração por indução. Suponha que C_{1}=V_{2} e C_{1}=Z , isto é, V_{2}=Z . Neste caso, V_{*}=V_{2} satisfaz as condições do lema. A seguir, suponha que, se n\\leq n^{*} , então existe V_{*} nas condições do lema e que n=n^{*}+1 . Se Z é descendente de V_{2} em {\\mathcal{G}}^{*} , então basta tomar V_{*}=Z . Caso contrário, existe um menor i tal que C_{i}\\leftarrow C_{i+1} . Existem 2 casos a considerar: i=1 e i>1 . Se i>1 , então C_{i-1}\\rightarrow C_{i}\\leftarrow C_{i+1} é um colisor em {\\mathcal{G}}^{*} . Como eles formam uma cadeia em {\\mathcal{G}} , e {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão, C_{i-1} e C_{i+1} são adjacentes em ambos os DAGs. Como C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i}\\stackrel{{% \\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i+1} e {\\mathcal{G}} é acíclico, C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i+1} . Portanto, C^{*}=(C_{1},\\ldots,C_{i-1},C_{i+1},\\ldots,C_{n}) é um caminho de tamanho n-1 de V_{2} a Z em {\\mathcal{G}}^{*} que é um caminho direcionado em {\\mathcal{G}} . Decorre da hipótese de indução que existe V_{*} tal qual desejado. Se i=1 , V_{2}\\stackrel{{\\scriptstyle{\\mathcal{G}}^{*}}}{{\\leftarrow}}C_{2} . Portanto, V_{1}\\rightarrow V_{2}\\leftarrow C_{2} e V_{3}\\rightarrow V_{2}\\leftarrow C_{2} são colisores em {\\mathcal{G}}^{*} , mas não em {\\mathcal{G}} . Como {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão, conclua que V_{1} e C_{2} , e V_{3} e C_{2} são adjacentes em ambos os DAGs. Como V_{1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}V_{2}\\stackrel{{% \\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{2} e {\\mathcal{G}} é acíclico, V_{1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{2} . Por simetria V_{3}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{2} . Portanto, V_{1}\\rightarrow C_{2}\\leftarrow V_{3} é um colisor em {\\mathcal{G}} . Como V_{1} e V_{3} não são adjacentes por suposição e {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão, V_{1}\\rightarrow C_{2}\\leftarrow V_{3} é um colisor em {\\mathcal{G}}^{*} , (C_{2},C_{3},\\ldots,C_{n}) é um caminho de tamanho n-1 em {\\mathcal{G}}^{*} e que também é um caminho direcionado em {\\mathcal{G}} . Portanto, pela hipótese de indução, existe V_{*} tal qual desejado. ∎ Lema 6.28. Se {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão, então {\\mathcal{G}} e {\\mathcal{G}}^{*} são fielmente equivalentes. Demonstração. Desejamos provar que para quaisquer V_{1},V_{2}\\in{\\mathcal{V}} e {\\mathbb{V}}\\subseteq{\\mathcal{V}} tais que V_{1}\\neq V_{2} e \\{V_{1},V_{2}\\}\\cap{\\mathbb{V}}=\\emptyset , se existe um caminho em {\\mathcal{G}} de V_{1} a V_{2} não bloqueado dado {\\mathbb{V}} , então existe um caminho em {\\mathcal{G}}^{*} de V_{1} a V_{2} não bloqueado dado {\\mathbb{V}} . Tome V_{1} , V_{2} e {\\mathbb{V}} arbitrários. Realizaremos a demonstração por indução. Suponha que há um caminho de tamanho 2 de V_{1} a V_{2} em {\\mathcal{G}} que não é bloqueado dado {\\mathbb{V}} . Portanto, V_{1} e V_{2} são adjacentes em {\\mathcal{G}} . Como {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão, V_{1} e V_{2} são adjacentes em {\\mathcal{G}}^{*} . Conclua do Lema 6.25 que (V_{1},V_{2}) é um caminho não bloqueado dado {\\mathbb{V}} em {\\mathcal{G}}^{*} . A seguir, suponha que, para todo caminho de tamanho menor ou igual a n em {\\mathcal{G}} de V_{1} a V_{2} que não é bloqueado dado {\\mathbb{V}} , existe um caminho em {\\mathcal{G}}^{*} de V_{1} a V_{2} que não é bloqueado dado {\\mathbb{V}} . Tome um caminho caminho arbitrário de tamanho n+1 , (C_{1},C_{2},\\ldots,C_{n},C_{n+1}) em {\\mathcal{G}} de V_{1} a V_{2} que não é bloqueado dado {\\mathbb{V}} . Desejamos provar que existe um caminho em {\\mathcal{G}}^{*} de V_{1} a V_{2} que não é bloqueado dado {\\mathbb{V}} . Para tal, observamos inicialmente que, como {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão, C é um caminho de V_{1} a V_{2} em {\\mathcal{G}}^{*} . A seguir, dividiremos a análise em alguns casos: a)​Considere que existe i tal que C_{i} é um colisor em {\\mathcal{G}} e não é um colisor em {\\mathcal{G}}^{*} . Como {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão, C_{i-1} e C_{i+1} são adjacentes em {\\mathcal{G}} . Sem perda de generalidade, suponha que C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i+1} . Tome C^{*}=(C_{1},\\ldots,C_{i-1},C_{i+1},\\ldots,C_{n+1}) . Com exceção de C_{i+1} , todos os vértices em C^{*} tem o mesmo tipo entre colisor e não-colisor que eles tem em C . Assim, todos os demais vértices estão abertos dado {\\mathbb{V}} . Caso C_{i+1} não seja um colisor em C^{*} , então ele tem o mesmo tipo que em C e, assim, está aberto em C^{*} dado {\\mathbb{V}} . A seguir, resta o caso em que C_{i+1} é um colisor em C^{*} . Como C_{i} é um colisor em C , C_{i} é descendente de C_{i+1} . Também, como C_{i} não estava bloqueado em C dado {\\mathbb{V}} , existe V\\in{\\mathbb{V}} tal que V é descendente de C_{i} . Conclua que V é descendente de C_{i+1} e, assim, C_{i+1} não está bloqueado em C^{*} dado {\\mathbb{V}} . Portanto, C^{*} é um caminho de tamanho n de V_{1} a V_{2} em {\\mathcal{G}} que não está bloqueado dado {\\mathbb{V}} . Assim, esse caso está resolvido pela hipótese de indução. b)​Considere que existe i tal que C_{i} não é um colisor em {\\mathcal{G}} e é um colisor em {\\mathcal{G}}^{*} . Como {\\mathcal{G}} e {\\mathcal{G}}^{*} tem o mesmo padrão, C_{i-1} e C_{i+1} são adjacentes em {\\mathcal{G}} . Há 3 casos a analisar: I)​Caso C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i}\\stackrel{{% \\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i+1} , como {\\mathcal{G}} é um DAG, obtemos que C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i+1} . Neste caso, todos os vértices no caminho C^{*}=(C_{1},\\ldots,C_{i-1},C_{i+1},\\ldots,C_{n+1}) tem o mesmo tipo entre colisor e não-colisor que eles tem em C . Assim, C^{*} é um caminho de tamanho n de V_{1} a V_{2} que não está bloqueado em {\\mathcal{G}} dado {\\mathbb{V}} . Assim, esse caso está resolvido pela hipótese de indução. II)​Caso C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\leftarrow}}C_{i}\\stackrel{{% \\scriptstyle{\\mathcal{G}}}}{{\\leftarrow}}C_{i+1} , a análise é análoga ao caso anterior. III)​Caso C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\leftarrow}}C_{i}\\stackrel{{% \\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i+1} , tome sem perda de generalidade que C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i+1} . A seguir, há dois casos a considerar. Primeiramente, considere que C_{i-1} é um colisor em C em {\\mathcal{G}} . Como C_{i} é um colisor em C em {\\mathcal{G}}^{*} , C_{i-1} não é um colisor em C em {\\mathcal{G}}^{*} . Portanto, C_{i-1} satisfaz as condições do caso tratado em a). A seguir, se C_{i-1} não é um colisor em C em {\\mathcal{G}} , então todos os vértices em C^{*}=(C_{1},\\ldots,C_{i-1},C_{i+1},\\ldots,C_{n+1}) tem o mesmo tipo entre colisor e não-colisor dado {\\mathbb{V}} que em C . Portanto, C^{*} é um caminho de tamanho n de V_{1} a V_{2} que não está bloqueado dado {\\mathbb{V}} em {\\mathcal{G}} . Assim, esse caso está resolvido pela hipótese de indução. c)​Resta o caso tal que, para todo i , tanto em {\\mathcal{G}} quanto em {\\mathcal{G}}^{*} , C_{i} tem o mesmo tipo em C entre colisor e não-colisor. Existem dois casos a analisar: I)​Caso exista algum i tal que C_{i} é um colisor em C tanto em {\\mathcal{G}} quanto em {\\mathcal{G}}^{*} e C_{i-1} e C_{i+1} são adjacentes em {\\mathcal{G}} . Sem perda de generalidade, suponha que C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i+1} e tome C^{*}=(C_{1},\\ldots,C_{i-1},C_{i+1},\\ldots,C_{n+1}) . Todos os vértices em C^{*} com exceção de C_{i+1} tem o mesmo tipo entre colisor e não-colisor que em C . Caso C_{i+1} também tenha o mesmo tipo, então C^{*} não está bloqueado dado {\\mathbb{V}} . Caso contrário, C_{i+1} é um colisor em C^{*} . Como C_{i} é um colisor em C , obtemos que C_{i+1}\\stackrel{{\\scriptstyle{\\mathcal{G}}}}{{\\rightarrow}}C_{i} . Além disso, como C_{i} não está bloqueado em C dado {\\mathbb{V}} , conclua que existe V\\in{\\mathbb{V}} tal que V é descendente de C_{i} . Conclua das duas últimas sentenças que V é descendente de C_{i+1} . Assim, C_{i+1} não está bloqueado em C^{*} dado {\\mathbb{V}} . Decorre que C^{*} é um caminho de tamanho n de V_{1} a V_{2} que não está bloqueado em {\\mathcal{G}} dado {\\mathbb{V}} . Assim, esse caso está resolvido pela hipótese de indução. II)​Caso contrário, para todo i em que C_{i} é um colisor em C , C_{i-1} e C_{i+1} não são adjacentes. Portanto, decorre do Lema 6.27 que existe C^{*}_{i} tal que C_{i-1}\\stackrel{{\\scriptstyle{\\mathcal{G}}^{*}}}{{\\rightarrow}}C^{*}_{i}% \\stackrel{{\\scriptstyle{\\mathcal{G}}^{*}}}{{\\leftarrow}}C_{i+1} e existe V\\in{\\mathbb{V}} que é descendente de C_{i} em {\\mathcal{G}}^{*} . Para todo i em que C_{i} não é colisor em C , tome C^{*}_{i}=C_{i} . Por construção, C^{*} não está bloqueado em {\\mathcal{G}}^{*} dado {\\mathbb{V}} . ∎ Prova do Teorema 5.5. Decorre dos Lemas 6.26 e 6.28 ∎ Previous page"],[["index.html","chapter6.html","chapter6.A2.html"],"Apêndice 6.B Seção 2.5 (2.5 Modelo Causal (Causal Model)) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. ( Apêndice 6.B Seção 2.5 (Modelo Causal (Causal Model)) Prova do Lema 2.27. Realizaremos a demonstração por indução. Para tal, defina {\\mathcal{V}}^{(1)}=\\{V\\in{\\mathcal{V}}:Pa(V)=\\emptyset\\} , e para cada i>2 , {\\mathcal{V}}^{(i)}=\\{V\\in{\\mathcal{V}}:Pa(V)\\subseteq{\\mathcal{V}}^{(i-1)}\\} . Se Y\\in{\\mathcal{V}}^{(1)} , então por construção {\\mathbb{E}}[Y]=\\mu_{Y} . Também, como Pa(Y)=\\emptyset , \\mathbb{C}_{V,Y}=\\emptyset , para todo V\\neq Y , e \\mathbb{C}_{Y,Y} tem apenas o caminho unitário, C^{*}=(Y) . Assim, \\displaystyle\\sum_{V\\in\\mathbb{{\\mathcal{V}}}}\\sum_{C\\in\\mathbb{C}_{V,Y}}\\mu_{% V}\\cdot\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{i}} \\displaystyle=\\mu_{Y}\\cdot\\prod_{i=1}^{|C^{*}|-1}\\beta_{C^{*}_{i+1},C^{*}_{i}} \\displaystyle=\\mu_{Y} A seguir, suponha que para todo W\\in{\\mathcal{V}}^{(i-1)} , {\\mathbb{E}}[W]=\\sum_{V\\in\\mathbb{{\\mathcal{V}}}}\\sum_{C\\in\\mathbb{C}_{V,W}}% \\mu_{V}\\cdot\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{i}} e tome Y\\in{\\mathcal{V}}^{(i)} . Como todo caminho direcionado que chega em Y a partir de V\\neq Y tem como penúltimo elemento um pai de Y , podemos escrever \\displaystyle\\mathbb{C}_{V,Y} \\displaystyle=\\cup_{W\\in Pa(Y)}\\{(C,Y):C\\in\\mathbb{C}_{V,W}\\} (7) Portanto, obtemos: \\displaystyle\\sum_{V\\in\\mathbb{{\\mathcal{V}}}}\\sum_{C\\in\\mathbb{C}_{V,Y}}\\mu_{% V}\\cdot\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{i}} \\displaystyle= \\displaystyle\\mu_{Y}+\\sum_{V\\neq Y}\\sum_{\\cup_{W\\in Pa(Y)}\\{C^{*}=(C,Y):C\\in% \\mathbb{C}_{V,W}\\}}\\mu_{V}\\cdot\\left(\\prod_{i=1}^{|C^{*}|-1}\\beta_{C^{*}_{i+1}% ,C^{*}_{i}}\\right) \\displaystyle= \\displaystyle\\mu_{Y}+\\sum_{V\\neq Y}\\sum_{W\\in Pa(Y)}\\sum_{C\\in\\mathbb{C}_{V,W}% }\\mu_{V}\\cdot\\left(\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{i}}\\right)\\cdot\\beta_{% Y,W} \\displaystyle= \\displaystyle\\mu_{Y}+\\sum_{W\\in Pa(Y)}\\beta_{Y,W}\\sum_{V\\in{\\mathcal{V}}}\\sum_% {C\\in\\mathbb{C}_{V,W}}\\mu_{V}\\cdot\\left(\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{i% }}\\right) \\displaystyle= \\displaystyle\\mu_{Y}+\\sum_{W\\in Pa(Y)}\\beta_{Y,W}{\\mathbb{E}}[W] \\displaystyle W\\in{\\mathcal{V}}^{(i-1)} \\displaystyle= \\displaystyle{\\mathbb{E}}\\left[\\mu_{Y}+\\sum_{W\\in Pa(Y)}\\beta_{Y,W}W\\right] \\displaystyle= \\displaystyle{\\mathbb{E}}[{\\mathbb{E}}[Y|Pa(Y)]]={\\mathbb{E}}[Y] ∎ Previous page Next page"],[["index.html","chapter6.html","chapter6.A3.html"],"Apêndice 6.C Seção 3 (3 Independência Condicional e D-separação) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. ( Apêndice 6.C Seção 3 (Independência Condicional e D-separação) 6.C.1 Relativas ao Lema 2.45 Prova do Lema 2.45. A prova consistirá em demonstrar que, para cada i , a afirmação i decorre da afirmação i-1 . Finalmente, a afirmação 1 decorre da afirmação 4 . Os símbolos {\\mathbf{X}} e {\\mathbf{x}} referem-se a ({\\mathbf{X}}_{1},\\ldots,{\\mathbf{X}}_{d}) e ({\\mathbf{x}}_{1},\\ldots,{\\mathbf{x}}_{d}) . •​ (1\\Longrightarrow 2) \\displaystyle f({\\mathbf{x}}|{\\mathbf{y}}) \\displaystyle=\\prod_{j=1}^{d}{f({\\mathbf{x}}_{j}|{\\mathbf{y}})} \\displaystyle(1) \\displaystyle=\\prod_{j=1}^{d}{h({\\mathbf{x}}_{j},{\\mathbf{y}})} \\displaystyle h({\\mathbf{x}}_{j},{\\mathbf{y}})=f({\\mathbf{x}}_{j}|{\\mathbf{y}}) •​ (2\\Longrightarrow 3) Note que, \\displaystyle f({\\mathbf{x}}_{i}|{\\mathbf{x}}_{-i},{\\mathbf{y}}) \\displaystyle=\\frac{f({\\mathbf{x}}|{\\mathbf{y}})}{f({\\mathbf{x}}_{1},\\ldots,{% \\mathbf{x}}_{i-1},{\\mathbf{x}}_{i+1},\\ldots{\\mathbf{x}}_{d}|{\\mathbf{y}})} \\displaystyle=\\frac{f({\\mathbf{x}}|{\\mathbf{y}})}{\\int_{\\mathbb{R}}{f({\\mathbf% {x}}|{\\mathbf{y}})d{\\mathbf{x}}_{i}}} \\displaystyle=\\frac{\\prod_{j=1}^{d}{h_{j}({\\mathbf{x}}_{j},{\\mathbf{y}})}}{% \\int_{\\mathbb{R}}{\\prod_{j=1}^{d}{h_{j}({\\mathbf{x}}_{j},{\\mathbf{y}})}d{% \\mathbf{x}}_{i}}}(2) \\displaystyle=\\frac{\\prod_{j=1}^{d}{h_{j}({\\mathbf{x}}_{j},{\\mathbf{y}})}}{% \\prod_{j\\neq i}{h_{j}({\\mathbf{x}}_{j},{\\mathbf{y}})}\\int_{\\mathbb{R}}{h_{i}({% \\mathbf{x}}_{i},{\\mathbf{y}})d{\\mathbf{x}}_{i}}} \\displaystyle=\\frac{\\tilde{h}_{i}({\\mathbf{x}}_{i},{\\mathbf{y}})}{\\int_{% \\mathbb{R}}{h_{i}({\\mathbf{x}}_{i},{\\mathbf{y}})d{\\mathbf{x}}_{i}}} \\displaystyle=\\frac{\\prod_{j\\neq i}{\\int_{\\mathbb{R}}{h_{j}({\\mathbf{x}}_{j},{% \\mathbf{y}})d{\\mathbf{x}}_{j}}}}{\\prod_{j\\neq i}{\\int_{\\mathbb{R}}{h_{j}({% \\mathbf{x}}_{j},{\\mathbf{y}})d{\\mathbf{x}}_{j}}}}\\cdot\\frac{h_{i}({\\mathbf{x}}% _{i},{\\mathbf{y}})}{\\int_{\\mathbb{R}}{h_{i}({\\mathbf{x}}_{i},{\\mathbf{y}})d{% \\mathbf{x}}_{i}}} \\displaystyle=\\frac{\\int_{\\mathbb{R}^{d-1}}{\\prod_{j=1}^{d}{h_{j}({\\mathbf{x}}% _{j},{\\mathbf{y}})}d{\\mathbf{x}}_{-i}}}{\\int_{\\mathbb{R}^{d}}{\\prod_{j=1}^{d}{% h_{j}({\\mathbf{x}}_{j},{\\mathbf{y}})}d{\\mathbf{x}}}} \\displaystyle=\\frac{\\int_{\\mathbb{R}^{d-1}}{f({\\mathbf{x}}|{\\mathbf{y}})d{% \\mathbf{x}}_{-i}}}{\\int_{\\mathbb{R}^{d}}{f({\\mathbf{x}}|{\\mathbf{y}})d{\\mathbf% {x}}}} \\displaystyle(2) \\displaystyle=f({\\mathbf{x}}_{i}|{\\mathbf{y}}) •​ (3\\Longrightarrow 4) \\displaystyle f({\\mathbf{x}}_{i}|{\\mathbf{x}}_{1}^{i-1},{\\mathbf{y}}) \\displaystyle=\\frac{f({\\mathbf{x}}_{1}^{i}|{\\mathbf{y}})}{f({\\mathbf{x}}_{1}^{% i-1}|{\\mathbf{y}})} \\displaystyle=\\frac{\\int_{\\mathbb{R}^{d-i}}{f({\\mathbf{x}}|{\\mathbf{y}})}d{% \\mathbf{x}}_{i+1}^{d}}{f({\\mathbf{x}}_{1}^{i-1}|{\\mathbf{y}})} \\displaystyle=\\frac{\\int_{\\mathbb{R}^{d-i}}{f({\\mathbf{x}}_{-i}|{\\mathbf{y}})f% ({\\mathbf{x}}_{i}|{\\mathbf{x}}_{-i},{\\mathbf{y}})d{\\mathbf{x}}_{i+1}^{d}}}{f({% \\mathbf{x}}_{1}^{i-1}|{\\mathbf{y}})} \\displaystyle=\\frac{f({\\mathbf{x}}_{i}|{\\mathbf{y}})\\int_{\\mathbb{R}^{d-i}}{f(% {\\mathbf{x}}_{-i}|{\\mathbf{y}})d{\\mathbf{x}}_{i+1}^{d}}}{f({\\mathbf{x}}_{1}^{i% -1}|{\\mathbf{y}})} \\displaystyle(3) \\displaystyle=\\frac{f({\\mathbf{x}}_{i}|{\\mathbf{y}})f({\\mathbf{x}}_{1}^{i-1}|{% \\mathbf{y}})}{f({\\mathbf{x}}_{1}^{i-1}|{\\mathbf{y}})} \\displaystyle=f({\\mathbf{x}}_{i}|{\\mathbf{y}}) •​ (4\\Longrightarrow 1) \\displaystyle f({\\mathbf{x}}|{\\mathbf{y}}) \\displaystyle=\\prod_{i=1}^{d}{f({\\mathbf{x}}_{i}|{\\mathbf{x}}_{1}^{i-1},{% \\mathbf{y}})} \\displaystyle=\\prod_{i=1}^{d}{f({\\mathbf{x}}_{i}|{\\mathbf{y}})} \\displaystyle(4) ∎ 6.C.2 Teorema 2.49 Lema 6.1. Seja {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) um DAG. Se {\\mathcal{A}}={\\mathbb{V}}_{1}\\cup{\\mathbb{V}}_{2}\\cup{\\mathbb{V}}_{3} é ancestral e {\\mathbb{V}}_{1}\\perp{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} , então, para todo f compatível com {\\mathcal{G}} , {\\mathbb{V}}_{1}\\perp\\!\\!\\!\\!\\perp^{f}{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} . Demonstração. Defina {\\mathbb{V}}^{*}_{1}=\\{V\\in{\\mathcal{A}}:V\\in{\\mathbb{V}}_{1}\\text{ ou }V_{1}% \\rightarrow V,\\text{ para algum }V_{1}\\in{\\mathbb{V}}_{1}\\} e {\\mathbb{V}}^{*}_{2}={\\mathcal{A}}-{\\mathbb{V}}^{*}_{1} . Como {\\mathbb{V}}_{1}\\perp{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} , decorre de Definição 2.48 que não existe V_{1}\\in{\\mathbb{V}}_{1} e V_{2}\\in{\\mathbb{V}}_{2} tal que V_{1}\\rightarrow V_{2} . Portanto, \\displaystyle{\\mathbb{V}}^{*}_{1}\\subseteq{\\mathbb{V}}_{1}\\cup{\\mathbb{V}}_{3}% \\text{ e }{\\mathbb{V}}^{*}_{2}\\subseteq{\\mathbb{V}}_{2}\\cup{\\mathbb{V}}_{3} (8) A seguir, demonstraremos que \\displaystyle\\forall i\\in\\{1,2\\}\\text{ e }V^{*}_{i}\\in{\\mathbb{V}}^{*}_{i}:Pa(% V^{*}_{i})\\subseteq{\\mathbb{V}}_{i}\\cup{\\mathbb{V}}_{3} (9) Tome V^{*}_{1}\\in{\\mathbb{V}}^{*}_{1} . Como V^{*}_{1}\\in{\\mathcal{A}} e {\\mathcal{A}} é ancestral, decorre da Definição 2.9 que Pa(V^{*}_{1})\\subseteq{\\mathcal{A}} . Assim, basta demonstrar que Pa(V^{*}_{1})\\cap{\\mathbb{V}}_{2}=\\emptyset . Se V^{*}_{1}\\in{\\mathbb{V}}_{1} , então decorre de Definição 2.48 que não existe V_{2}\\in{\\mathbb{V}}_{2} tal que V_{2}\\rightarrow V^{*}_{1} . Caso contrário, se V^{*}_{1}\\in{\\mathbb{V}}_{3} , então existe V_{1}\\in{\\mathbb{V}}_{1} tal que V_{1}\\rightarrow V^{*}_{1} . Decorre de Definição 2.48 que não existe V_{1}\\in{\\mathbb{V}}_{1} , V_{2}\\in{\\mathbb{V}}_{2} e V_{3}\\in{\\mathbb{V}}_{3} tais que V_{3} é um colisor entre V_{1} e V_{2} , isto é, V_{1}\\rightarrow V_{3}\\leftarrow V_{2} . Portanto, não existe V_{2}\\in{\\mathbb{V}}_{2} tal que V_{2}\\rightarrow V^{*}_{1} . Conclua que Pa(V^{*}_{1})\\subseteq{\\mathbb{V}}_{1}\\cup{\\mathbb{V}}_{3} . A seguir, note que pela definição de {\\mathbb{V}}^{*}_{1} , se V\\in{\\mathcal{A}} é tal que existe V_{1}\\in{\\mathbb{V}}_{1} com V_{1}\\rightarrow V , então V\\in{\\mathbb{V}}^{*}_{1} . Portanto, como {\\mathbb{V}}^{*}_{2}={\\mathcal{V}}-{\\mathbb{V}}^{*}_{1} , para todo V^{*}_{2}\\in{\\mathbb{V}}^{*}_{2} , não existe V_{1}\\in{\\mathbb{V}}_{1} tal que V_{1}\\rightarrow V^{*}_{2} . Isto é, Pa(V^{*}_{2})\\subseteq{\\mathcal{V}}-{\\mathbb{V}}_{1} . Como V^{*}_{2}\\in{\\mathcal{A}} e {\\mathcal{A}} é ancestral, conclua da Definição 2.9 que Pa(V^{*}_{2})\\subseteq{\\mathcal{A}} . Combinando as duas últimas frases, Pa(V^{*}_{2})\\subseteq{\\mathbb{V}}_{2}\\cup{\\mathbb{V}}_{3} . Decorre da conclusão dos dois últimos parágrafos que likning 9 está demonstrado. \\displaystyle f({\\mathbb{V}}_{1},{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3}) \\displaystyle=\\frac{f({\\mathbb{V}}_{1},{\\mathbb{V}}_{2},{\\mathbb{V}}_{3})}{f({% \\mathbb{V}}_{3})} \\displaystyle=\\frac{\\prod_{V\\in{\\mathcal{A}}}f(V|Pa(V))}{f({\\mathbb{V}}_{3})} \\displaystyle=\\frac{\\left(\\prod_{V^{*}_{1}\\in{\\mathbb{V}}^{*}_{1}}f(V^{*}_{1}|% Pa(V^{*}_{1}))\\right)\\left(\\prod_{V^{*}_{2}\\in{\\mathbb{V}}^{*}_{2}}f(V^{*}_{2}% |Pa(V^{*}_{2}))\\right)}{f({\\mathbb{V}}_{3})} \\displaystyle{\\mathbb{V}}^{*}_{1}\\text{ e }{\\mathbb{V}}^{*}_{2}\\text{ % particionam }{\\mathcal{A}} \\displaystyle=h_{1}({\\mathbb{V}}_{1},{\\mathbb{V}}_{3})h_{2}({\\mathbb{V}}_{2},{% \\mathbb{V}}_{3}) likningene 8 e 9 Assim, decorre do Lema 2.45 que {\\mathbb{V}}_{1}\\perp\\!\\!\\!\\!\\perp^{f}{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} . ∎ Lema 6.2. Se f é compatível com {\\mathcal{G}} e {\\mathbb{V}}_{1}\\perp{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} , então {\\mathbb{V}}_{1}\\perp\\!\\!\\!\\!\\perp^{f}{\\mathbb{V}}_{2}|{\\mathbb{V}}_{3} . Demonstração. Defina {\\mathcal{A}}=Anc({\\mathbb{V}}_{1}\\cup{\\mathbb{V}}_{2}\\cup{\\mathbb{V}}_{3}) , {\\mathbb{V}}^{*}_{1}=\\{V\\in{\\mathcal{A}}:V\\text{ não é d-separado de }{\\mathbb% {V}}_{1}|{\\mathbb{V}}_{3}\\} , e {\\mathbb{V}}^{*}_{2}={\\mathcal{A}}-{\\mathbb{V}}^{*}_{1} . Por definição, \\displaystyle{\\mathbb{V}}_{1}\\subseteq{\\mathbb{V}}^{*}_{1}\\text{ e }{\\mathbb{V% }}_{2}\\subseteq{\\mathbb{V}}^{*}_{2} (10) O primeiro é provar que {\\mathbb{V}}^{*}_{1}\\perp{\\mathbb{V}}^{*}_{2}|{\\mathbb{V}}_{3} . Pela definição de {\\mathbb{V}}^{*}_{2} , para todo V_{1}\\in{\\mathbb{V}}_{1} e V^{*}_{2}\\in{\\mathbb{V}}^{*}_{2} , V_{1}\\perp V^{*}_{2}|{\\mathbb{V}}_{3} , isto é, \\displaystyle{\\mathbb{V}}_{1}\\perp{\\mathbb{V}}^{*}_{2}|{\\mathbb{V}}_{3} (11) Suponha por absurdo que existam V^{*}_{1}\\in{\\mathbb{V}}^{*}_{1} e V^{*}_{2}\\in{\\mathbb{V}}^{*}_{2} tais que V^{*}_{1} e V^{*}_{2} não são d-separados dado {\\mathbb{V}}_{3} . Portanto, existe um caminho ativo dado {\\mathbb{V}}_{3} , (V^{*}_{1},C_{2},\\ldots,C_{n-1},V^{*}_{2}) . Pela definição de {\\mathbb{V}}^{*}_{1} , existe V_{1}\\in{\\mathbb{V}}_{1} e um caminho ativo dado {\\mathbb{V}}_{3} , (V_{1},C^{*}_{2},\\ldots,C^{*}_{m-1},V^{*}_{1}) . Assim, (V_{1},C^{*}_{2},\\ldots,C^{*}_{m-1},V^{*}_{1},C_{2},\\ldots,C_{n-1},V^{*}_{2}) é é um caminho ativo dado {\\mathbb{V}}_{3} de V_{1} a V^{*}_{2} , uma contradição com likning 11. Conclua que {\\mathbb{V}}^{*}_{1}\\perp{\\mathbb{V}}^{*}_{2}|{\\mathbb{V}}_{3} . A seguir, provaremos que {\\mathbb{V}}^{*}_{1}\\perp\\!\\!\\!\\!\\perp^{f}{\\mathbb{V}}^{*}_{2}|{\\mathbb{V}}_{3} . Como {\\mathcal{A}}=Anc({\\mathbb{V}}_{1}\\cup{\\mathbb{V}}_{2}\\cup{\\mathbb{V}}_{3}) , decorre do Lema 2.10 que {\\mathcal{A}} é ancestral. Portanto, como {\\mathcal{A}}={\\mathbb{V}}^{*}_{1}\\cup{\\mathbb{V}}^{*}_{2}\\cup{\\mathbb{V}}_{3} e {\\mathbb{V}}^{*}_{1}\\perp{\\mathbb{V}}^{*}_{2}|{\\mathbb{V}}_{3} , decorre do Lema 6.1 que {\\mathbb{V}}^{*}_{1}\\perp\\!\\!\\!\\!\\perp^{f}{\\mathbb{V}}^{*}_{2}|{\\mathbb{V}}_{3} . Como {\\mathbb{V}}^{*}_{1}\\perp\\!\\!\\!\\!\\perp^{f}{\\mathbb{V}}^{*}_{2}|{\\mathbb{V}}_{3} , a conclusão do lema decorre do fato de que {\\mathbb{V}}_{1}\\subseteq{\\mathbb{V}}^{*}_{1} e {\\mathbb{V}}_{2}\\subseteq{\\mathbb{V}}^{*}_{2} . ∎ Teorema 6.3 (Spirtes2000[p.66). ] Considere que ({\\mathcal{G}},f_{\\beta}) é um CM linear Gaussiano (Definição 2.25) de parâmetros \\sigma^{2}=1 , \\beta e \\mu=0 . Para qualquer distribuição contínua sobre \\beta , tem probabilidade 0 a ocorrência de valores de \\beta tais que existam {\\mathbf{X}},{\\mathbf{Y}},{\\mathbf{Z}} com {\\mathbf{X}}\\perp\\!\\!\\!\\!\\perp^{f_{\\beta}}{\\mathbf{Y}}|{\\mathbf{Z}} mas {\\mathbf{X}} e {\\mathbf{Y}} não serem d-separados dado {\\mathbf{Z}} em {\\mathcal{G}} . Lema 6.4. Se {\\mathbb{V}}_{1} não é d-separado de {\\mathbb{V}}_{2} dado {\\mathbb{V}}_{3} segundo o DAG {\\mathcal{G}}=({\\mathcal{V}},{\\mathcal{E}}) , então existe f compatível com {\\mathcal{G}} tal que {\\mathbb{V}}_{1} e {\\mathbb{V}}_{2} são condicionalmente dependentes dado {\\mathbb{V}}_{3} segundo f Demonstração. Decorre do Teorema 6.3. ∎ Prova do Teorema 2.49. Decorre dos Lemas 6.2 e 6.4. ∎ Previous page Next page"],[["index.html","chapter6.html","chapter6.A4.html"],"Apêndice 6.D Relativas à Seção 4 (4 O modelo de probabilidade para intervenções) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. Relativas à Apêndice 6.D Relativas à Seção 4 (O modelo de probabilidade para intervenções) Prova do Lema 3.2. Decorre da Definição 3.1 que f^{*}({\\mathcal{V}})={\\mathbb{I}}({\\mathbf{X}}={\\mathbf{x}})\\prod_{V\\notin{% \\mathbf{X}}}f(V|Pa(V)) . Definindo g_{X_{i}}(X_{i})={\\mathbb{I}}(X_{i}=x_{i}) , para todo X_{i}\\in{\\mathbf{X}} e g_{V}(V,Pa(V))=f(v|Pa(V)) , note que \\displaystyle f^{*}({\\mathcal{V}}) \\displaystyle=\\prod_{X_{i}\\in{\\mathbf{X}}}g_{X_{i}}(X_{i})\\prod_{V\\notin{% \\mathbf{X}}}g_{V}(V,Pa(V)) Portanto, decorre do Lema 2.14 que f^{*} é compatível com um grafo em que todo X_{i}\\in{\\mathbf{X}} não tem pais e todo V\\notin{\\mathbf{X}} tem os mesmos pais que em {\\mathcal{G}} . Isto é, {\\mathcal{G}} é compatível com {\\mathcal{G}}(\\bar{{\\mathbf{X}}}) . Além disso, tomando {\\mathbb{V}}={\\mathcal{V}}-{\\mathbf{X}} , \\displaystyle f^{*}({\\mathbf{X}}) \\displaystyle=\\int f^{*}(sV)d{\\mathbb{V}} \\displaystyle=\\int{\\mathbb{I}}({\\mathbf{X}}={\\mathbf{x}})\\prod_{V\\in{\\mathbb{V% }}}f(V|Pa(V))d{\\mathbb{V}} \\displaystyle={\\mathbb{I}}({\\mathbf{X}}={\\mathbf{x}})\\int\\prod_{V\\in{\\mathbb{V% }}}f(V|Pa(V))d{\\mathbb{V}} \\displaystyle={\\mathbb{I}}({\\mathbf{X}}={\\mathbf{x}}). Portanto, {\\mathbf{X}} é degenerado em {\\mathbf{x}} segundo f^{*} . ∎ 6.D.1 Relativas ao Teorema 3.6 Lema 6.5. Considere que ({\\mathcal{G}},f) é um CM linear Gaussiano e que f^{*}=f({\\mathcal{V}}|do({\\mathbf{X}}={\\mathbf{x}})) . Se {\\mathcal{G}}(\\bar{{\\mathbf{X}}}) é como no Lema 3.2, então ({\\mathcal{G}}(\\bar{{\\mathbf{X}}}),f^{*}) é um CM linear Gaussiano tal que, para todo V\\notin{\\mathbf{X}} , {\\mathbb{E}}_{f}[V|Pa(V)]={\\mathbb{E}}_{f^{*}}[V|Pa(V)] e {\\mathbb{E}}[{\\mathbf{X}}]={\\mathbf{x}} . Demonstração. Decorre do Lema 3.2 que f^{*} é compatível com {\\mathcal{G}}(\\bar{{\\mathbf{X}}}) . Além disso, também decorre do Lema 3.2 que para todo V\\notin{\\mathbf{X}} , f(V|Pa(V))=f^{*}(V|Pa(V)) . Portanto, {\\mathbb{E}}_{f}[V|Pa(V)]={\\mathbb{E}}_{f^{*}}[V|Pa(V)] . Finalmente, segundo o Lema 3.2, {\\mathbf{X}} é degenerado em {\\mathbf{x}} . Assim, {\\mathbb{E}}[{\\mathbf{X}}]={\\mathbf{x}} . ∎ Prova do Teorema 3.6. Defina f^{*}_{x}=f({\\mathcal{V}}|do(X=x)) . Assim, \\displaystyle{ACE}_{X,Y} \\displaystyle=\\frac{d{\\mathbb{E}}[Y|do(X=x)]}{dx} \\displaystyle=\\frac{d{\\mathbb{E}}_{f^{*}_{x}}[Y]}{dx} (12) Além disso, decorre do Lema 6.5 que f^{*} é um CM linear Gaussiano no grafo {\\mathcal{G}}(\\bar{X}) . Como X não tem pais no grafo {\\mathcal{G}}(\\bar{X}) , os únicos caminhos direcionados de X a Y que passam por X são aqueles que se iniciam em X . Formalmente, defina \\mathbb{C} como o conjunto de todos os caminhos direcionados em {\\mathcal{G}} . Além disso, \\mathbb{C}_{X}=\\{C\\in\\mathbb{C}:C_{i}=X\\text{, para algum }i\\} . Obtemos \\displaystyle\\cup_{V\\in{\\mathcal{V}}}\\mathbb{C}_{V,Y}\\cap\\mathbb{C}_{X} \\displaystyle=\\mathbb{C}_{X,Y}. (13) Portanto, \\displaystyle{ACE}_{X,Y} \\displaystyle=\\frac{d{\\mathbb{E}}_{f^{*}_{x}}[Y]}{dx} \\displaystyle=\\frac{d\\sum_{V\\in\\mathbb{{\\mathcal{V}}}}\\sum_{C\\in\\mathbb{C}_{V,% Y}}\\mu_{V}\\cdot\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{i}}}{dx} Lemas 2.27 e 6.5 \\displaystyle=\\frac{d\\sum_{C\\in\\mathbb{C}_{X,Y}}\\mu_{X}\\cdot\\prod_{i=1}^{|C|-1% }\\beta_{C_{i+1},C_{i}}}{dx} \\displaystyle=\\frac{d\\sum_{C\\in\\mathbb{C}_{X,Y}}x\\cdot\\prod_{i=1}^{|C|-1}\\beta% _{C_{i+1},C_{i}}}{dx} \\displaystyle=\\sum_{C\\in\\mathbb{C}_{X,Y}}\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{% i}} ∎ Previous page Next page"],[["index.html","chapter6.html","chapter6.A5.html"],"Apêndice 6.E Seção 5 (5 Controlando confundidores (critério backdoor)) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. ( Apêndice 6.E Seção 5 (Controlando confundidores (critério backdoor)) 6.E.1 Teorema 3.19 Para realizar a demonstração do Teorema 3.19, consideraremos um SCM aumentado, em que existe uma variável que representa a ocorrência de uma intervenção em X. Uma consequência interessante desta construção será a de que o modelo intervencional é equivalente ao condicionamento usual no SCM aumentado. Definição 6.6. Seja ({\\mathcal{G}}_{*},f_{*}) um SCM expandido tal que {\\mathcal{G}}_{*}=({\\mathcal{V}}\\cup\\{I_{X}:X\\in{\\mathbf{X}}\\},{\\mathcal{E}}_{% *}) , e {\\mathcal{E}}_{*}={\\mathcal{E}}\\cup\\{(I_{X}\\rightarrow X:X\\in{\\mathbf{X}})\\} . Isto é, {\\mathcal{G}}_{*} é uma cópia de {\\mathcal{G}} em que adicionamos para cada X\\in{\\mathbf{X}} os vértice I_{X}\\in\\{0,1\\} e arestas de I_{X} para X . {\\mathcal{G}}^{*} admite uma interpretação intuitiva. I_{X} é a indicadora de que fazemos uma intervenção em X , fazendo que esta assuma o valor x . Se I_{X}=0 , não há uma intervenção e, assim, X segue a sua distribuição observacional. Se I_{X}=1 , X assume o valor x com probabilidade 1 . Finalmente, considerando Pa(X) como os pais de X segundo {\\mathcal{G}} , definimos que: \\displaystyle f_{*}(X|Pa(X),I_{X}) \\displaystyle=\\begin{cases}f(X|Pa(X))&\\text{, se $I_{X}=0$, e}\\\\ {\\mathbb{I}}(X=x)&\\text{, caso contrário.}\\end{cases} Lema 6.7. Se ({\\mathcal{G}}_{*},f_{*}) é tal qual em Definição 6.6, então: \\displaystyle f({\\mathcal{V}}|do(X=x)) \\displaystyle=f_{*}({\\mathcal{V}}|I_{{\\mathbf{X}}}=1) Demonstração. \\displaystyle f_{*}({\\mathcal{V}}|I_{{\\mathbf{X}}}=1) \\displaystyle=\\frac{f_{*}({\\mathcal{V}},I_{{\\mathbf{X}}}=1)}{f(I_{{\\mathbf{X}}% }=1)} \\displaystyle=\\frac{f(I_{{\\mathbf{X}}}=1)\\prod_{X\\in{\\mathbf{X}}}{\\mathbb{I}}(% X=x)\\prod_{V\\notin{\\mathbf{X}}}f(V|Pa(V))}{f(I=1)} \\displaystyle=\\prod_{X\\in{\\mathbf{X}}}{\\mathbb{I}}(X=x)\\cdot\\prod_{V\\notin{% \\mathbb{V}}_{1}}f(V|Pa(V)) \\displaystyle=f({\\mathcal{V}}|do({\\mathbf{X}}={\\mathbf{x}})) ∎ Lema 6.8. Se ({\\mathcal{G}}_{*},f_{*}) é tal qual em Definição 6.6, então: \\displaystyle f_{*}({\\mathcal{V}}|I_{{\\mathbf{X}}}=0) \\displaystyle=f({\\mathcal{V}}). Demonstração. \\displaystyle f_{*}({\\mathcal{V}}|I_{{\\mathbf{X}}}=0) \\displaystyle=\\frac{f_{*}({\\mathcal{V}},I_{{\\mathbf{X}}}=0)}{f_{*}(I_{{\\mathbf% {X}}}=0)} \\displaystyle=\\frac{f_{*}(I_{{\\mathbf{X}}}=0)\\prod_{X\\in{\\mathbf{X}}}f_{*}(X|% Pa(X),I_{X}=0)\\prod_{V\\notin{\\mathbf{X}}}f(V|Pa(V))}{f_{*}(I=0)} \\displaystyle=\\prod_{X\\in{\\mathbf{X}}}f(X|Pa(X))\\prod_{V\\notin{\\mathbf{X}}}f(V% |Pa(V)) \\displaystyle=\\prod_{V\\in{\\mathcal{V}}}f(V|Pa(V)) \\displaystyle=f({\\mathcal{V}}) ∎ Lema 6.9. Se ({\\mathcal{G}}_{*},f_{*}) é tal qual em Definição 6.6 e {\\mathbf{Z}} satisfaz o segundo item do critério backdoor para medir o efeito causal de X em Y , então I\\perp^{d}Y|X,{\\mathbf{Z}} . Demonstração. Tome um caminho arbitrário de I em Y , C=(I,C_{2},\\ldots,C_{n-1},Y) . Por definição de I , C_{2}=X e I\\rightarrow X . Se X\\rightarrow C_{3} , então X não é um colisor em C e C está bloqueado dado X e {\\mathbf{Z}} . Se X\\leftarrow C_{3} , então (X,C_{3},\\ldots,C_{n-1},Y) está bloqueado dado {\\mathbf{Z}} , uma vez que {\\mathbf{Z}} satisfaz o segundo item do critério backdoor. Conclua que C está bloqueado dado X e {\\mathbf{Z}} . ∎ Lema 6.10. Se {\\mathbf{Z}} satisfaz o segundo item do critério backdoor para medir o efeito causal de X em Y , então \\displaystyle f(y|do(x),{\\mathbf{z}}) \\displaystyle=f(y|x,{\\mathbf{z}}). Demonstração. \\displaystyle f(y|do(x),{\\mathbf{z}}) \\displaystyle=f_{*}(y|I=1,{\\mathbf{z}}) \\displaystyle=\\int f_{*}(y,X|I=1,{\\mathbf{z}})dX \\displaystyle=\\int f_{*}(X|I=1,{\\mathbf{z}})f_{*}(y|X,I=1,{\\mathbf{z}})dX \\displaystyle=\\int{\\mathbb{I}}(X=x)f_{*}(y|X,I=1,{\\mathbf{z}})dX \\displaystyle=f_{*}(y|x,I=1,{\\mathbf{z}}) \\displaystyle=f_{*}(y|x,I=0,{\\mathbf{z}}) \\displaystyle=f(y|x,{\\mathbf{z}}) ∎ Lema 6.11. Se ({\\mathcal{G}}_{*},f_{*}) é tal qual em Definição 6.6 e {\\mathbf{Z}} satisfaz o critério backdoor para medir o efeito causal de X em Y , então I\\perp^{d}{\\mathbf{Z}} . Demonstração. Tome arbitrariamente um Z\\in{\\mathbf{Z}} e um caminho de I em Z , C=(I,C_{2},\\ldots,C_{n-1},Z) . Por definição de I , C_{2}=X e I\\rightarrow X . Suponha por absurdo que C não tem colisor. Como, I\\rightarrow X , decorre que C=I\\rightarrow X\\rightarrow\\ldots\\rightarrow C_{n-1}\\rightarrow Z . Assim, Z é um descendente de X , uma contradição com o critério backdoor (Definição 3.11). Conclua que C tem um colisor. Assim, C está marginalmente bloqueado (Definição 2.47). ∎ Lema 6.12. Se {\\mathbf{Z}} satisfaz o critério backdoor para medir o efeito causal de X em Y , então f({\\mathbf{z}}|do(x))=f({\\mathbf{z}}) . Demonstração. \\displaystyle f({\\mathbf{z}}|do(x)) \\displaystyle=f_{*}({\\mathbf{z}}|I=1) \\displaystyle=f_{*}({\\mathbf{z}}|I=0) \\displaystyle=f({\\mathbf{z}}) ∎ Prova do Teorema 3.19. Decorre diretamente dos Lemas 6.10 e 6.12. ∎ Prova do Corolário 3.20. \\displaystyle f(y|do(X=x)) \\displaystyle=\\int f(y,{\\mathbf{z}}|do(X=x))d{\\mathbf{z}} \\displaystyle=\\int f({\\mathbf{z}}|do(X=x))f(y|do(X=x),{\\mathbf{z}}) \\displaystyle=\\int f({\\mathbf{z}})f(y|x,{\\mathbf{z}}) ∎ 6.E.2 Teoremas 3.21 e 3.22 Prova do Teorema 3.21. \\displaystyle{\\mathbb{E}}[g(Y)|do(X=x),{\\mathbf{Z}}] \\displaystyle=\\int g(y)f(y|do(x),{\\mathbf{Z}})dy \\displaystyle=\\int g(y)f(y|x,{\\mathbf{Z}})dy \\displaystyle={\\mathbb{E}}[g(Y)|X=x,{\\mathbf{Z}}] (14) \\displaystyle{\\mathbb{E}}[g(Y)|do(X=x)] \\displaystyle={\\mathbb{E}}[{\\mathbb{E}}[g(Y)|do(X=x),{\\mathbf{Z}}]] \\displaystyle={\\mathbb{E}}[{\\mathbb{E}}[g(Y)|X=x,{\\mathbf{Z}}]] ∎ Prova do Teorema 3.22. \\displaystyle{\\mathbb{E}}[g(Y)|do(x),{\\mathbf{Z}}] \\displaystyle=\\int g(y)f(y|do(x),{\\mathbf{Z}})dy \\displaystyle=\\int g(y)f(y|x,{\\mathbf{Z}})dy \\displaystyle=\\int\\frac{g(y)f(y,x|{\\mathbf{Z}})}{f(x|{\\mathbf{Z}})}dy \\displaystyle=\\int\\frac{g(y){\\mathbb{I}}(x_{*}=x)f(y,x_{*}|{\\mathbf{Z}})}{f(x|% {\\mathbf{Z}})}d(x_{*},y) \\displaystyle={\\mathbb{E}}\\left[\\frac{g(Y){\\mathbb{I}}(X=x)}{f(x|{\\mathbf{Z}})% }|{\\mathbf{Z}}\\right] \\displaystyle=\\frac{{\\mathbb{E}}[g(Y){\\mathbb{I}}(X=x)|{\\mathbf{Z}}]}{f(x|{% \\mathbf{Z}})} \\displaystyle{\\mathbb{E}}[Y|do(x)] \\displaystyle={\\mathbb{E}}[{\\mathbb{E}}[Y|do(X),{\\mathbf{Z}}]] \\displaystyle={\\mathbb{E}}\\left[\\frac{{\\mathbb{E}}[g(Y){\\mathbb{I}}(X=x)|{% \\mathbf{Z}}]}{f(x|{\\mathbf{Z}})}\\right] \\displaystyle={\\mathbb{E}}\\left[{\\mathbb{E}}\\left[\\frac{g(Y){\\mathbb{I}}(X=x)}% {f(x|{\\mathbf{Z}})}|{\\mathbf{Z}}\\right]\\right] \\displaystyle={\\mathbb{E}}\\left[\\frac{g(Y){\\mathbb{I}}(X=x)}{f(x|{\\mathbf{Z}})% }\\right] ∎ 6.E.3 Teorema 3.26 Lema 6.13. Se (W_{n})_{n\\in\\mathbb{N}} é uma sequência de variáveis aleatórias tais que {\\mathbb{E}}[|W_{n}|]=o(1) , então W_{n}\\stackrel{{\\scriptstyle{\\mathbb{P}}}}{{\\longrightarrow}}0 . Demonstração. \\displaystyle{\\mathbb{P}}(|W_{n}|>\\epsilon) \\displaystyle\\leq\\frac{{\\mathbb{E}}[|W_{n}|]}{\\epsilon} Markov \\displaystyle=o(1) ∎ Prova do Teorema 3.26. Como {\\mathbb{E}}[|\\mu(x,{\\mathbf{Z}})|]<\\infty , pela Lei dos Grandes Números, \\displaystyle\\frac{\\sum_{i=1}^{n}\\mu(x,{\\mathbf{Z}}_{i})}{n} \\displaystyle\\stackrel{{\\scriptstyle{\\mathbb{P}}}}{{\\longrightarrow}}{\\mathbb{% E}}[\\mu(x,{\\mathbf{Z}})] Portanto, pelo Teorema 3.21, é suficiente provar que {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]-\\frac{\\sum_{i=1}^{n}\\mu(x,{\\mathbf{Z}}% _{i})}{n}\\stackrel{{\\scriptstyle{\\mathbb{P}}}}{{\\longrightarrow}}0 . Usando o Lema 6.13, é suficiente provar que {\\mathbb{E}}\\left[\\bigg{|}{\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]-\\frac{\\sum_{% i=1}^{n}\\mu(x,{\\mathbf{Z}}_{i})}{n}\\bigg{|}\\right]=o(1) . \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}{\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]% -\\frac{\\sum_{i=1}^{n}\\mu(x,{\\mathbf{Z}}_{i})}{n}\\bigg{|}\\right] \\displaystyle={\\mathbb{E}}\\left[\\bigg{|}\\frac{\\sum_{i=1}^{n}(\\hat{\\mu}(x,{% \\mathbf{Z}}_{i})-\\mu(x,{\\mathbf{Z}}_{i}))}{n}\\bigg{|}\\right] \\displaystyle\\leq n^{-1}\\sum_{i=1}^{n}{\\mathbb{E}}\\left[|\\hat{\\mu}(x,{\\mathbf{% Z}}_{i})-\\mu(x,{\\mathbf{Z}}_{i})|\\right] \\displaystyle={\\mathbb{E}}\\left[|\\hat{\\mu}(x,{\\mathbf{Z}})-\\mu(x,{\\mathbf{Z}})% |\\right] \\displaystyle=o(1)] ∎ 6.E.4 Teorema 3.29 Prova do Teorema 3.29. Pela Lei dos Grandes números, n^{-1}\\sum_{i=1}^{n}\\frac{Y_{i}{\\mathbb{I}}(X_{i}=x)}{f(x|{\\mathbf{Z}}_{i})}% \\stackrel{{\\scriptstyle{\\mathbb{P}}}}{{\\longrightarrow}}{\\mathbb{E}}\\left[% \\frac{Y{\\mathbb{I}}(X=x)}{f(x|{\\mathbf{Z}})}\\right] . Como pelo Teorema 3.22 temos que {\\mathbb{E}}\\left[\\frac{Y{\\mathbb{I}}(X=x)}{f(x|{\\mathbf{Z}})}\\right]={\\mathbb% {E}}[Y|do(X=x)] , usando o Lema 6.13 é suficiente provar que \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}n^{-1}\\sum_{i=1}^{n}\\frac{Y_{i}{\\mathbb% {I}}(X_{i}=x)}{\\widehat{f}(x|{\\mathbf{Z}}_{i})}-n^{-1}\\sum_{i=1}^{n}\\frac{Y_{i% }{\\mathbb{I}}(X_{i}=x)}{f(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle=o(1). \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}n^{-1}\\sum_{i=1}^{n}\\frac{Y_{i}{\\mathbb% {I}}(X_{i}=x)}{\\widehat{f}(x|{\\mathbf{Z}}_{i})}-n^{-1}\\sum_{i=1}^{n}\\frac{Y_{i% }{\\mathbb{I}}(X_{i}=x)}{f(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle\\leq \\displaystyle n^{-1}\\sum_{i=1}^{n}{\\mathbb{E}}\\left[\\bigg{|}\\frac{Y_{i}{% \\mathbb{I}}(X_{i}=x)}{\\widehat{f}(x|{\\mathbf{Z}}_{i})}-\\frac{Y_{i}{\\mathbb{I}}% (X_{i}=x)}{f(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle= \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}\\frac{Y_{1}{\\mathbb{I}}(X_{1}=x)}{% \\widehat{f}(x|{\\mathbf{Z}}_{1})}-\\frac{Y_{1}{\\mathbb{I}}(X_{1}=x)}{f(x|{% \\mathbf{Z}}_{1})}\\bigg{|}\\right] \\displaystyle= \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}\\frac{Y_{i}{\\mathbb{I}}(X_{i}=x)(% \\widehat{f}(x|{\\mathbf{Z}}_{i})-f(x|{\\mathbf{Z}}_{i}))}{\\widehat{f}(x|{\\mathbf% {Z}}_{i})f(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle\\leq \\displaystyle\\delta^{-2}{\\mathbb{E}}\\left[|Y_{i}{\\mathbb{I}}(X_{i}=x)(\\widehat% {f}(x|{\\mathbf{Z}}_{i})-f(x|{\\mathbf{Z}}_{i}))|\\right] \\displaystyle\\inf_{z}\\min\\{f(x|{\\mathbf{Z}}_{1}),\\widehat{f}(x|{\\mathbf{Z}}_{1% })\\}>\\delta \\displaystyle= \\displaystyle\\delta^{-2}{\\mathbb{E}}\\left[|\\widehat{f}(x|{\\mathbf{Z}}_{i})-f(x% |{\\mathbf{Z}}_{i})|\\cdot{\\mathbb{E}}[|Y_{i}{\\mathbb{I}}(X_{i}=x)|\\big{|}{% \\mathbf{Z}}]\\right] Lei da esperança total \\displaystyle\\leq \\displaystyle M\\delta^{-2}{\\mathbb{E}}\\left[|\\widehat{f}(x|{\\mathbf{Z}}_{i})-f% (x|{\\mathbf{Z}}_{i})|\\right] \\displaystyle\\sup_{z}{\\mathbb{E}}[|Y_{i}{\\mathbb{I}}(X_{i}=x)|{\\mathbf{Z}}={% \\mathbf{z}}]<M \\displaystyle= \\displaystyle o(1) ∎ 6.E.5 Teorema 3.32 Prova do Teorema 3.32. Se as condições do Teorema 3.26 estão satisfeitas, então decorre deste resultado que {\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]\\stackrel{{\\scriptstyle{\\mathbb{P}}}}{{% \\longrightarrow}}{\\mathbb{E}}[Y|do(X=x)] . Portanto, usando Lema 6.13, resta demonstrar que \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}{\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)]% -\\sum_{i=1}^{n}\\frac{{\\mathbb{I}}(X_{i}=x)\\hat{\\mu}(x,{\\mathbf{Z}}_{i})}{n\\hat% {f}(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle=o(1) \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}{\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)]% -\\sum_{i=1}^{n}\\frac{{\\mathbb{I}}(X_{i}=x)\\hat{\\mu}(x,{\\mathbf{Z}}_{i})}{n\\hat% {f}(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle= \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}\\sum_{i=1}^{n}\\frac{{\\mathbb{I}}(X_{i}=% x)(Y_{i}-\\hat{\\mu}(x,{\\mathbf{Z}}_{i}))}{n\\hat{f}(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle\\leq \\displaystyle n{-1}\\sum_{i=1}^{n}{\\mathbb{E}}\\left[\\bigg{|}\\frac{{\\mathbb{I}}(% X_{i}=x)(Y_{i}-\\hat{\\mu}(x,{\\mathbf{Z}}_{i}))}{\\hat{f}(x|{\\mathbf{Z}}_{i})}% \\bigg{|}\\right] \\displaystyle= \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}\\frac{{\\mathbb{I}}(X_{1}=x)(Y_{1}-\\hat{% \\mu}(x,{\\mathbf{Z}}_{1}))}{\\hat{f}(x|{\\mathbf{Z}}_{1})}\\bigg{|}\\right] \\displaystyle\\leq \\displaystyle\\delta^{-1}{\\mathbb{E}}\\left[\\big{|}{\\mathbb{I}}(X_{1}=x)(Y_{1}-% \\hat{\\mu}(x,{\\mathbf{Z}}_{1}))\\big{|}\\right] \\displaystyle\\inf_{\\mathbf{z}}\\widehat{f}(x|{\\mathbf{z}})>\\delta \\displaystyle\\leq \\displaystyle\\delta^{-1}{\\mathbb{E}}\\left[\\big{|}{\\mathbb{I}}(X_{1}=x)({% \\mathbb{E}}[Y_{1}|X_{1},{\\mathbf{Z}}_{1}]-\\hat{\\mu}(x,{\\mathbf{Z}}_{1}))\\big{|% }\\right] Lei da esperança total \\displaystyle= \\displaystyle\\delta^{-1}{\\mathbb{E}}\\left[\\big{|}{\\mathbb{I}}(X_{1}=x)({% \\mathbb{E}}[Y_{1}|X_{1}=x,{\\mathbf{Z}}_{1}]-\\hat{\\mu}(x,{\\mathbf{Z}}_{1}))\\big% {|}\\right] \\displaystyle{\\mathbb{I}}(X_{1}=x){\\mathbb{E}}[Y_{1}|X_{1},{\\mathbf{Z}}_{1}]% \\equiv{\\mathbb{I}}(X_{1}=x){\\mathbb{E}}[Y_{1}|X_{1}=x,{\\mathbf{Z}}_{1}] \\displaystyle\\leq \\displaystyle\\delta^{-1}{\\mathbb{E}}\\left[\\big{|}{\\mathbb{E}}[Y_{1}|X_{1}=x,{% \\mathbf{Z}}_{1}]-\\hat{\\mu}(x,{\\mathbf{Z}}_{1})\\big{|}\\right] \\displaystyle= \\displaystyle{\\mathbb{E}}\\left[\\big{|}\\mu(x,{\\mathbf{Z}}_{1})-\\hat{\\mu}(x,{% \\mathbf{Z}}_{1})\\big{|}\\right]=o(1) A seguir, se as condições do Teorema 3.29 estão satisfeitas, então decorre deste resultado que {\\widehat{{\\mathbb{E}}}}_{2}[Y|do(X=x)]\\stackrel{{\\scriptstyle{\\mathbb{P}}}}{{% \\longrightarrow}}{\\mathbb{E}}[Y|do(X=x)] . Portanto, usando Lema 6.13, resta demonstrar que \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}{\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]% -\\sum_{i=1}^{n}\\frac{{\\mathbb{I}}(X_{i}=x)\\hat{\\mu}(x,{\\mathbf{Z}}_{i})}{n\\hat% {f}(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle=o(1) \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}{\\widehat{{\\mathbb{E}}}}_{1}[Y|do(X=x)]% -\\sum_{i=1}^{n}\\frac{{\\mathbb{I}}(X_{i}=x)\\hat{\\mu}(x,{\\mathbf{Z}}_{i})}{n\\hat% {f}(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle= \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}\\sum_{i=1}^{n}\\frac{(\\widehat{f}(x|{% \\mathbf{Z}}_{i})-{\\mathbb{I}}(X_{i}=x))\\hat{\\mu}(x,{\\mathbf{Z}}_{i})}{n% \\widehat{f}(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle\\leq \\displaystyle n^{-1}\\sum_{i=1}^{n}{\\mathbb{E}}\\left[\\bigg{|}\\frac{(\\widehat{f}% (x|{\\mathbf{Z}}_{i})-{\\mathbb{I}}(X_{i}=x))\\hat{\\mu}(x,{\\mathbf{Z}}_{i})}{% \\widehat{f}(x|{\\mathbf{Z}}_{i})}\\bigg{|}\\right] \\displaystyle= \\displaystyle{\\mathbb{E}}\\left[\\bigg{|}\\frac{(\\widehat{f}(x|{\\mathbf{Z}}_{1})-% {\\mathbb{I}}(X_{1}=x))\\hat{\\mu}(x,{\\mathbf{Z}}_{1})}{\\widehat{f}(x|{\\mathbf{Z}% }_{1})}\\bigg{|}\\right] \\displaystyle\\leq \\displaystyle\\delta^{-1}{\\mathbb{E}}\\left[\\big{|}(\\widehat{f}(x|{\\mathbf{Z}}_{% 1})-{\\mathbb{I}}(X_{1}=x))\\hat{\\mu}(x,{\\mathbf{Z}}_{1})\\big{|}\\right] \\displaystyle\\inf_{\\mathbf{z}}\\widehat{f}(x|{\\mathbf{Z}}_{1})>\\delta \\displaystyle\\leq \\displaystyle\\delta^{-1}M{\\mathbb{E}}\\left[\\big{|}\\widehat{f}(x|{\\mathbf{Z}}_{% 1})-{\\mathbb{I}}(X_{1}=x)\\big{|}\\right] \\displaystyle\\sup_{\\mathbf{z}}\\hat{\\mu}(x,{\\mathbf{z}})<M \\displaystyle= \\displaystyle\\delta^{-1}M{\\mathbb{E}}\\left[\\big{|}\\widehat{f}(x|{\\mathbf{Z}}_{% 1})-{\\mathbb{E}}[{\\mathbb{I}}(X_{1}=x)|{\\mathbf{Z}}_{1}]\\big{|}\\right] Lei da esperança total \\displaystyle= \\displaystyle\\delta^{-1}M{\\mathbb{E}}\\left[\\big{|}\\widehat{f}(x|{\\mathbf{Z}}_{% 1})-f(x|{\\mathbf{Z}}_{1})\\big{|}\\right]=o(1) ∎ 6.E.6 Teorema 3.39 Prova do Teorema 3.39. Se X\\equiv{\\mathbb{I}}({\\mathbf{Z}}\\geq{\\mathbf{z}}_{1}) , então: \\displaystyle{CACE}({\\mathbf{Z}}={\\mathbf{z}}_{1}) \\displaystyle={\\mathbb{E}}[Y|do(X=1),{\\mathbf{Z}}={\\mathbf{z}}_{1}]-{\\mathbb{E% }}[Y|do(X=0),{\\mathbf{Z}}={\\mathbf{z}}_{1}] \\displaystyle=\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}}_{1}}{\\mathbb{E}}[Y|do(X% =1),{\\mathbf{Z}}={\\mathbf{z}}]-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}{% \\mathbb{E}}[Y|do(X=0),{\\mathbf{Z}}={\\mathbf{z}}] continuidade \\displaystyle=\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}}_{1}}{\\mathbb{E}}[Y|X=1,% {\\mathbf{Z}}={\\mathbf{z}}]-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}{\\mathbb% {E}}[Y|X=0,{\\mathbf{Z}}={\\mathbf{z}}] \\displaystyle=\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}}_{1}}{\\mathbb{E}}[Y|{% \\mathbf{Z}}={\\mathbf{z}}]-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}{\\mathbb{% E}}[Y|{\\mathbf{Z}}={\\mathbf{z}}] \\displaystyle X\\equiv{\\mathbb{I}}({\\mathbf{Z}}\\geq{\\mathbf{z}}_{1}) A seguir, considere que f(x|{\\mathbf{Z}})\\in(0,1) é contínua exceto em {\\mathbf{z}}_{1} . Primeiramente, note que \\displaystyle{\\mathbb{E}}[Y|{\\mathbf{Z}}] \\displaystyle={\\mathbb{E}}[{\\mathbb{E}}[Y|X,{\\mathbf{Z}}]|{\\mathbf{Z}}] \\displaystyle={\\mathbb{E}}[Y|X=1,{\\mathbf{Z}}]f(X=1|{\\mathbf{Z}})+{\\mathbb{E}}% [Y|X=0,{\\mathbf{Z}}](1-f(X=1|{\\mathbf{Z}})) \\displaystyle=({\\mathbb{E}}[Y|X=1,{\\mathbf{Z}}]-{\\mathbb{E}}[Y|X=0,{\\mathbf{Z}% }])f(X=1|{\\mathbf{Z}})+{\\mathbb{E}}[Y|X=0,{\\mathbf{Z}}] \\displaystyle={CACE}({\\mathbf{Z}})f(X=1|{\\mathbf{Z}})+{\\mathbb{E}}[Y|do(X=0),{% \\mathbf{Z}}] (15) Como {\\mathbb{E}}[Y|do(X=0),{\\mathbf{Z}}] e {\\mathbb{E}}[Y|do(X=1),{\\mathbf{Z}}] são contínuas em {\\mathbf{z}}_{1} , {CACE}({\\mathbf{Z}}) também é contínua em {\\mathbf{z}}_{1} . Assim, decorre da Seção 6.E.6 que \\displaystyle\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}}_{1}}{\\mathbb{E}}[Y|{% \\mathbf{Z}}={\\mathbf{z}}] \\displaystyle={CACE}({\\mathbf{z}}_{1})\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}}% _{1}}f(X=1|{\\mathbf{z}})+{\\mathbb{E}}[Y|do(X=0),{\\mathbf{Z}}={\\mathbf{z}}_{1}] \\displaystyle\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}{\\mathbb{E}}[Y|{% \\mathbf{Z}}={\\mathbf{z}}] \\displaystyle={CACE}({\\mathbf{z}}_{1})\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{% 1}}f(X=1|{\\mathbf{z}})+{\\mathbb{E}}[Y|do(X=0),{\\mathbf{Z}}={\\mathbf{z}}_{1}] Finalmente subtraindo as equações acima, obtemos \\displaystyle\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}}_{1}}{\\mathbb{E}}[Y|{% \\mathbf{Z}}={\\mathbf{z}}]-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}{\\mathbb{% E}}[Y|{\\mathbf{Z}}={\\mathbf{z}}] \\displaystyle={CACE}({\\mathbf{z}}_{1})(\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}% }_{1}}f(X=1|{\\mathbf{z}})-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}f(X=1|{% \\mathbf{z}})) \\displaystyle{CACE}({\\mathbf{z}}_{1}) \\displaystyle=\\frac{\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf{z}}_{1}}{\\mathbb{E}}[% Y|{\\mathbf{Z}}={\\mathbf{z}}]-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}{% \\mathbb{E}}[Y|{\\mathbf{Z}}={\\mathbf{z}}]}{\\lim_{{\\mathbf{z}}\\downarrow{\\mathbf% {z}}_{1}}f(X=1|{\\mathbf{z}})-\\lim_{{\\mathbf{z}}\\uparrow{\\mathbf{z}}_{1}}f(X=1|% {\\mathbf{z}})} ∎ Previous page Next page"],[["index.html","chapter6.html","chapter6.A6.html"],"Apêndice 6.F Seções 6 e 7 (6 Controlando mediadores (critério frontdoor)) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. e Apêndice 6.F Seções 6 e 7 (Controlando mediadores (critério frontdoor)) 6.F.1 Teorema 3.48 Lema 6.14. Se {\\mathbf{Y}}\\perp^{d}{\\mathbf{Z}}|{\\mathbf{X}}\\cup{\\mathbf{W}} em {\\mathcal{G}}(\\bar{{\\mathbf{X}}}) , então \\displaystyle f({\\mathbf{Y}}|do({\\mathbf{X}}),{\\mathbf{Z}},{\\mathbf{W}}) \\displaystyle=f({\\mathbf{Y}}|do({\\mathbf{X}}),{\\mathbf{W}}) Demonstração. Seja f^{*}({\\mathcal{V}})\\equiv f({\\mathcal{V}}|do({\\mathbf{X}}={\\mathbf{x}})) . Decorre do Lema 3.2 que f^{*} é compatível com {\\mathcal{G}}(\\bar{{\\mathbf{X}}}) e que {\\mathbf{X}} é degenerado em {\\mathbf{x}} segundo f^{*} . Assim, \\displaystyle f({\\mathbf{Y}}|do({\\mathbf{X}}),{\\mathbf{Z}},{\\mathbf{W}}) \\displaystyle=f^{*}({\\mathbf{Y}}|{\\mathbf{Z}},{\\mathbf{W}}) \\displaystyle=f^{*}({\\mathbf{Y}}|{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}},{% \\mathbf{W}}) \\displaystyle{\\mathbf{X}}\\text{ é degenerado segundo }f^{*} \\displaystyle=f^{*}({\\mathbf{Y}}|{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{W}}) \\displaystyle f^{*}\\text{ compatível com }{\\mathcal{G}}(do({\\mathbf{X}}))\\text% {,} \\displaystyle{\\mathbf{Y}}\\perp^{d}{\\mathbf{Z}}|{\\mathbf{X}}\\cup{\\mathbf{W}}% \\text{ em }{\\mathcal{G}}(do({\\mathbf{X}}))\\text{, e \\lx@cref{creftype~refnum}{% thm:d-sep}} \\displaystyle=f({\\mathbf{Y}}|do({\\mathbf{X}}={\\mathbf{x}}),{\\mathbf{W}}). ∎ Lema 6.15. Se Y\\perp^{d}{\\mathbf{W}}|{\\mathbf{Z}}\\cup{\\mathbf{X}} em {\\mathcal{G}}(\\bar{{\\mathbf{X}}},\\underline{{\\mathbf{W}}}) , então \\displaystyle f(Y|do({\\mathbf{X}}),do({\\mathbf{W}}),{\\mathbf{Z}}) \\displaystyle=f(Y|do({\\mathbf{X}}),{\\mathbf{W}},{\\mathbf{Z}}) Demonstração. Seja f^{*}({\\mathcal{V}})\\equiv f({\\mathcal{V}}|do({\\mathbf{X}}={\\mathbf{x}})) . Como Y\\perp^{d}{\\mathbf{W}}|{\\mathbf{Z}}\\cup{\\mathbf{X}} em {\\mathcal{G}}(\\bar{{\\mathbf{X}}},\\underline{{\\mathbf{W}}}) , não há nenhum caminho ativo em {\\mathcal{G}}(\\bar{{\\mathbf{X}}}) de {\\mathbf{X}} em Y que inicia com {\\mathbf{W}}\\leftarrow . Isto é, {\\mathbf{X}}\\cup{\\mathbf{Z}} satisfaz o segundo item do critério backdoor para medir o efeito causal de {\\mathbf{W}} em Y . Portanto, \\displaystyle f(Y|do({\\mathbf{X}}={\\mathbf{x}}),do({\\mathbf{W}}),{\\mathbf{Z}}) \\displaystyle=f^{*}(Y|do({\\mathbf{W}}),{\\mathbf{Z}}) \\displaystyle=f^{*}(Y|do({\\mathbf{W}}),{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}) \\displaystyle=f^{*}(Y|{\\mathbf{W}},{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}) \\displaystyle=f^{*}(Y|{\\mathbf{W}},{\\mathbf{Z}}) \\displaystyle=f(Y|do({\\mathbf{X}}={\\mathbf{x}}),{\\mathbf{W}},{\\mathbf{Z}}) ∎ Lema 6.16. Se {\\mathbf{Y}}\\perp^{d}I_{{\\mathbf{X}}}|{\\mathbf{Z}}\\cup{\\mathbf{W}} em {\\mathcal{G}}(\\bar{{\\mathbf{W}}},{\\mathbf{X}}^{+}) , então: \\displaystyle f(Y|do({\\mathbf{W}}),do({\\mathbf{X}}),{\\mathbf{Z}}) \\displaystyle=f(Y|do({\\mathbf{W}}),{\\mathbf{Z}}) Demonstração. Seja f^{*}({\\mathcal{V}})\\equiv f({\\mathcal{V}}|do({\\mathbf{W}}={\\mathbf{w}})) . \\displaystyle f(Y|do({\\mathbf{W}}={\\mathbf{w}}),do({\\mathbf{X}}),{\\mathbf{Z}}) \\displaystyle=f^{*}(Y|do({\\mathbf{X}}),{\\mathbf{Z}}) \\displaystyle=f^{*}(Y|do({\\mathbf{X}}),{\\mathbf{W}}={\\mathbf{w}},{\\mathbf{Z}}) \\displaystyle=f^{*}_{*}(Y|I_{{\\mathbf{X}}}=1,{\\mathbf{W}}={\\mathbf{w}},{% \\mathbf{Z}}) \\displaystyle=f^{*}_{*}(Y|{\\mathbf{W}}={\\mathbf{w}},{\\mathbf{Z}},I_{{\\mathbf{X% }}}=0) \\displaystyle Y\\perp^{d}I_{{\\mathbf{X}}}|{\\mathbf{W}}\\cup{\\mathbf{Z}}\\text{ em% }{\\mathcal{G}}(\\bar{{\\mathbf{W}}},{\\mathbf{X}}^{+}) \\displaystyle=f^{*}(Y|{\\mathbf{W}}={\\mathbf{w}},{\\mathbf{Z}}) \\displaystyle=f(Y|do({\\mathbf{W}}={\\mathbf{w}}),{\\mathbf{Z}}) ∎ Prova do Teorema 3.48. Decorre dos Lemas 6.14, 6.15 e 6.16. ∎ 6.F.2 Teorema 3.44 Lema 6.17. Se {\\mathbf{W}} satisfaz o critério frontdoor para medir o efeito causal de X em Y , então f(Y|do(X),{\\mathbf{W}})=f(Y|do(X),do({\\mathbf{W}})) . Demonstração. Decorre do critério frontdoor Definição 3.43.3 que X satisfaz o item 2 do critério backdoor para medir o efeito causal de {\\mathbf{W}} em Y . Portanto, pelo Lema 3.49, {\\mathbf{Y}}\\perp^{d}{\\mathbf{W}}|{\\mathbf{X}} em {\\mathcal{G}}(\\underline{{\\mathbf{W}}}) . Pelo Exercício 2.56, {\\mathbf{Y}}\\perp^{d}{\\mathbf{W}}|{\\mathbf{X}} em {\\mathcal{G}}(\\bar{{\\mathbf{X}}},\\underline{{\\mathbf{W}}}) . A prova se conclui aplicando o item 2 do Teorema 3.48. ∎ Lema 6.18. Se {\\mathbf{W}} satisfaz o critério frontdoor para medir o efeito causal de X em Y , então f(Y|do(X),do({\\mathbf{W}}))=f(Y|do({\\mathbf{W}})) . Demonstração. A prova consiste em aplicar o item 3 do Teorema 3.48. Para tal, desejamos provar que {\\mathbf{Y}}\\perp^{d}I_{X}|{\\mathbf{W}} em {\\mathcal{G}}(\\bar{{\\mathbf{W}}},X^{+}) . Tome C como um caminho arbitrário em {\\mathcal{G}}(\\bar{{\\mathbf{W}}},X^{+}) de I_{{\\mathbf{X}}} em Y . Primeiramente, provaremos que C não é um caminho direcionado. C não é um caminho direcionado de Y a I_{X} pois a única aresta em {\\mathcal{G}}(\\bar{{\\mathbf{W}}},X^{+}) ligada a I_{X} é I_{X}\\rightarrow X . A seguir, suponha que C é um caminho direcionado de I_{X} a Y . Pelo Definição 3.43.2, existe C_{i} tal que C_{i}\\in{\\mathbf{W}} . Como C é direcionado de I_{X} em Y , C_{i-1}\\rightarrow C_{i} . Este é um absurdo, pois não há aresta apontando para {\\mathbf{W}} em {\\mathcal{G}}(\\bar{{\\mathbf{W}}},X^{+}) . Portanto, C não é um caminho direcionado. Conclua que existe C_{i} que é um colisor. Como não há arestas apontando para {\\mathbf{W}} em {\\mathcal{G}}(\\bar{{\\mathbf{W}}},X^{+}) , não há W\\in{\\mathbf{W}} que é descendente de C_{i} . Como C_{i} é um colisor e C_{i} não tem descendentes em {\\mathbf{W}} , conclua que C está bloqueado. Como C era arbitrário, {\\mathbf{Y}}\\perp^{d}I_{X}|{\\mathbf{W}} em {\\mathcal{G}}(\\bar{{\\mathbf{W}}},X^{+}) . ∎ Prova do Teorema 3.44. \\displaystyle f(Y|do(X=x)) \\displaystyle=\\int f(Y|do(X=x),{\\mathbf{W}})f({\\mathbf{W}}|do(X=x))d{\\mathbf{W}} \\displaystyle=\\int f(Y|do(X=x),{\\mathbf{W}})f({\\mathbf{W}}|X=x)d{\\mathbf{W}} \\displaystyle=\\int f(Y|do(X=x),do({\\mathbf{W}}))f({\\mathbf{W}}|X=x)d{\\mathbf{W}} \\displaystyle=\\int f(Y|do({\\mathbf{W}}))f({\\mathbf{W}}|X=x)d{\\mathbf{W}} \\displaystyle=\\int\\int f(Y|{\\mathbf{W}},X)f(X)dXf({\\mathbf{W}}|X=x)d{\\mathbf{W}} ∎ 6.F.3 Teorema 3.45 Prova do Teorema 3.45. \\displaystyle{\\mathbb{E}}[Y|do(X=x)] \\displaystyle=\\int Yf(Y|do(X=x))dY \\displaystyle=\\int Y\\int f({\\mathbf{W}}|x)\\int f(Y|X,{\\mathbf{W}})f(X)dXdWdY \\displaystyle=\\int Yf(Y|X,{\\mathbf{W}})f(X)f({\\mathbf{W}}|x)d(Y\\times{\\mathbf{% W}}\\times Y) \\displaystyle=\\int\\frac{Yf({\\mathbf{W}}|x)}{f({\\mathbf{W}}|X)}f(Y,{\\mathbf{W}}% ,X)d(Y\\times{\\mathbf{W}}\\times Y) \\displaystyle={\\mathbb{E}}\\left[\\frac{Yf({\\mathbf{W}}|x)}{f({\\mathbf{W}}|X)}\\right] ∎ Previous page Next page"],[["index.html","chapter6.html","chapter6.A7.html"],"Apêndice 6.G Seção 8 (8 Levando a intuição do SCM ao POM) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. ( Apêndice 6.G Seção 8 (Levando a intuição do SCM ao POM) Prova do Lema 4.10. Se {\\mathbf{Z}}_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)\\neq{\\mathbf{z}} , então {\\mathbb{I}}({\\mathbf{Z}}_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)={\\mathbf{z}})=0 e \\displaystyle W_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega){\\mathbb{I}}({\\mathbf{Z}}_{% {\\mathbb{V}}={\\mathbf{v}}}(\\omega)={\\mathbf{z}}) \\displaystyle=0=W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}}(\\omega% ){\\mathbb{I}}({\\mathbf{Z}}_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)={\\mathbf{z}}) Se {\\mathbf{Z}}_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)={\\mathbf{z}} , então {\\mathbb{I}}({\\mathbf{Z}}_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)={\\mathbf{z}})=1 e basta provar que W_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)=W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}% }={\\mathbf{v}}}(\\omega) . Para tal, considere primeiramente que W\\in{\\mathbf{Z}} . Pela definição de \\omega , {\\mathbf{Z}}_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)={\\mathbf{z}}={\\mathbf{Z}}_{{% \\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}}(\\omega) . Portanto, W_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)=W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}% }={\\mathbf{v}}}(\\omega) . Similarmente, se W\\in{\\mathbb{V}} , decorre da Definição 4.7 que {\\mathbb{V}}_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)={\\mathbf{v}}={\\mathbb{V}}_{{% \\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}}(\\omega) . Portanto, W_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)=W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}% }={\\mathbf{v}}}(\\omega) . Assim, resta considerar o caso em que W\\notin({\\mathbf{Z}}\\cup{\\mathbb{V}}) . Para provar este fato, construíremos uma ordem sobre {\\mathcal{V}}-({\\mathbf{Z}}\\cup{\\mathbb{V}}) . Defina {\\mathcal{V}}^{(0)}=\\{W\\in{\\mathcal{V}}-({\\mathbf{Z}}\\cup{\\mathbb{V}}):Pa(W)=\\emptyset\\} , isto é {\\mathcal{V}}^{(0)} são vértices no DAG que não estão em {\\mathbf{Z}}\\cup{\\mathbb{V}} e que são raízes. Além disso, para todo 1\\leq i\\leq n , {\\mathcal{V}}^{(i)}=\\{W\\in{\\mathcal{V}}-({\\mathbf{Z}}\\cup{\\mathbb{V}}):Pa(W)% \\subseteq{\\mathcal{V}}^{(i-1)}\\cup({\\mathbf{Z}}\\cup{\\mathbb{V}})\\} . Isto é, todos os pais de {\\mathcal{V}}^{(1)} são raízes ou estão em ({\\mathbf{Z}}\\cup{\\mathbb{V}}) , todos os avós de {\\mathcal{V}}^{(2)} são raízes ou estão em ({\\mathbf{Z}}\\cup{\\mathbb{V}}) , e assim por diante. Como {\\mathcal{V}} é finito, existe n tal que {\\mathcal{V}}^{(n)}\\equiv{\\mathcal{V}}-({\\mathbb{U}}\\cup{\\mathbb{V}}) . Completaremos a prova por indução finita. Primeiramente, se W\\in{\\mathcal{V}}^{(0)} , decorre da Definição 4.7 que W_{{\\mathbb{V}}={\\mathbf{v}}}\\equiv g_{W}(U_{W})\\equiv W_{{\\mathbf{Z}}={% \\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}} . Em particular, W_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)=W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}% }={\\mathbf{v}}}(\\omega) . A seguir, suponha que para todo W\\in{\\mathcal{V}}^{(i-1)} , W_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)=W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}% }={\\mathbf{v}}}(\\omega) e tome W\\in{\\mathcal{V}}^{(i)} . Por definição de {\\mathcal{V}}^{(i)} , Pa(W)\\subseteq{\\mathcal{V}}^{(i-1)}\\cup({\\mathbf{Z}}\\cup{\\mathbb{V}}) . Tome W^{*}\\in Pa(W) . Por hipótese de indução, se W^{*}\\in{\\mathcal{V}}^{(i-1)} , então W^{*}_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)=W^{*}_{{\\mathbf{Z}}={\\mathbf{z}},{% \\mathbb{V}}={\\mathbf{v}}}(\\omega) . Também provamos que, se W^{*}\\in{\\mathbf{Z}}\\cup{\\mathbb{V}} , então W^{*}_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)=W^{*}_{{\\mathbf{Z}}={\\mathbf{z}},{% \\mathbb{V}}={\\mathbf{v}}}(\\omega) . Conclua que Pa^{*}(W_{{\\mathbb{V}}={\\mathbf{v}}})(\\omega)=Pa^{*}(W_{{\\mathbf{Z}}={\\mathbf{% z}},{\\mathbb{V}}={\\mathbf{v}}})(\\omega) . Como decorre da Definição 4.7 que W_{{\\mathbb{V}}={\\mathbf{v}}}\\equiv g_{W}(U_{W},Pa^{*}(W_{{\\mathbb{V}}={% \\mathbf{v}}})) e W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}}\\equiv g_{W}(U_{W},Pa^{% *}(W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}})) e Pa^{*}(W_{{\\mathbb{V}}={\\mathbf{v}}})(\\omega)=Pa(W^{*}_{{\\mathbf{Z}}={\\mathbf{% z}},{\\mathbb{V}}={\\mathbf{v}}})(\\omega) , conclua que W_{{\\mathbb{V}}={\\mathbf{v}}}(\\omega)=W_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}% }={\\mathbf{v}}}(\\omega) . A prova está completa observando que W\\in{\\mathcal{V}}^{(}n)={\\mathcal{V}}-({\\mathbf{Z}}\\cup{\\mathbb{V}}) . ∎ Prova do Lema 4.9. \\displaystyle{\\mathbb{P}}({\\mathcal{V}}_{{\\mathbb{V}}={\\mathbf{v}}}={\\mathcal{% V}}_{{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}}|{\\mathbf{Z}}_{{% \\mathbb{V}}={\\mathbf{v}}}={\\mathbf{z}}) \\displaystyle= \\displaystyle{\\mathbb{P}}(W_{{\\mathbb{V}}={\\mathbf{v}}}=W_{{\\mathbf{Z}}={% \\mathbf{z}},{\\mathbb{V}}={\\mathbf{v}}},\\forall W\\in{\\mathcal{V}}|{\\mathbf{Z}}_% {{\\mathbb{V}}={\\mathbf{v}}}={\\mathbf{z}}) \\displaystyle= \\displaystyle{\\mathbb{P}}(W_{{\\mathbb{V}}={\\mathbf{v}}}{\\mathbb{I}}({\\mathbf{Z% }}_{{\\mathbb{V}}={\\mathbf{v}}}={\\mathbf{z}})=W_{{\\mathbf{Z}}={\\mathbf{z}},{% \\mathbb{V}}={\\mathbf{v}}}{\\mathbb{I}}({\\mathbf{Z}}_{{\\mathbb{V}}={\\mathbf{v}}}% ={\\mathbf{z}}),\\forall W\\in{\\mathcal{V}}|{\\mathbf{Z}}_{{\\mathbb{V}}={\\mathbf{v% }}}={\\mathbf{z}}) \\displaystyle= \\displaystyle 1 ∎ Prova do Lema 4.11. \\displaystyle f({\\mathcal{V}}_{{\\mathbf{v}}}) \\displaystyle={\\mathbb{I}}({\\mathbb{V}}={\\mathbf{v}})\\cdot\\prod_{V\\in{\\mathcal% {V}}-{\\mathbb{V}}}f^{*}(V|Pa(V)) \\displaystyle={\\mathbb{I}}({\\mathbb{V}}={\\mathbf{v}})\\cdot\\prod_{V\\in{\\mathcal% {V}}-{\\mathbb{V}}}f(V|Pa(V)) Definições 4.3 e 4.7 \\displaystyle=f({\\mathcal{V}}|do({\\mathbb{V}}={\\mathbf{v}})) ∎ Prova do Lema 4.13. ( 1\\rightarrow 2 ) Para realizar esta demonstração provaremos que a negação de 2 implica a negação de 1 . Suponha que exista um ascendente comum de X e Y . Portanto, existe V\\in{\\mathcal{V}} , um caminho direcionado de V em X , C^{X}=(V,C^{X}_{2},\\ldots,C^{X}_{n-1},X) , e um caminho direcionado de V em Y , C^{Y}=(V,C^{Y}_{2},\\ldots,C^{Y}_{m-1},Y) . Defina C=(X,C^{X}_{n-1},\\ldots,C^{X}_{2},V,C^{Y}_{2},\\ldots C^{Y}_{m-1},Y) . Como C^{X} é um caminho direcionado, (C^{X}_{n-1},X)\\in{\\mathcal{E}} . Além disso, como C^{X} e C^{Y} são caminhos direcionados, não há colisor em C . Conclua que C está bloqueado dado \\emptyset . Isto é, \\emptyset não satisfaz o critério backdoor para medir o efeito causal de X em Y . ( 2\\rightarrow 3 ) Para realizar esta demonstração provaremos que a negação de 3 implica a negação de 2 . Suponha que no grafo potencial, {\\mathcal{G}}^{*} , existe um caminho não bloqueado de X a Y_{x} , C . Portanto, existe V\\in{\\mathcal{V}} tal que C=X,\\ldots V\\leftarrow U_{V}\\rightarrow V_{x},\\ldots Y_{x} . Como C não está bloqueado, não há colisor em C . Portanto, \\displaystyle C \\displaystyle=X\\leftarrow\\ldots\\leftarrow V\\leftarrow U_{V}\\rightarrow V_{x}% \\rightarrow\\ldots\\rightarrow Y_{x}. Decorre da Definição 4.5 que V é ancestral comum de X e Y . Para constatar essa última afirmação basta remover U_{V} e V_{x} do caminho e substituir cada vértice potencial a partir de V_{x} por sua cópia em {\\mathcal{V}} . ( 3\\rightarrow 1 ) Esta demonstração decorre do Lema 4.16, provado a seguir, tomando {\\mathbf{Z}}=\\emptyset . ∎ Prova do Lema 4.16. Suponha que {\\mathbf{Z}} não satisfaz o critério backdoor para medir o efeito causal de X em Y . Como X\\notin Anc({\\mathbf{Z}}) , existe um caminho não bloqueado de X em Y dado {\\mathbf{Z}} , C=(X,C_{2},\\ldots,C_{n-1},Y) tal que X\\leftarrow C_{2} . Há dois casos para considerar: C_{n-1}\\leftarrow Y e C_{n-1}\\rightarrow Y . Se C_{n-1}\\leftarrow Y , então não há colisor em C_{n-1}\\leftarrow Y\\leftarrow U_{Y}\\rightarrow Y_{x} . Portanto, C^{*}=(X,C_{2},\\ldots,C_{n-1},Y,U_{Y},Y_{x}) é um caminho desbloqueado de X a Y_{x} no grafo potencial. A seguir, considere que C_{n-1}\\rightarrow Y . Tome m=\\max(\\{1\\}\\cup\\{i:C_{i}\\text{ é colisor}\\}) . Assim, C_{m}\\leftarrow C_{m+1}\\ldots C_{n-1}\\rightarrow Y . Pelo diagrama acima, existe p>m tal que C_{p-1}\\leftarrow C_{p}\\rightarrow C_{p+1} . Defina C^{*}=(X_{x},(C_{2})_{x},\\ldots,(C_{n-1})_{x},Y_{x}) . Não há colisor em C_{p-1}\\leftarrow C_{p}\\leftarrow U_{C_{p}}\\rightarrow C^{*}_{p} . Também, como não há colisor em (C_{p},C_{p+1},\\ldots,Y) , decorre da Definição 4.5 que não há colisor em (C^{*}_{p},C^{*}_{p+1},\\ldots Y) . Portanto, não há colisor em (C_{p},U_{C_{p}},C^{*}_{p},C^{*}_{p+1},\\ldots Y) . Defina \\displaystyle C^{+} \\displaystyle=(X,C_{2},\\ldots,C_{p},U_{C_{p}},C^{*}_{p},C^{*}_{p+1},\\ldots Y). Como C está bloqueado dado {\\mathbf{Z}} , para todo i\\leq p , C_{i}\\in{\\mathbf{Z}} se e somente se C_{i} é um colisor em C . Além disso, para todo i>p , C^{+}_{i} não é colisor e C^{+}_{i} não está em {\\mathbf{Z}} , pois é um resultado potencial ou uma variável em U . Assim C^{+}_{i} não está bloqueado dado {\\mathbf{Z}} e é um caminho de X a Y_{x} . ∎ Previous page Next page"],[["index.html","chapter6.html","chapter6.A8.html"],"Apêndice 6.H Seção 9 (9 Variáveis Instrumentais) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. ( Apêndice 6.H Seção 9 (Variáveis Instrumentais) Definição 6.19. {\\mathbb{U}}=\\{U_{V}:V\\in{\\mathcal{V}}\\} , {\\mathcal{V}}^{(1)}=\\{V\\in{\\mathcal{V}}:Pa(V)=\\emptyset\\} , e {\\mathcal{V}}^{(i)}=\\{V\\in{\\mathcal{V}}:Pa(V)\\subseteq{\\mathcal{V}}^{(i-1)}\\} . Lema 6.20. No modelo de resultados potenciais (Definição 4.7) se Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}}})\\cap{\\mathbb{U}}=Anc^{*}({\\mathbf{Y}}_{{% \\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}})\\cap{\\mathbb{U}} , então para cada V\\in Pa(Y) , Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}}})\\cap{\\mathbb{U}}=Anc^{*}(V_{{\\mathbf{X}}% ={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}})\\cap{\\mathbb{U}} . Demonstração. Primeiramente, note que para todo Z\\in{\\mathbf{Z}} , tem-se que U_{Z}\\notin Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}})=% Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}}}) . Assim, como Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}}})\\subseteq Anc^{*}(Y_{{\\mathbf{X}}={% \\mathbf{x}}}) , U_{Z}\\notin Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}}}) . Isto é, {\\mathbf{Z}}_{{\\mathbf{X}}={\\mathbf{x}}}\\cap Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{% x}}})=\\emptyset . A seguir, por construção da Definição 4.7, Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}})\\cap{\\mathbb{U% }}\\subseteq Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}}})\\cap{\\mathbb{U}} . Assim, basta provar que para todo U\\in Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}}})\\cap{\\mathbb{U}} tem-se que U\\in Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}}) . Tome U\\in Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}}})\\cap{\\mathbb{U}} . Por construção, existem vértices, C_{1},\\ldots,C_{m}\\in{\\mathcal{V}} que constituem um caminho direcionado de U a V_{{\\mathbf{X}}={\\mathbf{x}}} , (U,(C_{1})_{{\\mathbf{X}}={\\mathbf{x}}},\\ldots,(C_{m})_{{\\mathbf{X}}={\\mathbf{x% }}},V_{{\\mathbf{X}}={\\mathbf{x}}}) . Como {\\mathbf{Z}}_{{\\mathbf{X}}={\\mathbf{x}}}\\cap Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{% x}}})=\\emptyset , não existe V_{i} tal que V_{i}\\in{\\mathbf{Z}} . Portanto, (U,(C_{1})_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}},\\ldots,(C_{m}% )_{{\\mathbf{X}}={\\mathbf{x}}},V_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}={% \\mathbf{z}}}) é um caminho direcionado de U a V_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}} , isto é, U\\in Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}}) . ∎ Lema 6.21. No modelo de resultados potenciais (Definição 4.7), se Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}}})\\cap{\\mathbb{U}}=Anc^{*}(Y_{{\\mathbf{X}}% ={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}})\\cap{\\mathbb{U}} , então Y_{{\\mathbf{X}}={\\mathbf{x}}}\\equiv Y_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}=% {\\mathbf{z}}} . Demonstração. Faremos a demonstração por indução. Para tal, utilizaremos a Definição 6.19. Se Y\\in{\\mathcal{V}}^{(1)} , então Pa(Y)=\\emptyset . Assim, Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}}}) é \\{U_{Y}\\} ou \\emptyset . Se Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}}})=\\emptyset , então Y\\in{\\mathbf{X}} . Portanto, tomando Y como X_{i} , Y_{{\\mathbf{X}}={\\mathbf{x}}}\\equiv{\\mathbf{x}}_{i}\\equiv Y_{{\\mathbf{X}}={% \\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}} . Se Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}}})=Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}},{% \\mathbf{Z}}={\\mathbf{z}}})=\\{U_{Y}\\} , então Y_{{\\mathbf{X}}={\\mathbf{x}}}\\equiv g_{Y}(U_{Y})\\equiv Y_{{\\mathbf{X}}={% \\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}} . Agora, suponha que se V\\in{\\mathcal{V}}^{(i-1)} e Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}}})\\cap{\\mathbb{U}}=Anc^{*}(V_{{\\mathbf{X}}% ={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}})\\cap{\\mathbb{U}} , então V_{{\\mathbf{X}}={\\mathbf{x}}}\\equiv V_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}=% {\\mathbf{z}}} . Tome Y\\in{\\mathcal{V}}^{(i)} tal que Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}}})\\cap{\\mathbb{U}}=Anc^{*}(Y_{{\\mathbf{X}}% ={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}})\\cap{\\mathbb{U}} . Se U_{Y}\\notin Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}}}) , então existe Y é algum X_{i} . Portanto, Y_{{\\mathbf{X}}={\\mathbf{x}}}\\equiv{\\mathbf{x}}_{i}\\equiv Y_{{\\mathbf{X}}={% \\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}} . A seguir, suponha que U_{Y}\\in Anc^{*}(Y_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}={\\mathbf{z}}}) . Para cada V\\in Pa(Y) , como Y\\in{\\mathcal{V}}^{(i)} , V\\in{\\mathcal{V}}^{(i-1)} . Além disso, decorre do Lema 6.20, que Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{x}}})\\cap U=Anc^{*}(V_{{\\mathbf{X}}={\\mathbf{% x}},{\\mathbf{Z}}={\\mathbf{z}}})\\cap U . Portanto, decorre da hipótese de indução que V_{{\\mathbf{X}}={\\mathbf{x}}}\\equiv V_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}=% {\\mathbf{z}}} . Assim, \\displaystyle Y_{{\\mathbf{X}}={\\mathbf{x}}}\\equiv g_{Y}(U_{Y},(Pa(Y))_{{% \\mathbf{X}}={\\mathbf{x}}})\\equiv g_{Y}(U_{Y},(Pa(Y))_{{\\mathbf{X}}={\\mathbf{x}% },{\\mathbf{Z}}={\\mathbf{z}}})\\equiv Y_{{\\mathbf{X}}={\\mathbf{x}},{\\mathbf{Z}}=% {\\mathbf{z}}} ∎ Prova do Lema 4.23. A seguir, suponha que existe um caminho direcionado de I a Y , (I,C_{1},\\ldots,C_{m},Y) , que não passa por X . Escolha f tal que I\\sim\\text{Bernoulli}(0.5) e I\\equiv C_{1}\\equiv\\ldots\\equiv C_{m}\\equiv Y . Y_{X=x}\\sim\\text{Bernoulli}(0.5) e Y_{X=x,I=i}\\equiv i . Portanto, {\\mathbb{P}}(Y_{X=x}\\neq Y_{X=x,I=i})>0 . A seguir, suponha que todo caminho direcionado de I a Y , C , é tal que existe j com C_{j}=X . Iremos provar que Anc^{*}(Y_{X=x})\\cap{\\mathbb{U}}=Anc^{*}(Y_{X=x,I=i})\\cap{\\mathbb{U}} e, com base no Lema 6.21, concluir que Y_{I=i,X=x}\\equiv Y_{X=x} . Como Anc^{*}(Y_{X=x,I=i})\\cap{\\mathbb{U}}\\subseteq Anc^{*}(Y_{X=x})\\cap{\\mathbb{U}} , basta provar que todo U\\in Anc^{*}(Y_{X=x})\\cap{\\mathbb{U}} satisfaz U\\in Anc^{*}(Y_{X=x,I=i}) . Tome U\\in Anc^{*}(Y_{X=x})\\cap{\\mathbb{U}} . Assim, existem vértices C_{1},\\ldots,C_{m}\\in{\\mathcal{V}} e um caminho direcionado de U a Y_{{\\mathbf{X}}={\\mathbf{x}}} , (U,(C_{1})_{X=x},\\ldots,(C_{m})_{X=x},Y_{X=x}) . Note que se algum C_{j} fosse I , então pela hipótese do lema, existiria algum C_{k} que seria X . Assim, (U,C_{1},\\ldots,C_{m},Y_{X=x}) não seria um caminho direcionado, afinal, X_{X=x} não tem pais. Portanto, I não está em C_{1},\\ldots,C_{m} . Conclua que (U,(C_{1})_{X=x,I=i},\\ldots,(C_{m})_{X=x,I=i},Y_{X=x,I=i}) é um caminho direcionado de U a Y_{X=x,I=i} . Isto é, U\\in Anc^{*}(Y_{X=x,I=i}) . Decorre do Lema 6.21 que Y_{I=i,X=x}\\equiv Y_{X=x} . ∎ Lema 6.22. Se I é um instrumento para medir o efeito causal de X em Y e X\\in Anc(Y) , então I é ignorável para o efeito em X . Demonstração. Provaremos a contra-positiva. Se I não é ignorável para medir o efeito em X , então decorre do Lema 4.13 que I e X tem um ancestral comum, Z . Como X é um ancestral de Y , decorre que Z é ancestral comum a I e Y . Portanto, conclui-se do Lema 4.13 que I não é ignorável para Y . Isto é, pela Definição 4.22.1, I não é um instrumento. ∎ Lema 6.23. Se X é ignorável para medir o efeito causal em Y em um CM linear Gaussiano, então \\displaystyle{ACE}=Cov[X,Y]\\cdot{\\mathbb{V}}^{-1}[X]. Demonstração. Decorre do Lema 2.26 que {\\mathcal{V}} segue uma normal multivariada. Portanto, existem \\alpha e \\beta tais que \\displaystyle{\\mathbb{E}}[Y|X]=\\alpha+\\beta\\cdot X. (16) Assim, \\displaystyle{ACE} \\displaystyle=\\frac{d{\\mathbb{E}}[Y|do(X=x)]}{dx} \\displaystyle=\\frac{d{\\mathbb{E}}[Y|X=x]}{dx} \\displaystyle=\\frac{d(\\alpha+\\beta x)}{dx}=\\beta (17) Finalmente, \\displaystyle Cov[X,Y] \\displaystyle={\\mathbb{E}}[XY]-{\\mathbb{E}}[X]{\\mathbb{E}}[Y] \\displaystyle={\\mathbb{E}}[X{\\mathbb{E}}[Y|X]]-{\\mathbb{E}}[X]{\\mathbb{E}}[{% \\mathbb{E}}[Y|X]] \\displaystyle={\\mathbb{E}}[X(\\alpha+\\beta X)]-{\\mathbb{E}}[X]{\\mathbb{E}}[% \\alpha+\\beta X] \\displaystyle=\\alpha{\\mathbb{E}}[X]+\\beta{\\mathbb{E}}[X^{2}]-\\alpha{\\mathbb{E}% }[X]-\\beta{\\mathbb{E}}[X]^{2} \\displaystyle=\\beta{\\mathbb{V}}[X] \\displaystyle={ACE}\\cdot{\\mathbb{V}}[X] Rearranjando os termos, obtenha {ACE}=Cov[X,Y]\\cdot{\\mathbb{V}}^{-1}[X] . ∎ Prova do Teorema 4.24. \\displaystyle Cov[I,Y]\\cdot{\\mathbb{V}}^{-1}[I] \\displaystyle={ACE}_{I,Y} \\displaystyle=\\sum_{C\\in\\mathbb{C}_{I,Y}}\\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{% i}} \\displaystyle=\\sum_{C\\in\\mathbb{C}_{I,X}}\\sum_{K\\in\\mathbb{C}_{X,Y}}\\left(% \\prod_{i=1}^{|C|-1}\\beta_{C_{i+1},C_{i}}\\right)\\left(\\prod_{j=1}^{|K|-1}\\beta_% {K_{i+1},K_{i}}\\right) \\displaystyle=\\left(\\sum_{C\\in\\mathbb{C}_{I,X}}\\prod_{i=1}^{|C|-1}\\beta_{C_{i+% 1},C_{i}}\\right)\\left(\\sum_{K\\in\\mathbb{C}_{X,Y}}\\left(\\prod_{j=1}^{|K|-1}% \\beta_{K_{i+1},K_{i}}\\right)\\right) \\displaystyle={ACE}_{I,X}\\cdot{ACE}_{X,Y} \\displaystyle=Cov[I,X]\\cdot{\\mathbb{V}}^{-1}[I]\\cdot{ACE}_{X,Y} Lemas 6.22 e 6.23 Rearranjando os termos, obtemos {ACE}_{X,Y}=\\frac{Cov[I,Y]}{Cov[I,X]} . ∎ Prova do Teorema 4.27. \\displaystyle Y_{I=1}-Y_{I=0} \\displaystyle= \\displaystyle Y_{I=1}{\\mathbb{I}}(X_{I=1}=1)+Y_{I=1}{\\mathbb{I}}(X_{I=1}=0)-Y_% {I=0}{\\mathbb{I}}(X_{I=0}=1)-Y_{I=0}{\\mathbb{I}}(X_{I=0}=0) \\displaystyle= \\displaystyle Y_{I=1,X=1}{\\mathbb{I}}(X_{I=1}=1)+Y_{I=1,X=0}{\\mathbb{I}}(X_{I=% 1}=0)-Y_{I=0,X=1}{\\mathbb{I}}(X_{I=0}=1)-Y_{I=0,X=0}{\\mathbb{I}}(X_{I=0}=0) \\displaystyle= \\displaystyle Y_{X=1}{\\mathbb{I}}(X_{I=1}=1)+Y_{X=0}{\\mathbb{I}}(X_{I=1}=0)-Y_% {X=1}{\\mathbb{I}}(X_{I=0}=1)-Y_{X=0}{\\mathbb{I}}(X_{I=0}=0) Definição 4.22.2 \\displaystyle= \\displaystyle(Y_{X=1}-Y_{X=0})({\\mathbb{I}}(X_{I=1}=1)-{\\mathbb{I}}(X_{I=0}=1)) \\displaystyle= \\displaystyle(Y_{X=1}-Y_{X=0})(X_{I=1}-X_{I=0}) \\displaystyle X\\in\\{0,1\\} (18) Portanto, \\displaystyle{\\mathbb{E}}[Y_{I=1}-Y_{I=0}] \\displaystyle={\\mathbb{E}}[(Y_{X=1}-Y_{X=0})(X_{I=1}-X_{I=0})] \\displaystyle={\\mathbb{E}}[Y_{X=1}-Y_{X=0}|X_{I=1}-X_{I=0}=1]{\\mathbb{P}}(X_{I% =1}-X_{I=0}=1) (19) Como I é um instrumento, decorre da Definição 4.22 que Cov[I,X]\\neq 0 . Portanto, {\\mathbb{P}}(X_{I=1}-X_{I=0}=1)\\neq 0 . Reagrupando os termos na tillegg 6.H, obtemos: \\displaystyle{\\mathbb{E}}[Y_{X=1}-Y_{X=0}|X_{I=1}-X_{I=0}=1] \\displaystyle=\\frac{{\\mathbb{E}}[Y_{I=1}-Y_{I=0}]}{{\\mathbb{P}}(X_{I=1}-X_{I=0% }=1)} \\displaystyle{LATE} \\displaystyle=\\frac{{\\mathbb{E}}[Y_{I=1}-Y_{I=0}]}{{\\mathbb{P}}(X_{I=1}-X_{I=0% }=1)} (21) \\displaystyle=\\frac{{\\mathbb{E}}[Y_{I=1}-Y_{I=0}]}{{\\mathbb{E}}[X_{I=1}-X_{I=0% }]} \\displaystyle X\\in\\{0,1\\},\\text{\\lx@cref{creftype~refnum}{def:monotone}} (22) Há dois casos a considerar. Se X\\in Anc(Y) . Assim, decorre do Lema 6.22 que I é ignorável para medir X . Neste caso, podemos continuar a desenvolver tillegg 6.H: \\displaystyle{LATE} \\displaystyle=\\frac{{\\mathbb{E}}[Y|I=1]-{\\mathbb{E}}[Y|I=0]}{{\\mathbb{E}}[X|I=% 1]-{\\mathbb{E}}[X|I=0]} Definição 4.22.1, LABEL:{cor:ignore} Se X\\notin Anc(Y) , então como I é um instrumento, decorre do Lema 4.23 que I\\notin Anc(Y) . Portanto, conclua do Exercício 3.36 que {\\mathbb{E}}[Y|do(I=1)]-{\\mathbb{E}}[Y|do(I=0)]=0 , o que implica pelo Lema 4.11 que {\\mathbb{E}}[Y_{I=1}-Y_{I=0}]=0 . Como I é ignorável para Y , decorre do Corolário 4.14 que {\\mathbb{E}}[Y|I=1]-{\\mathbb{E}}[Y|I=0]=0 . Assim, decorre do tillegg 6.H que {LATE}=0=\\frac{{\\mathbb{E}}[Y|I=1]-{\\mathbb{E}}[Y|I=0]}{{\\mathbb{E}}[X|I=1]-{% \\mathbb{E}}[X|I=0]} . ∎ Previous page Next page"],[["index.html","chapter6.html","chapter6.A9.html"],"Apêndice 6.I Seção 10 (10 Contrafactuais) ‣ Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. ( Apêndice 6.I Seção 10 (Contrafactuais) Prova do Teorema 4.30. Tome um caminho arbitrário de Y_{{\\mathbf{X}}={\\mathbf{x}}} a {\\mathbf{Z}} . Decorre da Definição 4.7 que o caminho necessariamente passará por U\\in{\\mathbb{U}} . Como U é uma raiz, ele não é um colisor no caminho. Portanto, o caminho está bloqueado dado {\\mathbb{U}} . Como o caminho era arbitrário, Y_{{\\mathbf{X}}={\\mathbf{x}}}\\perp^{d}{\\mathbf{Z}}|{\\mathbb{U}} . Decorre do Teorema 2.49 que Y_{{\\mathbf{X}}={\\mathbf{x}}} é independente de {\\mathbf{Z}} dado {\\mathbb{U}} . Assim, \\displaystyle{\\mathbb{P}}({\\mathbf{Y}}_{{\\mathbf{X}}={\\mathbf{x}}}\\leq{\\mathbf% {y}}|{\\mathbf{Z}}={\\mathbf{z}}) \\displaystyle=\\int{\\mathbb{P}}({\\mathbf{Y}}_{{\\mathbf{X}}={\\mathbf{x}}}\\leq{% \\mathbf{y}}|{\\mathbf{Z}}={\\mathbf{z}},{\\mathbb{U}})f({\\mathbb{U}}|{\\mathbf{Z}}% ={\\mathbf{z}})d{\\mathbb{U}} \\displaystyle=\\int{\\mathbb{P}}({\\mathbf{Y}}_{{\\mathbf{X}}={\\mathbf{x}}}\\leq{% \\mathbf{y}}|{\\mathbb{U}})f({\\mathbb{U}}|{\\mathbf{Z}}={\\mathbf{z}})d{\\mathbb{U}} \\displaystyle{\\mathbf{Y}}_{{\\mathbf{X}}={\\mathbf{x}}}\\perp^{f}{\\mathbf{Z}}|{% \\mathbb{U}} ∎ Previous page Next page"],[["index.html","chapter6.html"],"Capítulo 6 Demonstrações ‣ Inferência Causal","Skip to content. Demonstrações Capítulo 6 Demonstrações Previous page Next page"],[["index.html"],"Inferência Causal","Skip to content. Inferência Causal Inferência Causal Rafael Bassi Stern Última revisão: August 27, 2024 Por favor, enviem comentários, typos e erros para rbstern@gmail.com Agradecimentos: Vitor Mello “Teaching is giving opportunities to students to discover things by themselves.” George Pólya Next page"]]